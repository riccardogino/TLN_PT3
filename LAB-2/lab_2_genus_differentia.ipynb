{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Volpe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import json\n",
    "import polars as pl\n",
    "\n",
    "# Download WordNet data\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import utils as u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_genus(sentence):\n",
    "    genus = []\n",
    "\n",
    "    comma_split = sentence.split(',')\n",
    "\n",
    "    splitted_sent = []\n",
    "\n",
    "    for cs in comma_split:\n",
    "        if cs.split('.') != '':\n",
    "            splitted_sent += cs.split('.')\n",
    "\n",
    "    if '' in splitted_sent:\n",
    "        splitted_sent.remove('')\n",
    "\n",
    "    # Pattern di identificazione del genus\n",
    "    targets = {\n",
    "        1: [[\"it\"], [\"is\", \"'s\"], [\"a\", \"an\", \"the\", \"for\"]],\n",
    "        2: [[\"that\", \"of\", \"to\"]],\n",
    "        3: [[\"a\", \"an\", \"for\", \"to\"]]\n",
    "    }\n",
    "\n",
    "    for sentence in splitted_sent:\n",
    "        tokenized_def = u.get_lemmatized_tokens_list_pos(u.tokenizer(sentence))\n",
    "\n",
    "        for i in range(0, len(tokenized_def)):\n",
    "            if i > 0 and tokenized_def[i][0].lower() in targets[2][0]:\n",
    "                if tokenized_def[i-1][0] not in genus and tokenized_def[i-1][1] == 'n':\n",
    "                    genus.append(tokenized_def[i-1][0])\n",
    "\n",
    "            if (i < len(tokenized_def)-1):\n",
    "                if tokenized_def[i][1] == 'a' and tokenized_def[i+1][1] == 'n' and tokenized_def[i+1][0] not in genus:\n",
    "                    genus.append(tokenized_def[i+1][0])\n",
    "\n",
    "            if i < len(tokenized_def)-2 and tokenized_def[i][0].lower() in targets[3][0]:\n",
    "                if tokenized_def[i+1][0] not in genus and tokenized_def[i+1][1] == 'n':\n",
    "                    genus.append(tokenized_def[i+1][0])\n",
    "\n",
    "                elif tokenized_def[i+1][1] == 'a' and tokenized_def[i+2][1] == 'n':\n",
    "                    genus.append(tokenized_def[i+2][0])\n",
    "\n",
    "            if (i < len(tokenized_def)-3):\n",
    "                if tokenized_def[i][0].lower() in targets[1][0] and (tokenized_def[i+1][0] in targets[1][1]) and (tokenized_def[i+2][0] in targets[1][2]):\n",
    "                    if tokenized_def[i+3][0] not in genus:\n",
    "                        genus.append(tokenized_def[i+3][0])\n",
    "    return genus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['construction']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_genus(\"A construction used to divide two rooms, temporarily closing the passage between them\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_term(sentence):\n",
    "    genus = get_genus(sentence)\n",
    "\n",
    "    #print(genus)\n",
    "\n",
    "    # prendo gli iponimi dei sensi del genus\n",
    "    hy = []\n",
    "    for g in genus:\n",
    "        genus_synset = wn.synsets(u.lemmatize_word(g, 'n'))\n",
    "        # print(genus_synset)\n",
    "        for syns in genus_synset:\n",
    "            for h in syns.hyponyms():\n",
    "                hy.append(h)\n",
    "\n",
    "    #print(hy)\n",
    "\n",
    "    # vedo tra gli esempi quali sostantivi sono ricorrenti, e ne ricavo il synset\n",
    "    results = {}\n",
    "    for h in hy:\n",
    "        # prendo gli esempi\n",
    "        h_examples = h.examples() + [h.definition()]\n",
    "        # print(h_examples)\n",
    "\n",
    "        if len(h_examples) > 0:\n",
    "            for ex in h_examples:\n",
    "                if len(ex) > 1:\n",
    "                    for token in u.noise_reduction_en_pos(ex):\n",
    "\n",
    "                        if token[1] == 'n':\n",
    "                            if token[0] not in results:\n",
    "                                results[token[0]] = 1\n",
    "                            else:\n",
    "                                results[token[0]] += 1\n",
    "    if len(results) == 0:\n",
    "        return None\n",
    "\n",
    "    \n",
    "    \n",
    "    #print(average_value)\n",
    "\n",
    "    results = sorted(results.items(), key=lambda item: item[1], reverse=True)\n",
    "    risultato = {chiave: valore for chiave,\n",
    "                 valore in results if valore > 1}\n",
    "    total_values = sum(risultato.values())\n",
    "    if len(risultato) == 0:\n",
    "        return None\n",
    "    average_value = total_values/len(risultato)\n",
    "    risultato = {chiave: valore for chiave,\n",
    "                 valore in results if valore > average_value}\n",
    "\n",
    "    return risultato\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'structure': 58,\n",
       " 'building': 11,\n",
       " 'construction': 11,\n",
       " 'wall': 5,\n",
       " 'ship': 5,\n",
       " 'room': 5,\n",
       " 'part': 5,\n",
       " 'people': 5}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_target_term(\"A construction used to divide two rooms, temporarily closing the passage between them\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = json.loads(pl.read_csv(\n",
    "    \"../datasets/TLN-definitions-23.tsv\", separator='\\t').write_json())\n",
    "\n",
    "data = {}\n",
    "\n",
    "for col in df['columns']:\n",
    "    if col['name'] != '1':\n",
    "        data[col['name']] = []\n",
    "        for v in col['values']:\n",
    "            data[col['name']].append(v)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'door': 0.23333333333333334,\n",
       " 'ladybug': 0.0,\n",
       " 'pain': 0.06666666666666667,\n",
       " 'blurriness': 0.0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results = {}\n",
    "\n",
    "for word in data:\n",
    "    i = 0\n",
    "    for sentence in data[word]:\n",
    "        target_terms = get_target_term(sentence)\n",
    "        if target_terms is not None:\n",
    "            if word in target_terms:\n",
    "                i += 1\n",
    "    final_results[word] = i/len(data[word])\n",
    "\n",
    "final_results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
