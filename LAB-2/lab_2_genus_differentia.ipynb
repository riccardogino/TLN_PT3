{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\riccardo.gino\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\riccardo.gino\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\riccardo.gino\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import json\n",
    "import polars as pl\n",
    "\n",
    "# Download WordNet data\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import utils as u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_genus(sentence):\n",
    "    genus = []\n",
    "\n",
    "    comma_split = sentence.split(',')\n",
    "\n",
    "    splitted_sent = []\n",
    "\n",
    "    for cs in comma_split:\n",
    "        if cs.split('.') != '':\n",
    "            splitted_sent += cs.split('.')\n",
    "\n",
    "    if '' in splitted_sent:\n",
    "        splitted_sent.remove('')\n",
    "\n",
    "    # Pattern di identificazione del genus\n",
    "    targets = {\n",
    "        1: [[\"it\"], [\"is\", \"'s\"], [\"a\", \"an\", \"the\", \"for\"]],\n",
    "        2: [[\"that\", \"of\", \"to\"]],\n",
    "        3: [[\"a\", \"an\", \"for\", \"to\"]]\n",
    "    }\n",
    "\n",
    "    for sentence in splitted_sent:\n",
    "        tokenized_def = u.get_lemmatized_tokens_list_pos(u.tokenizer(sentence))\n",
    "        # print(tokenized_def)\n",
    "\n",
    "        for i in range(0, len(tokenized_def)):\n",
    "            if i > 0 and tokenized_def[i][0].lower() in targets[2][0]:\n",
    "                if tokenized_def[i-1][0] not in genus and tokenized_def[i-1][1] == 'n':\n",
    "                    genus.append(tokenized_def[i-1][0])\n",
    "\n",
    "            if (i < len(tokenized_def)-1):\n",
    "                if tokenized_def[i][1] == 'a' and tokenized_def[i+1][1] == 'n' and tokenized_def[i+1][0] not in genus:\n",
    "                    genus.append(tokenized_def[i+1][0])\n",
    "\n",
    "            if i < len(tokenized_def)-2 and tokenized_def[i][0].lower() in targets[3][0]:\n",
    "                if tokenized_def[i+1][0] not in genus and tokenized_def[i+1][1] == 'n':\n",
    "                    genus.append(tokenized_def[i+1][0])\n",
    "\n",
    "                elif tokenized_def[i+1][1] == 'a' and tokenized_def[i+2][1] == 'n':\n",
    "                    genus.append(tokenized_def[i+2][0])\n",
    "\n",
    "            if (i < len(tokenized_def)-3):\n",
    "                if tokenized_def[i][0].lower() in targets[1][0] and (tokenized_def[i+1][0] in targets[1][1]) and (tokenized_def[i+2][0] in targets[1][2]):\n",
    "                    if tokenized_def[i+3][0] not in genus:\n",
    "                        genus.append(tokenized_def[i+3][0])\n",
    "    return genus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['structure', 'space', 'access', 'area']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_genus(\"Auxiliary structure built on blank spaces of walls and used to access different areas of building.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creo la struttura dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = json.loads(pl.read_csv(\n",
    "    \"../datasets/TLN-definitions-23.tsv\", separator='\\t').write_json())\n",
    "\n",
    "data = {}\n",
    "\n",
    "for col in df['columns']:\n",
    "    if col['name'] != '1':\n",
    "        data[col['name']] = []\n",
    "        for v in col['values']:\n",
    "            data[col['name']].append(v)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per ogni definizione, trovo il genus e poi prendo i 2 pi첫 frequenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_more_frequent_genus(sentences):\n",
    "    genus_freq_dict = {}\n",
    "\n",
    "    for sent in sentences:\n",
    "        genus_list = get_genus(sent)\n",
    "        for genus in genus_list:\n",
    "            if genus in genus_freq_dict:\n",
    "                genus_freq_dict[genus] += 1\n",
    "            else:\n",
    "                genus_freq_dict[genus] = 1\n",
    "    genus_freq_dict = sorted(genus_freq_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return genus_freq_dict[0], genus_freq_dict[1]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('object', 11), ('access', 10)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = get_more_frequent_genus(data['door'])\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tra i synsets dei genus pi첫 frequenti, restituisco il pi첫 lontano dalla radice (che dovrebbe essere il pi첫 specifico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_farther_from_root_genus(synset_g1, synset_g2):\n",
    "    root  = wn.synset('entity.n.01')\n",
    "    if synset_g1[0].shortest_path_distance(root) > synset_g2[0].shortest_path_distance(root):\n",
    "        return synset_g1\n",
    "    else:\n",
    "        return synset_g2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('entree.n.02'),\n",
       " Synset('access.n.02'),\n",
       " Synset('access.n.03'),\n",
       " Synset('access.n.04'),\n",
       " Synset('access.n.05'),\n",
       " Synset('access.n.06')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_farther_from_root_genus(wn.synsets(test[0][0], 'n'), wn.synsets(test[1][0], 'n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per ogni senso nel synset, restutisco tutti gli iponimi fino ad arrivare alle foglie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_hyponyms(genus_synset):\n",
    "    stak = []\n",
    "    hy = []\n",
    "\n",
    "    for g in genus_synset:\n",
    "        if g.name().split('.')[1] == 'n':\n",
    "            stak.append(g)\n",
    "\n",
    "    while len(stak) != 0:\n",
    "        hypo = stak.pop()\n",
    "\n",
    "        new_hyponims = hypo.hyponyms()\n",
    "\n",
    "        for h in new_hyponims:\n",
    "            if h not in stak and h.name().split('.')[1] == 'n':\n",
    "                stak.append(h)\n",
    "\n",
    "\n",
    "        hy.append(hypo)\n",
    "        # print(hy)\n",
    "\n",
    "    \n",
    "    return hy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percorro ogni ramo originato dal nodo del genus fino ad arrivare alle foglie, e per ogni nodo calcolo l'overlap lessicale tra la definizione del nodo e tutte le quelle a mia disposizione, memorizzando quelle con overlap maggiore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "456\n",
      "116\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "for word in data:\n",
    "    genus_1, genus_2 = get_more_frequent_genus(data[word])\n",
    "    target_genus_synset = get_farther_from_root_genus(wn.synsets(u.lemmatize_word(genus_1[0], 'n')), wn.synsets(u.lemmatize_word(genus_2[0], 'n')))\n",
    "    target_hyponyms_set = get_all_hyponyms(target_genus_synset)\n",
    "        \n",
    "    print(len(target_hyponyms_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_term(sentence):\n",
    "    genus = get_genus(sentence)\n",
    "\n",
    "    #print(genus)\n",
    "\n",
    "    # prendo gli iponimi dei sensi del genus\n",
    "    hy = []\n",
    "    for g in genus:\n",
    "        genus_synset = wn.synsets(u.lemmatize_word(g, 'n'))\n",
    "        # print(genus_synset)\n",
    "        for syns in genus_synset:\n",
    "            for h in syns.hyponyms():\n",
    "                hy.append(h)\n",
    "\n",
    "    #print(hy)\n",
    "\n",
    "    # vedo tra gli esempi quali sostantivi sono ricorrenti, e ne ricavo il synset\n",
    "    results = {}\n",
    "    for h in hy:\n",
    "        # prendo gli esempi\n",
    "        h_examples = h.examples() + [h.definition()]\n",
    "        # print(h_examples)\n",
    "\n",
    "        if len(h_examples) > 0:\n",
    "            for ex in h_examples:\n",
    "                if len(ex) > 1:\n",
    "                    for token in u.noise_reduction_en_pos(ex):\n",
    "\n",
    "                        if token[1] == 'n':\n",
    "                            if token[0] not in results:\n",
    "                                results[token[0]] = 1\n",
    "                            else:\n",
    "                                results[token[0]] += 1\n",
    "    if len(results) == 0:\n",
    "        return None\n",
    "\n",
    "    \n",
    "    \n",
    "    #print(average_value)\n",
    "\n",
    "    results = sorted(results.items(), key=lambda item: item[1], reverse=True)\n",
    "    risultato = {chiave: valore for chiave,\n",
    "                 valore in results if valore > 1}\n",
    "    total_values = sum(risultato.values())\n",
    "    if len(risultato) == 0:\n",
    "        return None\n",
    "    average_value = total_values/len(risultato)\n",
    "    risultato = {chiave: valore for chiave,\n",
    "                 valore in results if valore > average_value}\n",
    "\n",
    "    return risultato\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'structure': 58,\n",
       " 'building': 11,\n",
       " 'construction': 11,\n",
       " 'wall': 5,\n",
       " 'ship': 5,\n",
       " 'room': 5,\n",
       " 'part': 5,\n",
       " 'people': 5}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_target_term(\"A construction used to divide two rooms, temporarily closing the passage between them\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'door': 0.23333333333333334,\n",
       " 'ladybug': 0.0,\n",
       " 'pain': 0.06666666666666667,\n",
       " 'blurriness': 0.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results = {}\n",
    "\n",
    "for word in data:\n",
    "    i = 0\n",
    "    for sentence in data[word]:\n",
    "        target_terms = get_target_term(sentence)\n",
    "        if target_terms is not None:\n",
    "            if word in target_terms:\n",
    "                i += 1\n",
    "    final_results[word] = i/len(data[word])\n",
    "\n",
    "final_results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
