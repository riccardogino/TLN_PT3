{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/rossellaborra/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/rossellaborra/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/rossellaborra/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import json\n",
    "import polars as pl\n",
    "\n",
    "# Download WordNet data\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import utils as u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(definition_1_tokens, definition_2_tokens):\n",
    "\n",
    "    if len(definition_2_tokens) == 0 or len(definition_1_tokens) == 0:\n",
    "        return 0\n",
    "\n",
    "    min_len = 0\n",
    "\n",
    "    if len(definition_1_tokens) > len(definition_2_tokens):\n",
    "        min_len = len(definition_2_tokens)\n",
    "    else:\n",
    "        min_len = len(definition_1_tokens)\n",
    "\n",
    "    return (len(set(definition_1_tokens).intersection(set(definition_2_tokens))) / min_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv with polars module\n",
    "df = json.loads(pl.read_csv(\"../datasets/TLN-definitions-23.tsv\", separator='\\t').write_json())\n",
    "\n",
    "data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df['columns']:\n",
    "    if col['name'] != '1':\n",
    "        data[col['name']] = {}\n",
    "        count = 1\n",
    "        for v in col['values']:\n",
    "            data[col['name']][count] = u.noise_reduction_en(v)\n",
    "            count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_synsets_definitions(word):\n",
    "    synset_list = wn.synsets(word)\n",
    "    definitions = {}\n",
    "    for s in synset_list:\n",
    "        definitions[s] = u.noise_reduction_en(s.definition())\n",
    "    return definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "devo vedere tra le definizioni di tutti i synset della definizione tokenizzata quella più simile alla definizione di partenza e prendere il synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_synsets(tagged_token_list):\n",
    "    tokens_definitions_list = []\n",
    "\n",
    "    for token in tagged_token_list:\n",
    "        tokens_definitions_list.append(get_all_synsets_definitions(token))\n",
    "    return tokens_definitions_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_synset(tagged_tokens_list, list_of_tagged_tokens_list):\n",
    "    best_synsets = {}\n",
    "    max_score = 0\n",
    "    for synset_list in list_of_tagged_tokens_list:\n",
    "        for synset in synset_list.keys():\n",
    "            score = compute_similarity(tagged_tokens_list, synset_list[synset])\n",
    "            if score > 0.7 and score >= max_score:\n",
    "                best_synsets[synset] = score\n",
    "                max_score = score\n",
    "    return best_synsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(definition_1_tokens, definition_2_tokens):\n",
    "\n",
    "    min_len = 0\n",
    "\n",
    "    if len(definition_1_tokens) > len(definition_2_tokens):\n",
    "        min_len = len(definition_2_tokens)\n",
    "    else:\n",
    "        min_len = len(definition_1_tokens)\n",
    "\n",
    "    return (len(set(definition_1_tokens).intersection(set(definition_2_tokens))) / min_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'door': {3: {Synset('door.n.05'): 0.75},\n",
       "  6: {Synset('obstruct.v.02'): 1.0},\n",
       "  7: {Synset('unlock.v.01'): 1.0},\n",
       "  12: {Synset('outside.r.02'): 1.0},\n",
       "  13: {Synset('inside.r.02'): 1.0, Synset('outside.r.02'): 1.0},\n",
       "  19: {Synset('obstruct.v.02'): 1.0},\n",
       "  30: {Synset('space.n.07'): 1.0}},\n",
       " 'ladybug': {4: {Synset('back.v.06'): 1.0},\n",
       "  7: {Synset('back.v.06'): 1.0},\n",
       "  16: {Synset('back.v.06'): 1.0},\n",
       "  26: {Synset('back.v.06'): 1.0},\n",
       "  27: {Synset('back.v.06'): 1.0}},\n",
       " 'pain': {2: {Synset('hurt.v.05'): 1.0},\n",
       "  9: {Synset('express.r.01'): 1.0},\n",
       "  12: {Synset('result.n.03'): 1.0},\n",
       "  14: {Synset('harm.v.01'): 1.0},\n",
       "  17: {Synset('harm.v.01'): 1.0},\n",
       "  19: {Synset('write_out.v.02'): 1.0}},\n",
       " 'blurriness': {18: {Synset('function.n.02'): 1.0}}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = {}\n",
    "\n",
    "for word in data:\n",
    "    result[word] = {}\n",
    "    for id in data[word]:\n",
    "        best_synset = get_best_synset(\n",
    "            data[word][id], get_all_synsets(data[word][id]))\n",
    "\n",
    "        if len(best_synset) > 0:\n",
    "            result[word][id] = best_synset\n",
    "\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "door ok\n"
     ]
    }
   ],
   "source": [
    "for word in result:\n",
    "    \n",
    "    for synset in wn.synsets(word):\n",
    "        for id in result[word].keys():\n",
    "            if synset in result[word][id]:\n",
    "                print(word, 'ok')\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- considerare gli esempi dei synset delle parole della definizione e prendere quelle più ricorrenti  (magari anche dei loro ipo e ipernimi??)\n",
    "- considerare le definizioni dei vari synset delle parole della definizione e prendere i synset delle definizioni più simili\n",
    "- usare sto principio \"genus-differentia\"\n",
    "- prendo dalle definizioni le parole più ricorrenti, e poi dalle definizioni dei loro synset provo a ricavarmi la parola target\n",
    "- mi prendo da tutte le definizioni a mia disposizione le parole importnti e faccio un corpus importante, poi cerco tra tutte le definizioni in wordnet quella con più parole in comune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creo la struttura dati dalle definizioni per ogni parola\n",
    "for col in df['columns']:\n",
    "    if col['name'] != '1':\n",
    "        data[col['name']] = {}\n",
    "        token_list = []\n",
    "        for v in col['values']:\n",
    "            token_list += u.noise_reduction_en(v)\n",
    "\n",
    "        dict_token_freq = {}\n",
    "\n",
    "        for token in token_list:\n",
    "            if token not in dict_token_freq:\n",
    "                dict_token_freq[token] = 1\n",
    "            else:\n",
    "                dict_token_freq[token] += 1\n",
    "\n",
    "        data[col['name']] = dict(sorted(dict_token_freq.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('area.n.05'), Synset('position.n.07'), Synset('opportunity.n.01'), Synset('gathering.n.01'), Synset('populate.v.01')]\n",
      "[Synset('physical_entity.n.01'), Synset('goal.n.01'), Synset('constituent.n.04'), Synset('content.n.05'), Synset('computer_science.n.01'), Synset('disapprove.v.02'), Synset('be.v.01')]\n",
      "[Synset('right.n.01'), Synset('right.n.01'), Synset('way.n.06'), Synset('code.n.02'), Synset('operation.n.04'), Synset('approach.n.02'), Synset('recover.v.01'), Synset('reach.v.01')]\n",
      "[Synset('area.n.01'), Synset('outside.n.01'), Synset('tournament.n.01'), Synset('public_knowledge.n.01'), Synset('change_state.v.01'), Synset('start.v.06'), Synset('undo.v.01'), Synset('yield.v.01'), Synset('arise.v.04'), Synset('move.v.15'), Synset('expose.v.03')]\n",
      "[Synset('accept.v.03'), Synset('give.v.03'), Synset('assign.v.02'), Synset('yield.v.01'), Synset('calculate.v.02'), Synset('discount.v.02'), Synset('permit.v.01')]\n",
      "[Synset('end.n.02'), Synset('section.n.01'), Synset('finish.n.09'), Synset('change_state.v.01'), Synset('end.v.02'), Synset('end.v.01'), Synset('end.v.02'), Synset('trade.v.03'), Synset('prosecute.v.03'), Synset('end.v.03'), Synset('move.v.03'), Synset('move.v.03'), Synset('approach.v.01'), Synset('join.v.02'), Synset('barricade.v.01'), Synset('fill.v.09'), Synset('join.v.02'), Synset('complete.v.01')]\n",
      "[Synset('attribute.n.02'), Synset('amorphous_shape.n.01'), Synset('location.n.01'), Synset('area.n.01'), Synset('location.n.01'), Synset('space.n.01'), Synset('character.n.08'), Synset('time_interval.n.01'), Synset('area.n.06'), Synset('area.n.06'), Synset('type.n.06'), Synset('put.v.01')]\n",
      "[Synset('activity.n.01'), Synset('utility.n.02'), Synset('utility.n.02'), Synset('demand.n.02'), Synset('custom.n.01'), Synset('influence.n.02'), Synset('legal_right.n.01'), Synset('consume.v.02'), Synset('exploit.v.01'), Synset('act.v.01')]\n",
      "[Synset('fastener.n.02'), Synset('hair.n.01'), Synset('mechanism.n.05'), Synset('enclosure.n.01'), Synset('restraint.n.06'), Synset('wrestling_hold.n.01'), Synset('fasten.v.01'), Synset('move.v.02'), Synset('engage.v.06'), Synset('hold.v.02'), Synset('embrace.v.02'), Synset('overwhelm.v.01'), Synset('confine.v.03'), Synset('pass.v.01'), Synset('construct.v.01')]\n",
      "[Synset('arthropod.n.01'), Synset('unpleasant_person.n.01')]\n",
      "[Synset('chromatic_color.n.01'), Synset('radical.n.03'), Synset('sum.n.01')]\n",
      "[Synset('achromatic_color.n.01'), Synset('dark.n.01'), Synset('person.n.01'), Synset('person_of_color.n.01'), Synset('man.n.10'), Synset('clothing.n.01'), Synset('discolor.v.03')]\n",
      "[Synset('disk.n.01'), Synset('executive_department.n.01'), Synset('telegraphic_signal.n.01'), Synset('lysergic_acid_diethylamide.n.01'), Synset('cover.v.03'), Synset('discharge.v.02'), Synset('write.v.07'), Synset('mark.v.05')]\n",
      "[Synset('body_part.n.01'), Synset('size.n.02')]\n",
      "[Synset('dipterous_insect.n.01'), Synset('flap.n.01'), Synset('opening.n.10'), Synset('hit.n.02'), Synset('fisherman's_lure.n.01'), Synset('travel.v.01'), Synset('move.v.03'), Synset('operate.v.03'), Synset('transport.v.02'), Synset('travel.v.01'), Synset('change.v.02'), Synset('elapse.v.01'), Synset('travel.v.05'), Synset('show.v.04'), Synset('scat.v.01'), Synset('travel.v.04'), Synset('hit.v.01'), Synset('decrease.v.01')]\n",
      "[Synset('condition.n.03'), Synset('phenomenon.n.01'), Synset('phenomenon.n.01')]\n",
      "[Synset('advantage.n.01'), Synset('morality.n.01'), Synset('quality.n.01'), Synset('artifact.n.01')]\n",
      "[Synset('visual_property.n.01'), Synset('interest.n.03'), Synset('timbre.n.01'), Synset('race.n.03'), Synset('appearance.n.01'), Synset('material.n.01'), Synset('kind.n.01'), Synset('appearance.n.01'), Synset('change.v.01'), Synset('affect.v.01'), Synset('influence.v.01'), Synset('decorate.v.01'), Synset('apologize.v.02'), Synset('change.v.02')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[Synset('perception.n.03'), Synset('expert.n.01'), Synset('stir.n.02'), Synset('excitement.n.02'), Synset('faculty.n.01')]\n",
      "[Synset('state.n.02'), Synset('idea.n.01'), Synset('atmosphere.n.01'), Synset('somesthesia.n.01'), Synset('perception.n.03'), Synset('intuition.n.01'), Synset('reason.v.01'), Synset('perceive.v.01'), Synset('be.v.01'), Synset('think.v.01'), Synset('experience.v.01'), Synset('look.v.02'), Synset('search.v.01'), Synset('touch.v.01'), Synset('find.v.03'), Synset('look.v.02'), Synset('touch.v.01')]\n",
      "[Synset('origin.n.03'), Synset('justification.n.02'), Synset('venture.n.01'), Synset('physical_entity.n.01'), Synset('proceeding.n.01'), Synset('make.v.03')]\n",
      "[Synset('seat.n.05'), Synset('perceive.v.01'), Synset('experience.v.01'), Synset('imagine.v.01'), Synset('think.v.01'), Synset('watch.v.01'), Synset('verify.v.01'), Synset('visit.v.03'), Synset('visit.v.03'), Synset('tour.v.01'), Synset('care.v.02'), Synset('receive.v.05'), Synset('consort.v.01'), Synset('see.v.01'), Synset('consider.v.05'), Synset('detect.v.01'), Synset('undergo.v.01'), Synset('accompany.v.02'), Synset('bet.v.02'), Synset('understand.v.01')]\n",
      "[Synset('representation.n.01'), Synset('appearance.n.01'), Synset('representation.n.02'), Synset('model.n.07'), Synset('rhetorical_device.n.01'), Synset('person.n.01'), Synset('set.n.02'), Synset('impression.n.02'), Synset('representation.n.02'), Synset('visualize.v.02'), Synset('imagine.v.01')]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for word in data:\n",
    "    for token in data[word]:\n",
    "        if data[word][token] > 5:\n",
    "            synsets = wn.synsets(token)\n",
    "            hypernims = []\n",
    "            hyponims = []\n",
    "            for syns in synsets:\n",
    "                hypernims += syns.hypernyms()\n",
    "                hyponims += syns.hyponyms()\n",
    "            print(hypernims)\n",
    "        else:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parola chiave: il\n",
      "Definizione: a midwestern state in north-central United States\n",
      "Esempi: []\n",
      "---\n",
      "Parola chiave: un\n",
      "Definizione: an organization of independent states formed in 1945 to promote international peace and security\n",
      "Esempi: []\n",
      "---\n",
      "Parola chiave: cane\n",
      "Definizione: a stick that people can lean on to help them walk\n",
      "Esempi: []\n",
      "---\n",
      "Parola chiave: la\n",
      "Definizione: a white soft metallic element that tarnishes readily; occurs in rare earth minerals and is usually classified as a rare earth\n",
      "Esempi: []\n",
      "---\n",
      "Parola chiave: al\n",
      "Definizione: a silvery ductile metallic element found primarily in bauxite\n",
      "Esempi: []\n",
      "---\n",
      "Parola chiave: rosa\n",
      "Definizione: large genus of erect or climbing prickly shrubs including roses\n",
      "Esempi: []\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/rossellaborra/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Scarica i dati di WordNet (se necessario)\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Funzione per ottenere il Synset corrispondente a una parola chiave\n",
    "def get_synset_from_word(keyword):\n",
    "    synsets = wordnet.synsets(keyword)\n",
    "    if synsets:\n",
    "        return synsets[0]  # Ritorna il primo Synset trovato\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Esempio di definizioni nel corpus (puoi aggiungere le tue definizioni qui)\n",
    "definitions_corpus = [\n",
    "    \"Il gatto è un mammifero carnivoro della famiglia dei Felidi.\",\n",
    "    \"Il cane è un animale domestico appartenente alla famiglia dei Canidi.\",\n",
    "    \"La rosa è un fiore appartenente al genere Rosa.\",\n",
    "    \"La mela è il frutto del melo, pianta della famiglia delle Rosacee.\"\n",
    "]\n",
    "\n",
    "# Estrai le parole chiave dalle definizioni (in questo caso, i sostantivi)\n",
    "keywords = []\n",
    "for definition in definitions_corpus:\n",
    "    tokens = nltk.word_tokenize(definition)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    for word, pos in tagged_tokens:\n",
    "        if pos.startswith('NN'):  # Filtra solo i sostantivi (NN, NNS, NNP, NNPS)\n",
    "            keywords.append(word.lower())\n",
    "\n",
    "# Cerca i Synset corrispondenti alle parole chiave\n",
    "synsets_found = {}\n",
    "for keyword in keywords:\n",
    "    synset = get_synset_from_word(keyword)\n",
    "    if synset:\n",
    "        synsets_found[keyword] = synset\n",
    "\n",
    "# Stampa i risultati\n",
    "for keyword, synset in synsets_found.items():\n",
    "    print(f\"Parola chiave: {keyword}\")\n",
    "    print(f\"Definizione: {synset.definition()}\")\n",
    "    print(f\"Esempi: {synset.examples()}\")\n",
    "    print(\"---\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
