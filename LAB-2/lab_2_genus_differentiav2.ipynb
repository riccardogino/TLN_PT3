{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T17:05:39.037465Z",
     "start_time": "2023-11-12T17:05:38.791379Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\r.borra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\r.borra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\r.borra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\r.borra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk import pos_tag\n",
    "sys.path.append('..')\n",
    "nltk.download('stopwords')\n",
    "from utils import utils as u\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import Counter\n",
    "from prettytable import PrettyTable\n",
    "from colorama import Fore, Style\n",
    "from nltk.wsd import lesk\n",
    "nltk.download('wordnet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Creo la struttura dati\n",
    "\n",
    "La struttura dati utilizzata avrÃ  la seguente forma: \n",
    "$$data[word] = [definition_i, definition_{i+1}, ..., definition_n]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_path = '../datasets/TLN-definitions-23.tsv'\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "data = {}\n",
    "definitions = []\n",
    "for col in df.columns:\n",
    "    definitions = []\n",
    "    if col != '1':\n",
    "        data[col] = {} ## data[ladybug]\n",
    "        for riga in df[col]:\n",
    "            if col != '1':\n",
    "                definitions.append(riga)\n",
    "        data[col] = definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Tokenizzazione e pulizia delle definizioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T14:43:36.404281Z",
     "start_time": "2023-11-11T14:43:36.354934Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def cleaning_definition_token(data):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Frase di esempio\n",
    "    words= []\n",
    "    words_for_every_label = {}\n",
    "\n",
    "    for item,content in data.items():\n",
    "        words= []\n",
    "        for definition in content:\n",
    "            tokens = word_tokenize(definition)\n",
    "            words_clean = [token.lower() for token in tokens if token.isalpha()\n",
    "                    and token.lower() not in stop_words]\n",
    "            tagged_words = pos_tag(words_clean)\n",
    "            nouns = [word for word, pos in tagged_words if pos == 'NN']\n",
    "            for n in nouns:\n",
    "                words.append(n)\n",
    "        \n",
    "        words_for_every_label[item] = words\n",
    "    return words_for_every_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizzazione e pulizia delle definizioni degli iponimi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T17:41:55.898884Z",
     "start_time": "2023-11-11T17:41:55.757422Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def cleaning_definition_token_hypo(data):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    words= []\n",
    "    words_for_every_label = {}\n",
    "    list_of_tuple=[]\n",
    "    for label, content in data.items():\n",
    "        list_of_tuple=[]\n",
    "        for tupla in data[label]:\n",
    "\n",
    "            tokens = word_tokenize(tupla[1])\n",
    "                \n",
    "            words_clean = [token.lower() for token in tokens if token.isalpha()\n",
    "                       and token.lower() not in stop_words]\n",
    "            \n",
    "            tagged_words = pos_tag(words_clean)\n",
    "          \n",
    "            nouns = [word for word, pos in tagged_words if pos == 'NN']\n",
    "        \n",
    "            list_of_tuple.append((tupla[0],nouns))\n",
    "        words_for_every_label[label] = list_of_tuple\n",
    "    return words_for_every_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ricerca genus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T17:25:21.627698Z",
     "start_time": "2023-11-12T17:25:21.608032Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_genus(words_for_every_label,k_genus):\n",
    "\n",
    "    top_tokens_for_labels = {}\n",
    "    genus = []\n",
    "    for label, label_tokens in words_for_every_label.items():\n",
    "        genus = []\n",
    " \n",
    "        token_counts = Counter(label_tokens)\n",
    "\n",
    "        top_tokens = token_counts.most_common(k_genus)\n",
    "\n",
    "        for t in top_tokens:\n",
    "            genus.append(t[0])\n",
    "  \n",
    "            top_tokens_for_labels[label] = genus\n",
    " \n",
    "    return top_tokens_for_labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ricerca dei genus, mantenendo il numero di frequenza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T17:25:19.818394Z",
     "start_time": "2023-11-12T17:25:19.722809Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_genus_with_score(words_for_every_label,k_genus):\n",
    "   \n",
    "    top_tokens_for_labels = {}\n",
    "    genus = []\n",
    "    for label, label_tokens in words_for_every_label.items():\n",
    "        genus = []\n",
    "        token_counts = Counter(label_tokens)\n",
    "        top_tokens = token_counts.most_common(k_genus)\n",
    "\n",
    "        for t in top_tokens:\n",
    "            genus.append((t[0],t[1]))\n",
    "            top_tokens_for_labels[label] = genus\n",
    "    return top_tokens_for_labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Algoritmo di lesk per la WSD dei genus trovati metodo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T17:43:59.055233Z",
     "start_time": "2023-11-12T17:43:58.973270Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lesk_for_disambiguation(genus_for_every_label, definition_clean):\n",
    "    genus_best_sense = {}\n",
    "    best_senses = []\n",
    "    for label1,genus in genus_for_every_label.items():\n",
    "        best_senses = []\n",
    "        for label2,tokens in definition_clean.items():\n",
    "            if label1 == label2:\n",
    "                for gen in genus_for_every_label[label1]:\n",
    "                    if lesk(tokens,gen)  is not None:\n",
    "                        best_senses.append((gen,lesk(definition_clean[label1],gen)))\n",
    "        genus_best_sense[label1] = best_senses\n",
    "    return genus_best_sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo di lesk per la WSD dei genus trovati metodo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T17:44:03.480605Z",
     "start_time": "2023-11-12T17:44:03.405906Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lesk_for_disambiguation_metod2(genus_for_every_label, definition_clean):\n",
    "    genus_best_sense = {}\n",
    "    best_senses = []\n",
    "    for label1,genus in genus_for_every_label.items():\n",
    "        best_senses = []\n",
    "        for label2,tokens in definition_clean.items():\n",
    "            if label1 == label2:\n",
    "                for gen in genus_for_every_label[label1]:\n",
    "    \n",
    "                    if lesk(tokens,gen[0])  is not None:\n",
    "                        \n",
    "                        best_senses.append((gen[0],lesk(definition_clean[label1],gen[0])))\n",
    "\n",
    "        genus_best_sense[label1] = best_senses\n",
    " \n",
    "    return genus_best_sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ricerca dei genus (metodo 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T15:20:39.767774Z",
     "start_time": "2023-11-12T15:20:39.697807Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_synset_genus_metodo2(genus_for_every_label):\n",
    "    genus_sense = {}\n",
    "    best_senses = []\n",
    "    for label1,genus in genus_for_every_label.items():\n",
    "        best_senses = []\n",
    "        for gen in genus_for_every_label[label1]:\n",
    "             best_senses.append((gen[0],wn.synsets(gen[0])))\n",
    "\n",
    "        genus_sense[label1] = best_senses\n",
    "  \n",
    "    return genus_sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ricerca degli iponimi fino alle foglie dell'albero di wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_hyponyms(genus_synset):\n",
    "    stak = []\n",
    "    hy = []\n",
    "\n",
    "    for g in genus_synset:\n",
    "        if g.name().split('.')[1] == 'n':\n",
    "            stak.append(g)\n",
    "\n",
    "    while len(stak) != 0:\n",
    "        hypo = stak.pop()\n",
    "\n",
    "        new_hyponims = hypo.hyponyms()\n",
    "\n",
    "        for h in new_hyponims:\n",
    "            if h not in stak and h.name().split('.')[1] == 'n':\n",
    "                stak.append(h)\n",
    "\n",
    "\n",
    "        hy.append(hypo)\n",
    "   \n",
    "\n",
    "    \n",
    "    return hy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ricerca iponimi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T14:43:13.628862Z",
     "start_time": "2023-11-11T14:43:13.573402Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_hypo(synsets_genus):\n",
    "    hyponyms_genus={}\n",
    "\n",
    "    for l,bestsense_genus in synsets_genus.items():\n",
    "        hyponyms = []\n",
    "        for genus_bs in bestsense_genus:\n",
    "            \n",
    "            #hyponyms_ = get_all_hyponyms(genus_bs[1].hyponyms())\n",
    "            hyponyms_ = genus_bs[1].hyponyms()\n",
    "            if hyponyms_ != []:\n",
    "                for h in hyponyms_ :\n",
    "                    hyponyms.append(h)\n",
    "       \n",
    "        hyponyms_genus[l]= hyponyms\n",
    "    return hyponyms_genus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ricerca delle definizioni degli iponimi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T16:07:02.906379Z",
     "start_time": "2023-11-11T16:07:02.858149Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_definitions_hypo(synsets_hypo):\n",
    "    hypo_definitions={}\n",
    " \n",
    "    for label, content in synsets_hypo.items():\n",
    "        definitions=[]\n",
    "        for synset in synsets_hypo[label]:\n",
    "            if synset.definition() is not None:\n",
    "                definitions.append((synset,synset.definition()))\n",
    "                \n",
    "        hypo_definitions[label] = definitions\n",
    "\n",
    "\n",
    "    return hypo_definitions\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intersezione tra definizioni target e definizioni degli iponimi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T16:44:36.256297Z",
     "start_time": "2023-11-11T16:44:36.206653Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def token_intersection_definitions(def_hypo,def_targets):\n",
    "    synset_score={}\n",
    "    tupla_score=[]\n",
    "    for label,content in def_hypo.items():\n",
    "        tupla_score=[]\n",
    "        for token_def in def_hypo[label]:\n",
    "            interc = set(token_def[1]).intersection(set(def_targets[label]))\n",
    "            score = len(interc) \n",
    "            tupla_score.append((score,token_def[0]))\n",
    "        synset_score[label] = tupla_score\n",
    "    return synset_score\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ricerca del best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T17:52:52.243973Z",
     "start_time": "2023-11-11T17:52:52.154252Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def search_best_score(hypo_score):\n",
    "    max_score = 1\n",
    "    best_synsets_list = []\n",
    "    best_synsets_score = {}\n",
    "    for label, c in hypo_score.items():\n",
    "        max_score = 0\n",
    "        best_synsets_list = []\n",
    "        for content in hypo_score[label]:\n",
    "            if  content[0] >= max_score:\n",
    "                if best_synsets_list != [] :\n",
    "                    last_elem= best_synsets_list[-1][1]\n",
    "                    if last_elem < content[0]:\n",
    "                        best_synsets_list.clear()\n",
    "                max_score = content[0]\n",
    "                best_synsets_list.append((content[1],content[0]))\n",
    "        best_synsets_score[label] =  best_synsets_list\n",
    "    return best_synsets_score\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conteggio dei genus all'interno delle definizioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T16:31:02.153504Z",
     "start_time": "2023-11-12T16:31:02.106370Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_of_genus_in_hypo_definition(genus_with_score,hypo_defs):\n",
    "    tupla3_to_append = []\n",
    "    genus_score_in_def = {}\n",
    "    count_score =0\n",
    "    for label,c in genus_with_score.items():\n",
    "        tupla3_to_append = []\n",
    "        for synset_def in hypo_defs[label]:\n",
    "            final_score= 0\n",
    "            for genus in genus_with_score[label]:\n",
    "                count_score =0\n",
    "                count_score = synset_def[1].count(genus[0]) \n",
    "                if count_score != 0:\n",
    "                    final_score = final_score + genus[1] + count_score\n",
    "            tupla3_to_append.append((final_score,synset_def[0]))\n",
    "        genus_score_in_def[label] = tupla3_to_append\n",
    "    return genus_score_in_def\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcolo della distanza nell'albero di wordnet tra la parola target e la parola trovata dall'algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T17:59:17.240588Z",
     "start_time": "2023-11-12T17:59:17.192309Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def similarity_synsets(s1,s2):\n",
    "    \n",
    "\n",
    "    similarity_score = s1.path_similarity(s2)\n",
    "    similarity_score = \"{:.2f}\".format(similarity_score)\n",
    "   \n",
    "    return similarity_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T17:15:11.181142Z",
     "start_time": "2023-11-12T17:15:11.145860Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def search_synset_target_words(data):\n",
    "    synset_target_word = {}\n",
    "    for label,c in data.items():\n",
    "        synset_target_word[label] = wn.synsets(label)[0]\n",
    "    return synset_target_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulizia definizioni e concatenazione di tutti i nomi contenuti nell'insieme delle definizioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "definition_clean = cleaning_definition_token(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numero di genus in input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_genus = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METODO 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'door': [(1, Synset('anechoic_chamber.n.01')), (3, Synset('anteroom.n.01')), (1, Synset('back_room.n.01')), (1, Synset('ballroom.n.01')), (1, Synset('barroom.n.01')), (1, Synset('bathroom.n.01')), (1, Synset('bedroom.n.01')), (1, Synset('belfry.n.02')), (1, Synset('billiard_room.n.01')), (1, Synset('boardroom.n.01')), (1, Synset('cardroom.n.01')), (1, Synset('cell.n.06')), (1, Synset('cell.n.07')), (1, Synset('chamber.n.03')), (1, Synset('checkroom.n.01')), (2, Synset('classroom.n.01')), (1, Synset('clean_room.n.01')), (1, Synset('cloakroom.n.02')), (1, Synset('closet.n.04')), (1, Synset('clubroom.n.01')), (3, Synset('compartment.n.02')), (1, Synset('conference_room.n.01')), (1, Synset('control_room.n.01')), (1, Synset('court.n.02')), (1, Synset('cubby.n.01')), (1, Synset('cutting_room.n.01')), (1, Synset('darkroom.n.01')), (1, Synset('den.n.04')), (1, Synset('dinette.n.01')), (1, Synset('dining_room.n.01')), (2, Synset('door.n.05')), (1, Synset('dressing_room.n.01')), (1, Synset('durbar.n.01')), (1, Synset('engineering.n.03')), (1, Synset('floor.n.10')), (2, Synset('furnace_room.n.01')), (1, Synset('gallery.n.03')), (1, Synset('gallery.n.04')), (1, Synset('greenroom.n.01')), (1, Synset('guardroom.n.02')), (1, Synset('hall.n.03')), (1, Synset('hospital_room.n.01')), (1, Synset('kitchen.n.01')), (1, Synset('library.n.01')), (2, Synset('living_room.n.01')), (1, Synset('locker_room.n.01')), (1, Synset('lounge.n.02')), (1, Synset('manor_hall.n.01')), (1, Synset('poolroom.n.01')), (2, Synset('presence_chamber.n.01')), (1, Synset('rathole.n.02')), (1, Synset('reading_room.n.01')), (2, Synset('reception_room.n.01')), (1, Synset('recreation_room.n.01')), (1, Synset('rotunda.n.02')), (1, Synset('scriptorium.n.01')), (1, Synset('scullery.n.01')), (1, Synset('sewing_room.n.01')), (1, Synset('shipping_room.n.01')), (1, Synset('shower_room.n.01')), (1, Synset('sickbay.n.01')), (2, Synset('sickroom.n.01')), (1, Synset('smoking_room.n.01')), (1, Synset('squad_room.n.01')), (1, Synset('squad_room.n.02')), (1, Synset('steam_bath.n.01')), (1, Synset('storeroom.n.01')), (1, Synset('study.n.05')), (1, Synset('sun_parlor.n.01')), (1, Synset('surgery.n.02')), (1, Synset('television_room.n.01')), (1, Synset('test_room.n.01')), (2, Synset('toilet.n.01')), (1, Synset('torture_chamber.n.01')), (1, Synset('vestry.n.02')), (2, Synset('walk-in.n.04')), (1, Synset('war_room.n.01')), (1, Synset('workroom.n.01')), (2, Synset('back_door.n.02')), (1, Synset('adit.n.01')), (1, Synset('aisle.n.01')), (1, Synset('channel.n.02')), (1, Synset('conduit.n.01')), (2, Synset('cul.n.01')), (1, Synset('fish_ladder.n.01')), (1, Synset('passageway.n.01')), (2, Synset('right_of_way.n.03')), (1, Synset('shaft.n.10')), (1, Synset('throat.n.03')), (2, Synset('attic.n.04')), (2, Synset('bearing_wall.n.01')), (2, Synset('cavity_wall.n.01')), (1, Synset('chimney_breast.n.01')), (2, Synset('firewall.n.03')), (1, Synset('gable.n.01')), (1, Synset('parapet.n.01')), (1, Synset('proscenium.n.02')), (2, Synset('sidewall.n.02')), (1, Synset('wainscoting.n.01')), (1, Synset('arch.n.03')), (6, Synset('doorway.n.01')), (1, Synset('gateway.n.01')), (1, Synset('hatchway.n.01')), (1, Synset('pithead.n.01')), (1, Synset('portal.n.01')), (2, Synset('porte-cochere.n.01')), (2, Synset('service_door.n.01')), (2, Synset('stage_door.n.01')), (1, Synset('vomitory.n.01')), (1, Synset('angle.n.01')), (2, Synset('cavity.n.02')), (2, Synset('compartment.n.01')), (1, Synset('crenel.n.02')), (1, Synset('enclosure.n.03')), (2, Synset('expanse.n.03')), (1, Synset('hole.n.04')), (1, Synset('opening.n.01')), (1, Synset('pleural_space.n.01')), (1, Synset('pocket.n.04')), (1, Synset('subarachnoid_space.n.01')), (1, Synset('swath.n.01')), (2, Synset('void.n.02')), (0, Synset('anomaly.n.03')), (1, Synset('back.n.03')), (1, Synset('front.n.09')), (0, Synset('half-mast.n.01')), (0, Synset('juxtaposition.n.02')), (0, Synset('landmark.n.01')), (0, Synset('lead.n.09')), (1, Synset('left.n.01')), (1, Synset('lie.n.03')), (0, Synset('pitch.n.03')), (0, Synset('pole_position.n.01')), (1, Synset('polls.n.01')), (0, Synset('post.n.01')), (1, Synset('pride_of_place.n.01')), (1, Synset('right.n.02')), (1, Synset('setting.n.05')), (0, Synset('site.n.02')), (1, Synset('stand.n.02')), (0, Synset('station.n.03')), (1, Synset('vantage.n.01')), (0, Synset('wing.n.07')), (0, Synset('outfall.n.01'))], 'ladybug': [(1, Synset('good_luck.n.02')), (0, Synset('dye.n.01')), (0, Synset('hematochrome.n.01')), (0, Synset('indicator.n.04')), (0, Synset('mordant.n.01')), (0, Synset('paint.n.01')), (0, Synset('pigment.n.01')), (1, Synset('pigment.n.02')), (0, Synset('stain.n.02')), (0, Synset('tincture.n.01')), (2, Synset('reddish_orange.n.01')), (0, Synset('affine.n.01'))], 'pain': [(1, Synset('constriction.n.03')), (1, Synset('agony.n.01')), (2, Synset('discomfort.n.02')), (0, Synset('throes.n.01')), (0, Synset('erupt.v.08')), (0, Synset('birth_trauma.n.01')), (0, Synset('blast_trauma.n.01')), (0, Synset('bleeding.n.01')), (1, Synset('blunt_trauma.n.01')), (1, Synset('brain_damage.n.01')), (0, Synset('bruise.n.01')), (1, Synset('bump.n.01')), (0, Synset('burn.n.03')), (0, Synset('dislocation.n.03')), (1, Synset('electric_shock.n.02')), (0, Synset('fracture.n.01')), (0, Synset('frostbite.n.01')), (0, Synset('intravasation.n.01')), (1, Synset('penetrating_trauma.n.01')), (1, Synset('pinch.n.02')), (0, Synset('rupture.n.01')), (0, Synset('sting.n.03')), (1, Synset('strain.n.07')), (0, Synset('wale.n.01')), (0, Synset('whiplash.n.01')), (0, Synset('wound.n.01')), (0, Synset('wrench.n.01')), (2, Synset('anger.n.01')), (1, Synset('anxiety.n.02')), (1, Synset('conditioned_emotional_response.n.01')), (1, Synset('emotional_state.n.01')), (2, Synset('fear.n.01')), (1, Synset('fear.n.03')), (2, Synset('hate.n.01')), (1, Synset('joy.n.01')), (1, Synset('love.n.01'))], 'blurriness': [(1, Synset('bitmap.n.01')), (1, Synset('chiaroscuro.n.01')), (1, Synset('collage.n.01')), (1, Synset('foil.n.04')), (1, Synset('graphic.n.01')), (1, Synset('iconography.n.01')), (1, Synset('inset.n.01')), (3, Synset('likeness.n.02')), (1, Synset('panorama.n.02')), (2, Synset('reflection.n.05')), (1, Synset('scan.n.02')), (1, Synset('sonogram.n.01')), (1, Synset('provide.v.03')), (1, Synset('action.n.02')), (1, Synset('being.n.01')), (1, Synset('cleavage.n.01')), (1, Synset('condition.n.01')), (1, Synset('condition.n.03')), (1, Synset('conditionality.n.01')), (1, Synset('conflict.n.04')), (1, Synset('damnation.n.02')), (2, Synset('dead_letter.n.01')), (2, Synset('death.n.03')), (0, Synset('degree.n.02')), (2, Synset('dependence.n.01')), (0, Synset('disorder.n.03')), (1, Synset('dystopia.n.01')), (1, Synset('employment.n.01')), (1, Synset('end.n.06')), (1, Synset('enlargement.n.02')), (0, Synset('feeling.n.01')), (1, Synset('flux.n.05')), (1, Synset('freedom.n.01')), (1, Synset('grace.n.01')), (1, Synset('ground_state.n.01')), (1, Synset('heterozygosity.n.01')), (1, Synset('homozygosity.n.01')), (1, Synset('hostility.n.02')), (0, Synset('illumination.n.02')), (0, Synset('immaturity.n.01')), (1, Synset('imminence.n.01')), (1, Synset('imperfection.n.01')), (1, Synset('inaction.n.01')), (0, Synset('integrity.n.01')), (1, Synset('isomerism.n.01')), (0, Synset('kalemia.n.01')), (1, Synset('maturity.n.02')), (1, Synset('medium.n.08')), (1, Synset('merchantability.n.01')), (1, Synset('motion.n.04')), (1, Synset('motionlessness.n.01')), (1, Synset('nationhood.n.01')), (1, Synset('neotony.n.01')), (1, Synset('nonbeing.n.01')), (2, Synset('obligation.n.02')), (0, Synset('office.n.04')), (1, Synset('omnipotence.n.01')), (1, Synset('omniscience.n.01')), (1, Synset('order.n.03')), (1, Synset('ornamentation.n.01')), (1, Synset('ownership.n.03')), (1, Synset('paternity.n.01')), (1, Synset('perfection.n.01')), (1, Synset('plurality.n.01')), (1, Synset('polyvalence.n.01')), (2, Synset('polyvalence.n.02')), (2, Synset('readiness.n.01')), (2, Synset('receivership.n.01')), (1, Synset('relationship.n.02')), (1, Synset('relationship.n.03')), (1, Synset('representation.n.04')), (1, Synset('revocation.n.01')), (1, Synset('separation.n.01')), (1, Synset('situation.n.01')), (1, Synset('skillfulness.n.01')), (0, Synset('status.n.01')), (1, Synset('temporary_state.n.01')), (1, Synset('tribalism.n.01')), (1, Synset('turgor.n.01')), (1, Synset('unemployment.n.01')), (1, Synset('union.n.04')), (2, Synset('utilization.n.02')), (1, Synset('utopia.n.02')), (1, Synset('wild.n.01')), (0, Synset('dream.n.02')), (1, Synset('fancy.n.02')), (0, Synset('fantasy.n.01')), (1, Synset('imaginary_being.n.01')), (0, Synset('imaginary_place.n.01')), (2, Synset('aftereffect.n.02')), (1, Synset('bummer.n.02')), (2, Synset('side_effect.n.01')), (1, Synset('absence.n.01')), (0, Synset('dearth.n.01')), (0, Synset('deficit.n.02')), (0, Synset('mineral_deficiency.n.01')), (2, Synset('shortness.n.02')), (1, Synset('stringency.n.01')), (1, Synset('distinctness.n.01')), (1, Synset('translucence.n.01')), (1, Synset('transparency.n.02')), (0, Synset('visibility.n.03')), (1, Synset('compound_eye.n.01')), (3, Synset('naked_eye.n.01')), (1, Synset('oculus_dexter.n.01')), (1, Synset('oculus_sinister.n.01')), (1, Synset('peeper.n.02')), (1, Synset('simple_eye.n.01'))]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##METODO 1\n",
    "genus_for_every_label = get_genus(definition_clean,k_genus)\n",
    "synsets_genus= lesk_for_disambiguation(genus_for_every_label, definition_clean)\n",
    "hypo_for_every_genus = get_hypo(synsets_genus)\n",
    "definitions_hypo = get_definitions_hypo(hypo_for_every_genus)\n",
    "definitions_hypo_cleaning= cleaning_definition_token_hypo(definitions_hypo)\n",
    "hypo_score = token_intersection_definitions(definitions_hypo_cleaning,definition_clean)\n",
    "print(hypo_score)\n",
    "targets_words = search_best_score(hypo_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METODO 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T18:05:15.830739Z",
     "start_time": "2023-11-12T18:05:12.859393Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "##METODO 2\n",
    "genus_for_every_label_with_Score = get_genus_with_score(definition_clean,k_genus)\n",
    "synsets_genus= get_synset_genus_metodo2(genus_for_every_label_with_Score)\n",
    "synsets_genus_disambiguated = lesk_for_disambiguation_metod2(synsets_genus, definition_clean)\n",
    "hypo_for_every_genus_metodo2 = get_hypo(synsets_genus_disambiguated )\n",
    "definitions_hypo_metodo2 = get_definitions_hypo(hypo_for_every_genus_metodo2)\n",
    "definitions_hypo_cleaning_metodo2= cleaning_definition_token_hypo(definitions_hypo_metodo2)\n",
    "count_of_genus = count_of_genus_in_hypo_definition(genus_for_every_label_with_Score,definitions_hypo_cleaning_metodo2)\n",
    "targets_words_met2 = search_best_score(count_of_genus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "synsets_target_word = search_synset_target_words(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## STAMPA RISULTATI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T18:05:22.861194Z",
     "start_time": "2023-11-12T18:05:22.811139Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------------------------+---------------------------------+\n",
      "|   \u001b[34mParola   |               \u001b[32mMetodo1\u001b[0m               |             \u001b[33mMetodo2\u001b[0m             |\n",
      "+------------+-------------------------------------+---------------------------------+\n",
      "|    \u001b[34mdoor    |     \u001b[32mSynset('doorway.n.01'): 0.10\u001b[33m    |   Synset('doorway.n.01'): 0.10\u001b[0m  |\n",
      "|  \u001b[34mladybug   | \u001b[32mSynset('reddish_orange.n.01'): 0.05\u001b[33m |  Synset('good_luck.n.02'): 0.07\u001b[0m |\n",
      "|    \u001b[34mpain    |   \u001b[32mSynset('discomfort.n.02'): 0.08\u001b[33m   |    Synset('anger.n.01'): 0.08\u001b[0m   |\n",
      "| \u001b[34mblurriness |    \u001b[32mSynset('likeness.n.02'): 0.07\u001b[33m    | Synset('reflection.n.05'): 0.07\u001b[0m |\n",
      "+------------+-------------------------------------+---------------------------------+\n"
     ]
    }
   ],
   "source": [
    "table = PrettyTable()\n",
    "table.field_names = [Fore.BLUE +\"Parola\" , Fore.GREEN + \"Metodo1\" + Style.RESET_ALL, Fore.YELLOW + \"Metodo2\" + Style.RESET_ALL]\n",
    "for i,c in targets_words.items():\n",
    "\n",
    "    table.add_row([Fore.BLUE + i ,Fore.GREEN + str(targets_words[i][0][0]) + \": \"+ str(similarity_synsets(targets_words[i][0][0],synsets_target_word[i]))+ Fore.YELLOW, str(targets_words_met2[i][0][0]) + \": \"+str(similarity_synsets(targets_words_met2[i][0][0],synsets_target_word[i])) +  Style.RESET_ALL])\n",
    "  \n",
    "\n",
    "\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test lesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T18:07:47.838849Z",
     "start_time": "2023-11-12T18:07:47.710967Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENUS  insect\n",
      "GENUS SYNSETS  [Synset('insect.n.01'), Synset('worm.n.02')]\n",
      "CONTESTO {'luck', 'pattern', 'spot', 'shape', 'insect', 'harmless', 'orange', 'bug', 'yellow', 'control', 'head', 'culture', 'family', 'coat', 'person', 'color', 'fly', 'insectivore', 'round'}\n",
      "Significato identificato: Synset('worm.n.02')\n",
      "GENUS  luck\n",
      "GENUS SYNSETS  [Synset('fortune.n.04'), Synset('luck.n.02'), Synset('luck.n.03')]\n",
      "CONTESTO {'luck', 'pattern', 'spot', 'shape', 'insect', 'harmless', 'orange', 'bug', 'yellow', 'control', 'head', 'culture', 'family', 'coat', 'person', 'color', 'fly', 'insectivore', 'round'}\n",
      "Significato identificato: Synset('luck.n.03')\n",
      "GENUS  color\n",
      "GENUS SYNSETS  [Synset('color.n.01'), Synset('color.n.02'), Synset('color.n.03'), Synset('color.n.04'), Synset('semblance.n.01'), Synset('coloring_material.n.01'), Synset('color.n.07'), Synset('color.n.08'), Synset('color.v.01'), Synset('tinge.v.01'), Synset('color.v.03'), Synset('color.v.04'), Synset('color.v.05'), Synset('discolor.v.03'), Synset('color.a.01')]\n",
      "CONTESTO {'luck', 'pattern', 'spot', 'shape', 'insect', 'harmless', 'orange', 'bug', 'yellow', 'control', 'head', 'culture', 'family', 'coat', 'person', 'color', 'fly', 'insectivore', 'round'}\n",
      "Significato identificato: Synset('coloring_material.n.01')\n",
      "GENUS  round\n",
      "GENUS SYNSETS  [Synset('round.n.01'), Synset('cycle.n.01'), Synset('beat.n.01'), Synset('round.n.04'), Synset('round_of_golf.n.01'), Synset('round.n.06'), Synset('turn.n.09'), Synset('round.n.08'), Synset('round.n.09'), Synset('round.n.10'), Synset('round.n.11'), Synset('round.n.12'), Synset('rung.n.01'), Synset('circle.n.08'), Synset('round.v.01'), Synset('round.v.02'), Synset('round.v.03'), Synset('attack.v.02'), Synset('polish.v.03'), Synset('round_off.v.03'), Synset('round.v.07'), Synset('round.a.01'), Synset('orotund.s.02'), Synset('round.s.03'), Synset('round.r.01')]\n",
      "CONTESTO {'luck', 'pattern', 'spot', 'shape', 'insect', 'harmless', 'orange', 'bug', 'yellow', 'control', 'head', 'culture', 'family', 'coat', 'person', 'color', 'fly', 'insectivore', 'round'}\n",
      "Significato identificato: Synset('round_off.v.03')\n",
      "GENUS  fly\n",
      "GENUS SYNSETS  [Synset('fly.n.01'), Synset('tent-fly.n.01'), Synset('fly.n.03'), Synset('fly.n.04'), Synset('fly.n.05'), Synset('fly.v.01'), Synset('fly.v.02'), Synset('fly.v.03'), Synset('fly.v.04'), Synset('fly.v.05'), Synset('fly.v.06'), Synset('fly.v.07'), Synset('fly.v.08'), Synset('fly.v.09'), Synset('fly.v.10'), Synset('flee.v.01'), Synset('fly.v.12'), Synset('fly.v.13'), Synset('vanish.v.05'), Synset('fly.s.01')]\n",
      "CONTESTO {'luck', 'pattern', 'spot', 'shape', 'insect', 'harmless', 'orange', 'bug', 'yellow', 'control', 'head', 'culture', 'family', 'coat', 'person', 'color', 'fly', 'insectivore', 'round'}\n",
      "Significato identificato: Synset('fly.v.13')\n",
      "GENUS  coat\n",
      "GENUS SYNSETS  [Synset('coat.n.01'), Synset('coating.n.01'), Synset('coat.n.03'), Synset('coat.v.01'), Synset('coat.v.02'), Synset('coat.v.03')]\n",
      "CONTESTO {'luck', 'pattern', 'spot', 'shape', 'insect', 'harmless', 'orange', 'bug', 'yellow', 'control', 'head', 'culture', 'family', 'coat', 'person', 'color', 'fly', 'insectivore', 'round'}\n",
      "Significato identificato: Synset('coat.v.03')\n",
      "GENUS  bug\n",
      "GENUS SYNSETS  [Synset('bug.n.01'), Synset('bug.n.02'), Synset('bug.n.03'), Synset('hemipterous_insect.n.01'), Synset('microbe.n.01'), Synset('tease.v.01'), Synset('wiretap.v.01')]\n",
      "CONTESTO {'luck', 'pattern', 'spot', 'shape', 'insect', 'harmless', 'orange', 'bug', 'yellow', 'control', 'head', 'culture', 'family', 'coat', 'person', 'color', 'fly', 'insectivore', 'round'}\n",
      "Significato identificato: Synset('bug.n.01')\n",
      "GENUS  orange\n",
      "GENUS SYNSETS  [Synset('orange.n.01'), Synset('orange.n.02'), Synset('orange.n.03'), Synset('orange.n.04'), Synset('orange.n.05'), Synset('orange.s.01')]\n",
      "CONTESTO {'luck', 'pattern', 'spot', 'shape', 'insect', 'harmless', 'orange', 'bug', 'yellow', 'control', 'head', 'culture', 'family', 'coat', 'person', 'color', 'fly', 'insectivore', 'round'}\n",
      "Significato identificato: Synset('orange.n.02')\n",
      "GENUS  family\n",
      "GENUS SYNSETS  [Synset('family.n.01'), Synset('family.n.02'), Synset('class.n.01'), Synset('family.n.04'), Synset('kin.n.01'), Synset('family.n.06'), Synset('syndicate.n.01'), Synset('family.n.08')]\n",
      "CONTESTO {'luck', 'pattern', 'spot', 'shape', 'insect', 'harmless', 'orange', 'bug', 'yellow', 'control', 'head', 'culture', 'family', 'coat', 'person', 'color', 'fly', 'insectivore', 'round'}\n",
      "Significato identificato: Synset('kin.n.01')\n",
      "GENUS  insectivore\n",
      "GENUS SYNSETS  [Synset('insectivore.n.01'), Synset('insectivore.n.02')]\n",
      "CONTESTO {'luck', 'pattern', 'spot', 'shape', 'insect', 'harmless', 'orange', 'bug', 'yellow', 'control', 'head', 'culture', 'family', 'coat', 'person', 'color', 'fly', 'insectivore', 'round'}\n",
      "Significato identificato: Synset('insectivore.n.02')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for gen in genus_for_every_label[\"ladybug\"]:\n",
    "    print(\"GENUS \", gen)\n",
    "    print(\"GENUS SYNSETS \",wn.synsets(gen))\n",
    "# Definizione per \"ladybug\"\n",
    "    definition_ladybug = set(definition_clean[\"ladybug\"])\n",
    "    print(\"CONTESTO\", definition_ladybug)\n",
    "\n",
    "# Parola ambigua: \"ladybug\"\n",
    "    ambiguous_word = gen\n",
    "\n",
    "\n",
    "# Applica la funzione lesk\n",
    "    meaning = lesk(definition_ladybug, ambiguous_word)\n",
    "\n",
    "# Stampa il significato identificato\n",
    "    print(\"Significato identificato:\", meaning)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
