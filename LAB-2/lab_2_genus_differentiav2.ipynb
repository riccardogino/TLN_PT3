{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/rossellaborra/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from utils import utils as u\n",
    "\n",
    "from nltk import pos_tag\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from prettytable import PrettyTable\n",
    "from colorama import Fore, Style\n",
    "from nltk.wsd import lesk\n",
    "nltk.download('wordnet')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:05:39.037465Z",
     "start_time": "2023-11-12T17:05:38.791379Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creo la struttura dati\n",
    "\n",
    "La struttura dati utilizzata avrà la seguente forma: \n",
    "$$data[word] = [definition_i, definition_{i+1}, ..., definition_n]$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_path = '../datasets/TLN-definitions-23.tsv'\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "data = {}\n",
    "definitions = []\n",
    "for col in df.columns:\n",
    "    definitions = []\n",
    "    if col != '1':\n",
    "        data[col] = {} ## data[ladybug]\n",
    "        for riga in df[col]:\n",
    "            if col != '1':\n",
    "                definitions.append(riga)\n",
    "        data[col] = definitions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenizzazione e pulizia delle definizioni"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [],
   "source": [
    "\n",
    "def cleaning_definition_token(data):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Frase di esempio\n",
    "    words= []\n",
    "    words_for_every_label = {}\n",
    "\n",
    "    for item,content in data.items():\n",
    "        words= []\n",
    "        for definition in content:\n",
    "            tokens = word_tokenize(definition)\n",
    "            words_clean = [token.lower() for token in tokens if token.isalpha()\n",
    "                    and token.lower() not in stop_words]\n",
    "            tagged_words = pos_tag(words_clean)\n",
    "            nouns = [word for word, pos in tagged_words if pos == 'NN']\n",
    "            for n in nouns:\n",
    "                words.append(n)\n",
    "        \n",
    "        words_for_every_label[item] = words\n",
    "    return words_for_every_label"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T14:43:36.404281Z",
     "start_time": "2023-11-11T14:43:36.354934Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "outputs": [],
   "source": [
    "\n",
    "def cleaning_definition_token_hypo(data):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Frase di esempio\n",
    "    words= []\n",
    "    words_for_every_label = {}\n",
    "    list_of_tuple=[]\n",
    "    for label, content in data.items():\n",
    "        list_of_tuple=[]\n",
    "        for tupla in data[label]:\n",
    "\n",
    "            tokens = word_tokenize(tupla[1])\n",
    "            #print(tokens)\n",
    "                \n",
    "            words_clean = [token.lower() for token in tokens if token.isalpha()\n",
    "                       and token.lower() not in stop_words]\n",
    "            #print(words_clean)\n",
    "            \n",
    "            tagged_words = pos_tag(words_clean)\n",
    "            #print(tagged_words)\n",
    "            nouns = [word for word, pos in tagged_words if pos == 'NN']\n",
    "        \n",
    "            #print(nouns)\n",
    "            list_of_tuple.append((tupla[0],nouns))\n",
    "        words_for_every_label[label] = list_of_tuple\n",
    "    return words_for_every_label"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T17:41:55.898884Z",
     "start_time": "2023-11-11T17:41:55.757422Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "outputs": [],
   "source": [
    "def get_genus(words_for_every_label,k_genus):\n",
    "# Dizionario per memorizzare i primi 5 token più frequenti per ogni label\n",
    "    top_tokens_for_labels = {}\n",
    "    genus = []\n",
    "    for label, label_tokens in words_for_every_label.items():\n",
    "        genus = []\n",
    "    # Conta la frequenza di ciascun token per la label corrente\n",
    "        token_counts = Counter(label_tokens)\n",
    "\n",
    "    # Ottieni i primi 5 token più frequenti per la label corrente\n",
    "        top_tokens = token_counts.most_common(k_genus)\n",
    "\n",
    "        for t in top_tokens:\n",
    "            genus.append(t[0])\n",
    "    # Aggiungi i risultati al dizionario top_tokens_for_labels\n",
    "            top_tokens_for_labels[label] = genus\n",
    "    #for l,c in top_tokens_for_labels.items():\n",
    "        #print(l,\"--->\", top_tokens_for_labels[l])\n",
    "    return top_tokens_for_labels\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:25:21.627698Z",
     "start_time": "2023-11-12T17:25:21.608032Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "outputs": [],
   "source": [
    "def get_genus_with_score(words_for_every_label,k_genus):\n",
    "    # Dizionario per memorizzare i primi 5 token più frequenti per ogni label\n",
    "    top_tokens_for_labels = {}\n",
    "    genus = []\n",
    "    for label, label_tokens in words_for_every_label.items():\n",
    "        genus = []\n",
    "        # Conta la frequenza di ciascun token per la label corrente\n",
    "        token_counts = Counter(label_tokens)\n",
    "        # Ottieni i primi 5 token più frequenti per la label corrente\n",
    "        top_tokens = token_counts.most_common(k_genus)\n",
    "\n",
    "        for t in top_tokens:\n",
    "            genus.append((t[0],t[1]))\n",
    "            # Aggiungi i risultati al dizionario top_tokens_for_labels\n",
    "            top_tokens_for_labels[label] = genus\n",
    "    #for l,c in top_tokens_for_labels.items():\n",
    "    #print(l,\"--->\", top_tokens_for_labels[l])\n",
    "    return top_tokens_for_labels\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:25:19.818394Z",
     "start_time": "2023-11-12T17:25:19.722809Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "outputs": [],
   "source": [
    "def lesk_for_disambiguation(genus_for_every_label, definition_clean):\n",
    "    genus_best_sense = {}\n",
    "    best_senses = []\n",
    "    for label1,genus in genus_for_every_label.items():\n",
    "        #print(label1,genus)\n",
    "        best_senses = []\n",
    "        for label2,tokens in definition_clean.items():\n",
    "            #print(label2,tokens)\n",
    "            if label1 == label2:\n",
    "                #print(\"ok\")\n",
    "                \n",
    "                for gen in genus_for_every_label[label1]:\n",
    "                    if lesk(tokens,gen)  is not None:\n",
    "                        best_senses.append((gen,lesk(definition_clean[label1],gen)))\n",
    "                    \n",
    "                #print(best_sense)\n",
    "        genus_best_sense[label1] = best_senses\n",
    "    #for l,bs in genus_best_sense.items():\n",
    "        #print(l, genus_best_sense[l])\n",
    "    return genus_best_sense"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:43:59.055233Z",
     "start_time": "2023-11-12T17:43:58.973270Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "outputs": [],
   "source": [
    "def lesk_for_disambiguation_metod2(genus_for_every_label, definition_clean):\n",
    "    genus_best_sense = {}\n",
    "    best_senses = []\n",
    "    for label1,genus in genus_for_every_label.items():\n",
    "        #print(label1,genus)\n",
    "        best_senses = []\n",
    "        for label2,tokens in definition_clean.items():\n",
    "            #print(label2,tokens)\n",
    "            if label1 == label2:\n",
    "                #print(\"ok\")\n",
    "\n",
    "                for gen in genus_for_every_label[label1]:\n",
    "                    \n",
    "                    if lesk(tokens,gen[0])  is not None:\n",
    "                        \n",
    "                        best_senses.append((gen[0],lesk(definition_clean[label1],gen[0])))\n",
    "\n",
    "                #print(best_sense)\n",
    "        genus_best_sense[label1] = best_senses\n",
    "    #for l,bs in genus_best_sense.items():\n",
    "    #print(l, genus_best_sense[l])\n",
    "    return genus_best_sense"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:44:03.480605Z",
     "start_time": "2023-11-12T17:44:03.405906Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "outputs": [],
   "source": [
    "def get_synset_genus_metodo2(genus_for_every_label):\n",
    "    genus_sense = {}\n",
    "    best_senses = []\n",
    "    for label1,genus in genus_for_every_label.items():\n",
    "        best_senses = []\n",
    "        for gen in genus_for_every_label[label1]:\n",
    "             best_senses.append((gen[0],wn.synsets(gen[0])))\n",
    "\n",
    "                #print(best_sense)\n",
    "        genus_sense[label1] = best_senses\n",
    "    #for l,bs in genus_best_sense.items():\n",
    "    #print(l, genus_best_sense[l])\n",
    "    return genus_sense"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T15:20:39.767774Z",
     "start_time": "2023-11-12T15:20:39.697807Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "outputs": [],
   "source": [
    "def get_hypo(synsets_genus):\n",
    "    hyponyms_genus={}\n",
    "   \n",
    "    for l,bestsense_genus in synsets_genus.items():\n",
    "        hyponyms = []\n",
    "        #print(\"LABEL --> \",l)\n",
    "        for genus_bs in bestsense_genus:\n",
    "            #print(\"GENUS -->\",genus_bs[0],\" \",genus_bs[1],type(genus_bs[1]))\n",
    "            hyponyms_ = genus_bs[1].hyponyms()\n",
    "            #print(hyponyms_)\n",
    "            if hyponyms_ != []:\n",
    "                for h in hyponyms_ :\n",
    "                    hyponyms.append(h)\n",
    "        #print(l,bestsense,hyponyms)\n",
    "        hyponyms_genus[l]= hyponyms\n",
    "    #for l,content in hyponyms_genus.items():\n",
    "        #print(l, \"---->\",hyponyms_genus[l])\n",
    "    return hyponyms_genus"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T14:43:13.628862Z",
     "start_time": "2023-11-11T14:43:13.573402Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "outputs": [],
   "source": [
    "def get_hypo_metodo2(synsets_genus):\n",
    "    hyponyms_genus={}\n",
    "\n",
    "    for l,bestsense_genus in synsets_genus.items():\n",
    "        hyponyms = []\n",
    "        #print(\"LABEL --> \",l)\n",
    "        for genus_bs in bestsense_genus:\n",
    "            #print(\"GENUS -->\",genus_bs[0],\" \",genus_bs[1],type(genus_bs[1]))\n",
    "            syns_list_genus = genus_bs[1]\n",
    "            for s in syns_list_genus:\n",
    "                hyponyms_ = s.hyponyms()\n",
    "            #print(hyponyms_)\n",
    "                if hyponyms_ != []:\n",
    "                    for h in hyponyms_ :\n",
    "                        hyponyms.append(h)\n",
    "        #print(l,bestsense,hyponyms)\n",
    "        hyponyms_genus[l]= hyponyms\n",
    "    #for l,content in hyponyms_genus.items():\n",
    "    #print(l, \"---->\",hyponyms_genus[l])\n",
    "    return hyponyms_genus"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T15:22:26.167844Z",
     "start_time": "2023-11-12T15:22:26.087572Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "outputs": [],
   "source": [
    "def get_definitions_hypo(synsets_hypo):\n",
    "    hypo_definitions={}\n",
    " \n",
    "    for label, content in synsets_hypo.items():\n",
    "        definitions=[]\n",
    "        for synset in synsets_hypo[label]:\n",
    "            if synset.definition() is not None:\n",
    "                #for d in synset.definition():\n",
    "                definitions.append((synset,synset.definition()))\n",
    "                \n",
    "        hypo_definitions[label] = definitions\n",
    "\n",
    "  \n",
    "    #for l,content in hypo_definitions.items():\n",
    "        #print(\"----------------------------------------\")\n",
    "        #print(l, \"---->\",hypo_definitions[l])\n",
    "    return hypo_definitions\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T16:07:02.906379Z",
     "start_time": "2023-11-11T16:07:02.858149Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "outputs": [],
   "source": [
    "def token_intersection_definitions(def_hypo,def_targets):\n",
    "    synset_score={}\n",
    "    tupla_score=[]\n",
    "    for label,content in def_hypo.items():\n",
    "        tupla_score=[]\n",
    "        for token_def in def_hypo[label]:\n",
    "            interc = set(token_def[1]).intersection(set(def_targets[label]))\n",
    "            score = len(interc) \n",
    "            tupla_score.append((score,token_def[0]))\n",
    "        synset_score[label] = tupla_score\n",
    "    return synset_score\n",
    "        \n",
    "        \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T16:44:36.256297Z",
     "start_time": "2023-11-11T16:44:36.206653Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "outputs": [],
   "source": [
    "def search_best_score(hypo_score):\n",
    "    max_score = 1\n",
    "    best_synsets_list = []\n",
    "    best_synsets_score = {}\n",
    "    for label, c in hypo_score.items():\n",
    "        max_score = 0\n",
    "        best_synsets_list = []\n",
    "        for content in hypo_score[label]:\n",
    "            if  content[0] >= max_score:\n",
    "                if best_synsets_list != [] :\n",
    "                    last_elem= best_synsets_list[-1][1]\n",
    "                    if last_elem < content[0]:\n",
    "                        best_synsets_list.clear()\n",
    "                max_score = content[0]\n",
    "                best_synsets_list.append((content[1],content[0]))\n",
    "        best_synsets_score[label] =  best_synsets_list\n",
    "    return best_synsets_score\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T17:52:52.243973Z",
     "start_time": "2023-11-11T17:52:52.154252Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "outputs": [],
   "source": [
    "def count_of_genus_in_hypo_definition(genus_with_score,hypo_defs):\n",
    "    tupla3_to_append = []\n",
    "    genus_score_in_def = {}\n",
    "    count_score =0\n",
    "    for label,c in genus_with_score.items():\n",
    "        tupla3_to_append = []\n",
    "        for synset_def in hypo_defs[label]:\n",
    "            final_score= 0\n",
    "            for genus in genus_with_score[label]:\n",
    "                count_score =0\n",
    "                count_score = synset_def[1].count(genus[0]) \n",
    "                if count_score != 0:\n",
    "                    final_score = final_score + genus[1] + count_score\n",
    "            tupla3_to_append.append((final_score,synset_def[0]))\n",
    "        genus_score_in_def[label] = tupla3_to_append\n",
    "    return genus_score_in_def\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:31:02.153504Z",
     "start_time": "2023-11-12T16:31:02.106370Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "outputs": [],
   "source": [
    "def similarity_synsets(s1,s2):\n",
    "    \n",
    "\n",
    "# Calcola la similarità tra i due Synset utilizzando path_similarity\n",
    "    similarity_score = s1.path_similarity(s2)\n",
    "    similarity_score = \"{:.2f}\".format(similarity_score)\n",
    "\n",
    "# Converte la similarità in percentuale\n",
    "   \n",
    "\n",
    "    return similarity_score\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:59:17.240588Z",
     "start_time": "2023-11-12T17:59:17.192309Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "outputs": [],
   "source": [
    "def search_synset_target_words(data):\n",
    "    synset_target_word = {}\n",
    "    for label,c in data.items():\n",
    "        synset_target_word[label] = wn.synsets(label)[0]\n",
    "    return synset_target_word"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:15:11.181142Z",
     "start_time": "2023-11-12T17:15:11.145860Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metodo1\n",
      "-----\n",
      "door\n",
      "[(Synset('doorway.n.01'), 32)]\n",
      "-----\n",
      "ladybug\n",
      "[(Synset('good_luck.n.02'), 7), (Synset('reddish_orange.n.01'), 7)]\n",
      "-----\n",
      "pain\n",
      "[(Synset('anger.n.01'), 7), (Synset('hate.n.01'), 7), (Synset('quantity.n.03'), 7)]\n",
      "-----\n",
      "blurriness\n",
      "[(Synset('reflection.n.05'), 12), (Synset('dead_letter.n.01'), 12), (Synset('dependence.n.01'), 12), (Synset('obligation.n.02'), 12), (Synset('shortness.n.02'), 12)]\n"
     ]
    }
   ],
   "source": [
    "## PULIZIA DEFINIZIONI (TOKENIZZAZIONE, RIMOZIONE STOP WORDS , ESTRAZIONE DI NOMI)\n",
    "definition_clean = cleaning_definition_token(data)\n",
    "synsets_target_word = search_synset_target_words(data)\n",
    "\n",
    "\n",
    "k_genus = int(input(\"Inserisci numero di genus:\"))\n",
    "##METODO 1\n",
    "\n",
    "genus_for_every_label = get_genus(definition_clean,k_genus)\n",
    "synsets_genus= lesk_for_disambiguation(genus_for_every_label, definition_clean)\n",
    "hypo_for_every_genus = get_hypo(synsets_genus)\n",
    "definitions_hypo = get_definitions_hypo(hypo_for_every_genus)\n",
    "definitions_hypo_cleaning= cleaning_definition_token_hypo(definitions_hypo)\n",
    "hypo_score = token_intersection_definitions(definitions_hypo_cleaning,definition_clean)\n",
    "targets_words = search_best_score(hypo_score)\n",
    "\n",
    "\n",
    "##METODO 2\n",
    "genus_for_every_label_with_Score = get_genus_with_score(definition_clean,k_genus)\n",
    "synsets_genus= get_synset_genus_metodo2(genus_for_every_label_with_Score)\n",
    "synsets_genus_disambiguated = lesk_for_disambiguation_metod2(synsets_genus, definition_clean)\n",
    "hypo_for_every_genus_metodo2 = get_hypo(synsets_genus_disambiguated )\n",
    "definitions_hypo_metodo2 = get_definitions_hypo(hypo_for_every_genus_metodo2)\n",
    "definitions_hypo_cleaning_metodo2= cleaning_definition_token_hypo(definitions_hypo_metodo2)\n",
    "count_of_genus = count_of_genus_in_hypo_definition(genus_for_every_label_with_Score,definitions_hypo_cleaning_metodo2)\n",
    "targets_words_met2 = search_best_score(count_of_genus)\n",
    "\n",
    "print(\"Metodo1\")\n",
    "for i,c in targets_words_met2.items():\n",
    "    print(\"-----\")\n",
    "    print(i)\n",
    "    print(targets_words_met2[i])\n",
    "#print(\"Metodo2\")\n",
    "#for i,c in targets_words_met2.items():\n",
    "    #print(\"-----\")\n",
    "    #print(i)\n",
    "    #print(targets_words_met2[i])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T18:05:15.830739Z",
     "start_time": "2023-11-12T18:05:12.859393Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## STAMPA RISULTATI"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------------------------+---------------------------------+\n",
      "|   \u001B[34mParola   |               \u001B[32mMetodo1\u001B[0m               |             \u001B[33mMetodo2\u001B[0m             |\n",
      "+------------+-------------------------------------+---------------------------------+\n",
      "|    \u001B[34mdoor    |     \u001B[32mSynset('doorway.n.01'): 0.10\u001B[33m    |   Synset('doorway.n.01'): 0.10\u001B[0m  |\n",
      "|  \u001B[34mladybug   | \u001B[32mSynset('reddish_orange.n.01'): 0.05\u001B[33m |  Synset('good_luck.n.02'): 0.07\u001B[0m |\n",
      "|    \u001B[34mpain    |   \u001B[32mSynset('discomfort.n.02'): 0.08\u001B[33m   |    Synset('anger.n.01'): 0.08\u001B[0m   |\n",
      "| \u001B[34mblurriness |    \u001B[32mSynset('likeness.n.02'): 0.07\u001B[33m    | Synset('reflection.n.05'): 0.07\u001B[0m |\n",
      "+------------+-------------------------------------+---------------------------------+\n"
     ]
    }
   ],
   "source": [
    "table = PrettyTable()\n",
    "table.field_names = [Fore.BLUE +\"Parola\" , Fore.GREEN + \"Metodo1\" + Style.RESET_ALL, Fore.YELLOW + \"Metodo2\" + Style.RESET_ALL]\n",
    "for i,c in targets_words.items():\n",
    "\n",
    "    table.add_row([Fore.BLUE + i ,Fore.GREEN + str(targets_words[i][0][0]) + \": \"+ str(similarity_synsets(targets_words[i][0][0],synsets_target_word[i]))+ Fore.YELLOW, str(targets_words_met2[i][0][0]) + \": \"+str(similarity_synsets(targets_words_met2[i][0][0],synsets_target_word[i])) +  Style.RESET_ALL])\n",
    "  \n",
    "\n",
    "\n",
    "print(table)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T18:05:22.861194Z",
     "start_time": "2023-11-12T18:05:22.811139Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENUS  insect\n",
      "GENUS SYNSETS  [Synset('insect.n.01'), Synset('worm.n.02')]\n",
      "CONTESTO {'coat', 'orange', 'yellow', 'head', 'pattern', 'round', 'person', 'harmless', 'fly', 'shape', 'insectivore', 'luck', 'insect', 'control', 'culture', 'family', 'color', 'spot', 'bug'}\n",
      "Significato identificato: Synset('worm.n.02')\n",
      "GENUS  luck\n",
      "GENUS SYNSETS  [Synset('fortune.n.04'), Synset('luck.n.02'), Synset('luck.n.03')]\n",
      "CONTESTO {'coat', 'orange', 'yellow', 'head', 'pattern', 'round', 'person', 'harmless', 'fly', 'shape', 'insectivore', 'luck', 'insect', 'control', 'culture', 'family', 'color', 'spot', 'bug'}\n",
      "Significato identificato: Synset('luck.n.03')\n",
      "GENUS  color\n",
      "GENUS SYNSETS  [Synset('color.n.01'), Synset('color.n.02'), Synset('color.n.03'), Synset('color.n.04'), Synset('semblance.n.01'), Synset('coloring_material.n.01'), Synset('color.n.07'), Synset('color.n.08'), Synset('color.v.01'), Synset('tinge.v.01'), Synset('color.v.03'), Synset('color.v.04'), Synset('color.v.05'), Synset('discolor.v.03'), Synset('color.a.01')]\n",
      "CONTESTO {'coat', 'orange', 'yellow', 'head', 'pattern', 'round', 'person', 'harmless', 'fly', 'shape', 'insectivore', 'luck', 'insect', 'control', 'culture', 'family', 'color', 'spot', 'bug'}\n",
      "Significato identificato: Synset('coloring_material.n.01')\n",
      "GENUS  round\n",
      "GENUS SYNSETS  [Synset('round.n.01'), Synset('cycle.n.01'), Synset('beat.n.01'), Synset('round.n.04'), Synset('round_of_golf.n.01'), Synset('round.n.06'), Synset('turn.n.09'), Synset('round.n.08'), Synset('round.n.09'), Synset('round.n.10'), Synset('round.n.11'), Synset('round.n.12'), Synset('rung.n.01'), Synset('circle.n.08'), Synset('round.v.01'), Synset('round.v.02'), Synset('round.v.03'), Synset('attack.v.02'), Synset('polish.v.03'), Synset('round_off.v.03'), Synset('round.v.07'), Synset('round.a.01'), Synset('orotund.s.02'), Synset('round.s.03'), Synset('round.r.01')]\n",
      "CONTESTO {'coat', 'orange', 'yellow', 'head', 'pattern', 'round', 'person', 'harmless', 'fly', 'shape', 'insectivore', 'luck', 'insect', 'control', 'culture', 'family', 'color', 'spot', 'bug'}\n",
      "Significato identificato: Synset('round_off.v.03')\n",
      "GENUS  fly\n",
      "GENUS SYNSETS  [Synset('fly.n.01'), Synset('tent-fly.n.01'), Synset('fly.n.03'), Synset('fly.n.04'), Synset('fly.n.05'), Synset('fly.v.01'), Synset('fly.v.02'), Synset('fly.v.03'), Synset('fly.v.04'), Synset('fly.v.05'), Synset('fly.v.06'), Synset('fly.v.07'), Synset('fly.v.08'), Synset('fly.v.09'), Synset('fly.v.10'), Synset('flee.v.01'), Synset('fly.v.12'), Synset('fly.v.13'), Synset('vanish.v.05'), Synset('fly.s.01')]\n",
      "CONTESTO {'coat', 'orange', 'yellow', 'head', 'pattern', 'round', 'person', 'harmless', 'fly', 'shape', 'insectivore', 'luck', 'insect', 'control', 'culture', 'family', 'color', 'spot', 'bug'}\n",
      "Significato identificato: Synset('fly.v.13')\n",
      "GENUS  coat\n",
      "GENUS SYNSETS  [Synset('coat.n.01'), Synset('coating.n.01'), Synset('coat.n.03'), Synset('coat.v.01'), Synset('coat.v.02'), Synset('coat.v.03')]\n",
      "CONTESTO {'coat', 'orange', 'yellow', 'head', 'pattern', 'round', 'person', 'harmless', 'fly', 'shape', 'insectivore', 'luck', 'insect', 'control', 'culture', 'family', 'color', 'spot', 'bug'}\n",
      "Significato identificato: Synset('coat.v.03')\n",
      "GENUS  bug\n",
      "GENUS SYNSETS  [Synset('bug.n.01'), Synset('bug.n.02'), Synset('bug.n.03'), Synset('hemipterous_insect.n.01'), Synset('microbe.n.01'), Synset('tease.v.01'), Synset('wiretap.v.01')]\n",
      "CONTESTO {'coat', 'orange', 'yellow', 'head', 'pattern', 'round', 'person', 'harmless', 'fly', 'shape', 'insectivore', 'luck', 'insect', 'control', 'culture', 'family', 'color', 'spot', 'bug'}\n",
      "Significato identificato: Synset('bug.n.01')\n",
      "GENUS  orange\n",
      "GENUS SYNSETS  [Synset('orange.n.01'), Synset('orange.n.02'), Synset('orange.n.03'), Synset('orange.n.04'), Synset('orange.n.05'), Synset('orange.s.01')]\n",
      "CONTESTO {'coat', 'orange', 'yellow', 'head', 'pattern', 'round', 'person', 'harmless', 'fly', 'shape', 'insectivore', 'luck', 'insect', 'control', 'culture', 'family', 'color', 'spot', 'bug'}\n",
      "Significato identificato: Synset('orange.n.02')\n",
      "GENUS  family\n",
      "GENUS SYNSETS  [Synset('family.n.01'), Synset('family.n.02'), Synset('class.n.01'), Synset('family.n.04'), Synset('kin.n.01'), Synset('family.n.06'), Synset('syndicate.n.01'), Synset('family.n.08')]\n",
      "CONTESTO {'coat', 'orange', 'yellow', 'head', 'pattern', 'round', 'person', 'harmless', 'fly', 'shape', 'insectivore', 'luck', 'insect', 'control', 'culture', 'family', 'color', 'spot', 'bug'}\n",
      "Significato identificato: Synset('kin.n.01')\n",
      "GENUS  insectivore\n",
      "GENUS SYNSETS  [Synset('insectivore.n.01'), Synset('insectivore.n.02')]\n",
      "CONTESTO {'coat', 'orange', 'yellow', 'head', 'pattern', 'round', 'person', 'harmless', 'fly', 'shape', 'insectivore', 'luck', 'insect', 'control', 'culture', 'family', 'color', 'spot', 'bug'}\n",
      "Significato identificato: Synset('insectivore.n.02')\n",
      "GENUS  control\n",
      "GENUS SYNSETS  [Synset('control.n.01'), Synset('control.n.02'), Synset('control.n.03'), Synset('control_condition.n.01'), Synset('control.n.05'), Synset('dominance.n.02'), Synset('restraint.n.02'), Synset('command.n.06'), Synset('control.n.09'), Synset('control.n.10'), Synset('control.n.11'), Synset('control.v.01'), Synset('control.v.02'), Synset('operate.v.03'), Synset('manipulate.v.05'), Synset('control.v.05'), Synset('control.v.06'), Synset('see.v.10'), Synset('master.v.04')]\n",
      "CONTESTO {'coat', 'orange', 'yellow', 'head', 'pattern', 'round', 'person', 'harmless', 'fly', 'shape', 'insectivore', 'luck', 'insect', 'control', 'culture', 'family', 'color', 'spot', 'bug'}\n",
      "Significato identificato: Synset('manipulate.v.05')\n",
      "GENUS  head\n",
      "GENUS SYNSETS  [Synset('head.n.01'), Synset('head.n.02'), Synset('mind.n.01'), Synset('head.n.04'), Synset('head.n.05'), Synset('head.n.06'), Synset('head.n.07'), Synset('fountainhead.n.02'), Synset('head.n.09'), Synset('head.n.10'), Synset('head.n.11'), Synset('capitulum.n.01'), Synset('principal.n.02'), Synset('head.n.14'), Synset('head.n.15'), Synset('promontory.n.01'), Synset('head.n.17'), Synset('head.n.18'), Synset('forefront.n.01'), Synset('pass.n.09'), Synset('headway.n.02'), Synset('point.n.20'), Synset('question.n.02'), Synset('heading.n.01'), Synset('head.n.25'), Synset('head.n.26'), Synset('read/write_head.n.01'), Synset('head.n.28'), Synset('head.n.29'), Synset('head.n.30'), Synset('head.n.31'), Synset('drumhead.n.01'), Synset('oral_sex.n.01'), Synset('head.v.01'), Synset('head.v.02'), Synset('lead.v.04'), Synset('head.v.04'), Synset('steer.v.01'), Synset('head.v.06'), Synset('head.v.07'), Synset('head.v.08'), Synset('head.v.09')]\n",
      "CONTESTO {'coat', 'orange', 'yellow', 'head', 'pattern', 'round', 'person', 'harmless', 'fly', 'shape', 'insectivore', 'luck', 'insect', 'control', 'culture', 'family', 'color', 'spot', 'bug'}\n",
      "Significato identificato: Synset('head.v.09')\n",
      "GENUS  culture\n",
      "GENUS SYNSETS  [Synset('culture.n.01'), Synset('culture.n.02'), Synset('acculturation.n.02'), Synset('culture.n.04'), Synset('polish.n.02'), Synset('culture.n.06'), Synset('culture.n.07'), Synset('culture.v.01')]\n",
      "CONTESTO {'coat', 'orange', 'yellow', 'head', 'pattern', 'round', 'person', 'harmless', 'fly', 'shape', 'insectivore', 'luck', 'insect', 'control', 'culture', 'family', 'color', 'spot', 'bug'}\n",
      "Significato identificato: Synset('polish.n.02')\n",
      "GENUS  yellow\n",
      "GENUS SYNSETS  [Synset('yellow.n.01'), Synset('yellow.v.01'), Synset('yellow.s.01'), Synset('chicken.s.01'), Synset('yellow.s.03'), Synset('scandalmongering.s.01'), Synset('yellow.s.05'), Synset('jaundiced.s.01')]\n",
      "CONTESTO {'coat', 'orange', 'yellow', 'head', 'pattern', 'round', 'person', 'harmless', 'fly', 'shape', 'insectivore', 'luck', 'insect', 'control', 'culture', 'family', 'color', 'spot', 'bug'}\n",
      "Significato identificato: Synset('yellow.s.01')\n",
      "GENUS  harmless\n",
      "GENUS SYNSETS  [Synset('harmless.a.01')]\n",
      "CONTESTO {'coat', 'orange', 'yellow', 'head', 'pattern', 'round', 'person', 'harmless', 'fly', 'shape', 'insectivore', 'luck', 'insect', 'control', 'culture', 'family', 'color', 'spot', 'bug'}\n",
      "Significato identificato: Synset('harmless.a.01')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for gen in genus_for_every_label[\"ladybug\"]:\n",
    "    print(\"GENUS \", gen)\n",
    "    print(\"GENUS SYNSETS \",wn.synsets(gen))\n",
    "# Definizione per \"ladybug\"\n",
    "    definition_ladybug = set(definition_clean[\"ladybug\"])\n",
    "    print(\"CONTESTO\", definition_ladybug)\n",
    "\n",
    "# Parola ambigua: \"ladybug\"\n",
    "    ambiguous_word = gen\n",
    "\n",
    "\n",
    "# Applica la funzione lesk\n",
    "    meaning = lesk(definition_ladybug, ambiguous_word)\n",
    "\n",
    "# Stampa il significato identificato\n",
    "    print(\"Significato identificato:\", meaning)\n",
    "#print(s)\n",
    "#print(s[0].hyponyms())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T18:07:47.838849Z",
     "start_time": "2023-11-12T18:07:47.710967Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from nltk.wsd import lesk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Definizione per \"ladybug\"\n",
    "definition_ladybug = definition_clean[\"ladybug\"]\n",
    "\n",
    "# Parola ambigua: \"ladybug\"\n",
    "ambiguous_word = \"ladybug\"\n",
    "\n",
    "# Contesto come definizione della parola\n",
    "context_sentence = ' '.join(definition_ladybug)\n",
    "\n",
    "# Tokenizza la frase di definizione\n",
    "tokenized_context = word_tokenize(context_sentence)\n",
    "\n",
    "# Applica la funzione lesk\n",
    "meaning = lesk(tokenized_context, ambiguous_word)\n",
    "\n",
    "# Stampa il significato identificato\n",
    "print(\"Significato identificato:\", meaning)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
