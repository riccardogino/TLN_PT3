{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T17:05:39.037465Z",
     "start_time": "2023-11-12T17:05:38.791379Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [Errno 11001] getaddrinfo failed>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk import pos_tag\n",
    "sys.path.append('..')\n",
    "nltk.download('stopwords')\n",
    "from utils import utils as u\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import Counter\n",
    "from prettytable import PrettyTable\n",
    "from colorama import Fore, Style\n",
    "from nltk.wsd import lesk\n",
    "nltk.download('wordnet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Creo la struttura dati\n",
    "\n",
    "La struttura dati utilizzata avrÃ  la seguente forma: \n",
    "$$data[word] = [definition_i, definition_{i+1}, ..., definition_n]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_path = '../datasets/TLN-definitions-23.tsv'\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "data = {}\n",
    "definitions = []\n",
    "for col in df.columns:\n",
    "    definitions = []\n",
    "    if col != '1':\n",
    "        data[col] = {} ## data[ladybug]\n",
    "        for riga in df[col]:\n",
    "            if col != '1':\n",
    "                definitions.append(riga)\n",
    "        data[col] = definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Tokenizzazione e pulizia delle definizioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T14:43:36.404281Z",
     "start_time": "2023-11-11T14:43:36.354934Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def cleaning_definition_token(data):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Frase di esempio\n",
    "    words= []\n",
    "    words_for_every_label = {}\n",
    "\n",
    "    for item,content in data.items():\n",
    "        words= []\n",
    "        for definition in content:\n",
    "            tokens = word_tokenize(definition)\n",
    "            words_clean = [token.lower() for token in tokens if token.isalpha()\n",
    "                    and token.lower() not in stop_words]\n",
    "            tagged_words = pos_tag(words_clean)\n",
    "            nouns = [word for word, pos in tagged_words if pos == 'NN']\n",
    "            for n in nouns:\n",
    "                words.append(n)\n",
    "        \n",
    "        words_for_every_label[item] = words\n",
    "    return words_for_every_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizzazione e pulizia delle definizioni degli iponimi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T17:41:55.898884Z",
     "start_time": "2023-11-11T17:41:55.757422Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def cleaning_definition_token_hypo(data):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    words= []\n",
    "    words_for_every_label = {}\n",
    "    list_of_tuple=[]\n",
    "    for label, content in data.items():\n",
    "        list_of_tuple=[]\n",
    "        for tupla in data[label]:\n",
    "\n",
    "            tokens = word_tokenize(tupla[1])\n",
    "                \n",
    "            words_clean = [token.lower() for token in tokens if token.isalpha()\n",
    "                       and token.lower() not in stop_words]\n",
    "            \n",
    "            tagged_words = pos_tag(words_clean)\n",
    "          \n",
    "            nouns = [word for word, pos in tagged_words if pos == 'NN']\n",
    "        \n",
    "            list_of_tuple.append((tupla[0],nouns))\n",
    "        words_for_every_label[label] = list_of_tuple\n",
    "    return words_for_every_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ricerca genus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T17:25:21.627698Z",
     "start_time": "2023-11-12T17:25:21.608032Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_genus(words_for_every_label,k_genus):\n",
    "\n",
    "    top_tokens_for_labels = {}\n",
    "    genus = []\n",
    "    for label, label_tokens in words_for_every_label.items():\n",
    "        genus = []\n",
    " \n",
    "        token_counts = Counter(label_tokens)\n",
    "\n",
    "        top_tokens = token_counts.most_common(k_genus)\n",
    "\n",
    "        for t in top_tokens:\n",
    "            genus.append(t[0])\n",
    "  \n",
    "            top_tokens_for_labels[label] = genus\n",
    " \n",
    "    return top_tokens_for_labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ricerca dei genus, mantenendo il numero di frequenza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T17:25:19.818394Z",
     "start_time": "2023-11-12T17:25:19.722809Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_genus_with_score(words_for_every_label,k_genus):\n",
    "   \n",
    "    top_tokens_for_labels = {}\n",
    "    genus = []\n",
    "    for label, label_tokens in words_for_every_label.items():\n",
    "        genus = []\n",
    "        token_counts = Counter(label_tokens)\n",
    "        top_tokens = token_counts.most_common(k_genus)\n",
    "\n",
    "        for t in top_tokens:\n",
    "            genus.append((t[0],t[1]))\n",
    "            top_tokens_for_labels[label] = genus\n",
    "    return top_tokens_for_labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Algoritmo di lesk per la WSD dei genus trovati metodo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T17:43:59.055233Z",
     "start_time": "2023-11-12T17:43:58.973270Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lesk_for_disambiguation(genus_for_every_label, definition_clean):\n",
    "    genus_best_sense = {}\n",
    "    best_senses = []\n",
    "    for label1,genus in genus_for_every_label.items():\n",
    "        best_senses = []\n",
    "        for label2,tokens in definition_clean.items():\n",
    "            if label1 == label2:\n",
    "                for gen in genus_for_every_label[label1]:\n",
    "                    if lesk(tokens,gen)  is not None:\n",
    "                        best_senses.append((gen,lesk(definition_clean[label1],gen)))\n",
    "        genus_best_sense[label1] = best_senses\n",
    "    return genus_best_sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo di lesk per la WSD dei genus trovati metodo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T17:44:03.480605Z",
     "start_time": "2023-11-12T17:44:03.405906Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lesk_for_disambiguation_metod2(genus_for_every_label, definition_clean):\n",
    "    genus_best_sense = {}\n",
    "    best_senses = []\n",
    "    for label1,genus in genus_for_every_label.items():\n",
    "        best_senses = []\n",
    "        for label2,tokens in definition_clean.items():\n",
    "            if label1 == label2:\n",
    "                for gen in genus_for_every_label[label1]:\n",
    "    \n",
    "                    if lesk(tokens,gen[0])  is not None:\n",
    "                        \n",
    "                        best_senses.append((gen[0],lesk(definition_clean[label1],gen[0])))\n",
    "\n",
    "        genus_best_sense[label1] = best_senses\n",
    " \n",
    "    return genus_best_sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ricerca dei genus (metodo 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T15:20:39.767774Z",
     "start_time": "2023-11-12T15:20:39.697807Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_synset_genus_metodo2(genus_for_every_label):\n",
    "    genus_sense = {}\n",
    "    best_senses = []\n",
    "    for label1,genus in genus_for_every_label.items():\n",
    "        best_senses = []\n",
    "        for gen in genus_for_every_label[label1]:\n",
    "             best_senses.append((gen[0],wn.synsets(gen[0])))\n",
    "\n",
    "        genus_sense[label1] = best_senses\n",
    "  \n",
    "    return genus_sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ricerca degli iponimi fino alle foglie dell'albero di wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_hyponyms(genus_synset):\n",
    "    stak = []\n",
    "    hy = []\n",
    "\n",
    "    for g in genus_synset:\n",
    "        if g.name().split('.')[1] == 'n':\n",
    "            stak.append(g)\n",
    "\n",
    "    while len(stak) != 0:\n",
    "        hypo = stak.pop()\n",
    "\n",
    "        new_hyponims = hypo.hyponyms()\n",
    "\n",
    "        for h in new_hyponims:\n",
    "            if h not in stak and h.name().split('.')[1] == 'n':\n",
    "                stak.append(h)\n",
    "\n",
    "\n",
    "        hy.append(hypo)\n",
    "   \n",
    "\n",
    "    \n",
    "    return hy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ricerca iponimi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T14:43:13.628862Z",
     "start_time": "2023-11-11T14:43:13.573402Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_hypo(synsets_genus):\n",
    "    hyponyms_genus={}\n",
    "\n",
    "    for l,bestsense_genus in synsets_genus.items():\n",
    "        hyponyms = []\n",
    "        for genus_bs in bestsense_genus:\n",
    "            \n",
    "            #hyponyms_ = get_all_hyponyms(genus_bs[1].hyponyms())\n",
    "            hyponyms_ = genus_bs[1].hyponyms()\n",
    "            if hyponyms_ != []:\n",
    "                for h in hyponyms_ :\n",
    "                    hyponyms.append(h)\n",
    "       \n",
    "        hyponyms_genus[l]= hyponyms\n",
    "    return hyponyms_genus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ricerca delle definizioni degli iponimi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T16:07:02.906379Z",
     "start_time": "2023-11-11T16:07:02.858149Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_definitions_hypo(synsets_hypo):\n",
    "    hypo_definitions={}\n",
    " \n",
    "    for label, content in synsets_hypo.items():\n",
    "        definitions=[]\n",
    "        for synset in synsets_hypo[label]:\n",
    "            if synset.definition() is not None:\n",
    "                definitions.append((synset,synset.definition()))\n",
    "                \n",
    "        hypo_definitions[label] = definitions\n",
    "\n",
    "\n",
    "    return hypo_definitions\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intersezione tra definizioni target e definizioni degli iponimi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T16:44:36.256297Z",
     "start_time": "2023-11-11T16:44:36.206653Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def token_intersection_definitions(def_hypo,def_targets):\n",
    "    synset_score={}\n",
    "    tupla_score=[]\n",
    "    for label,content in def_hypo.items():\n",
    "        tupla_score=[]\n",
    "        for token_def in def_hypo[label]:\n",
    "            \n",
    "            interc = set(token_def[1]).intersection(set(def_targets[label]))\n",
    "            score = len(interc) \n",
    "            tupla_score.append((score,token_def[0]))\n",
    "        synset_score[label] = tupla_score\n",
    "    return synset_score\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ricerca del best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T17:52:52.243973Z",
     "start_time": "2023-11-11T17:52:52.154252Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def search_best_score(hypo_score):\n",
    "    max_score = 1\n",
    "    best_synsets_list = []\n",
    "    best_synsets_score = {}\n",
    "    for label, c in hypo_score.items():\n",
    "        max_score = 0\n",
    "        best_synsets_list = []\n",
    "        for content in hypo_score[label]:\n",
    "            if  content[0] >= max_score:\n",
    "                if best_synsets_list != [] :\n",
    "                    last_elem= best_synsets_list[-1][1]\n",
    "                    if last_elem < content[0]:\n",
    "                        best_synsets_list.clear()\n",
    "                max_score = content[0]\n",
    "                best_synsets_list.append((content[1],content[0]))\n",
    "        best_synsets_score[label] =  best_synsets_list\n",
    "    return best_synsets_score\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conteggio dei genus all'interno delle definizioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T16:31:02.153504Z",
     "start_time": "2023-11-12T16:31:02.106370Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_of_genus_in_hypo_definition(genus_with_score,hypo_defs):\n",
    "    tupla3_to_append = []\n",
    "    genus_score_in_def = {}\n",
    "    count_score =0\n",
    "    for label,c in genus_with_score.items():\n",
    "        tupla3_to_append = []\n",
    "        for synset_def in hypo_defs[label]:\n",
    "            final_score= 0\n",
    "            for genus in genus_with_score[label]:\n",
    "                count_score =0\n",
    "                count_score = synset_def[1].count(genus[0]) \n",
    "                if count_score != 0:\n",
    "                    final_score = final_score + genus[1] + count_score\n",
    "            tupla3_to_append.append((final_score,synset_def[0]))\n",
    "        genus_score_in_def[label] = tupla3_to_append\n",
    "    return genus_score_in_def\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcolo della distanza nell'albero di wordnet tra la parola target e la parola trovata dall'algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T17:59:17.240588Z",
     "start_time": "2023-11-12T17:59:17.192309Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def similarity_synsets(s1,s2):\n",
    "    \n",
    "\n",
    "    similarity_score = s1.path_similarity(s2)\n",
    "    similarity_score = \"{:.2f}\".format(similarity_score)\n",
    "   \n",
    "    return similarity_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T17:15:11.181142Z",
     "start_time": "2023-11-12T17:15:11.145860Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def search_synset_target_words(data):\n",
    "    synset_target_word = {}\n",
    "    for label,c in data.items():\n",
    "        synset_target_word[label] = wn.synsets(label)[0]\n",
    "    return synset_target_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulizia definizioni e concatenazione di tutti i nomi contenuti nell'insieme delle definizioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "definition_clean = cleaning_definition_token(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numero di genus in input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_genus = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METODO 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##METODO 1\n",
    "genus_for_every_label = get_genus(definition_clean,k_genus)\n",
    "synsets_genus= lesk_for_disambiguation(genus_for_every_label, definition_clean)\n",
    "hypo_for_every_genus = get_hypo(synsets_genus)\n",
    "definitions_hypo = get_definitions_hypo(hypo_for_every_genus)\n",
    "definitions_hypo_cleaning= cleaning_definition_token_hypo(definitions_hypo)\n",
    "hypo_score = token_intersection_definitions(definitions_hypo_cleaning,definition_clean)\n",
    "targets_words = search_best_score(hypo_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METODO 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T18:05:15.830739Z",
     "start_time": "2023-11-12T18:05:12.859393Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "##METODO 2\n",
    "genus_for_every_label_with_Score = get_genus_with_score(definition_clean,k_genus)\n",
    "synsets_genus= get_synset_genus_metodo2(genus_for_every_label_with_Score)\n",
    "synsets_genus_disambiguated = lesk_for_disambiguation_metod2(synsets_genus, definition_clean)\n",
    "hypo_for_every_genus_metodo2 = get_hypo(synsets_genus_disambiguated )\n",
    "definitions_hypo_metodo2 = get_definitions_hypo(hypo_for_every_genus_metodo2)\n",
    "definitions_hypo_cleaning_metodo2= cleaning_definition_token_hypo(definitions_hypo_metodo2)\n",
    "count_of_genus = count_of_genus_in_hypo_definition(genus_for_every_label_with_Score,definitions_hypo_cleaning_metodo2)\n",
    "targets_words_met2 = search_best_score(count_of_genus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "synsets_target_word = search_synset_target_words(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## STAMPA RISULTATI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T18:05:22.861194Z",
     "start_time": "2023-11-12T18:05:22.811139Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------------------------+---------------------------------+\n",
      "|   \u001b[34mParola   |               \u001b[32mMetodo1\u001b[0m               |             \u001b[33mMetodo2\u001b[0m             |\n",
      "+------------+-------------------------------------+---------------------------------+\n",
      "|    \u001b[34mdoor    |     \u001b[32mSynset('doorway.n.01'): 0.10\u001b[33m    |   Synset('doorway.n.01'): 0.10\u001b[0m  |\n",
      "|  \u001b[34mladybug   | \u001b[32mSynset('reddish_orange.n.01'): 0.05\u001b[33m |  Synset('good_luck.n.02'): 0.07\u001b[0m |\n",
      "|    \u001b[34mpain    |   \u001b[32mSynset('discomfort.n.02'): 0.08\u001b[33m   |    Synset('anger.n.01'): 0.08\u001b[0m   |\n",
      "| \u001b[34mblurriness |    \u001b[32mSynset('likeness.n.02'): 0.07\u001b[33m    | Synset('reflection.n.05'): 0.07\u001b[0m |\n",
      "+------------+-------------------------------------+---------------------------------+\n"
     ]
    }
   ],
   "source": [
    "table = PrettyTable()\n",
    "table.field_names = [Fore.BLUE +\"Parola\" , Fore.GREEN + \"Metodo1\" + Style.RESET_ALL, Fore.YELLOW + \"Metodo2\" + Style.RESET_ALL]\n",
    "for i,c in targets_words.items():\n",
    "\n",
    "    table.add_row([Fore.BLUE + i ,Fore.GREEN + str(targets_words[i][0][0]) + \": \"+ str(similarity_synsets(targets_words[i][0][0],synsets_target_word[i]))+ Fore.YELLOW, str(targets_words_met2[i][0][0]) + \": \"+str(similarity_synsets(targets_words_met2[i][0][0],synsets_target_word[i])) +  Style.RESET_ALL])\n",
    "  \n",
    "\n",
    "\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test lesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T18:07:47.838849Z",
     "start_time": "2023-11-12T18:07:47.710967Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENUS  insect\n",
      "GENUS SYNSETS  [Synset('insect.n.01'), Synset('worm.n.02')]\n",
      "CONTESTO {'luck', 'pattern', 'spot', 'shape', 'insect', 'harmless', 'orange', 'bug', 'yellow', 'control', 'head', 'culture', 'family', 'coat', 'person', 'color', 'fly', 'insectivore', 'round'}\n",
      "Significato identificato: Synset('worm.n.02')\n",
      "GENUS  luck\n",
      "GENUS SYNSETS  [Synset('fortune.n.04'), Synset('luck.n.02'), Synset('luck.n.03')]\n",
      "CONTESTO {'luck', 'pattern', 'spot', 'shape', 'insect', 'harmless', 'orange', 'bug', 'yellow', 'control', 'head', 'culture', 'family', 'coat', 'person', 'color', 'fly', 'insectivore', 'round'}\n",
      "Significato identificato: Synset('luck.n.03')\n",
      "GENUS  color\n",
      "GENUS SYNSETS  [Synset('color.n.01'), Synset('color.n.02'), Synset('color.n.03'), Synset('color.n.04'), Synset('semblance.n.01'), Synset('coloring_material.n.01'), Synset('color.n.07'), Synset('color.n.08'), Synset('color.v.01'), Synset('tinge.v.01'), Synset('color.v.03'), Synset('color.v.04'), Synset('color.v.05'), Synset('discolor.v.03'), Synset('color.a.01')]\n",
      "CONTESTO {'luck', 'pattern', 'spot', 'shape', 'insect', 'harmless', 'orange', 'bug', 'yellow', 'control', 'head', 'culture', 'family', 'coat', 'person', 'color', 'fly', 'insectivore', 'round'}\n",
      "Significato identificato: Synset('coloring_material.n.01')\n",
      "GENUS  round\n",
      "GENUS SYNSETS  [Synset('round.n.01'), Synset('cycle.n.01'), Synset('beat.n.01'), Synset('round.n.04'), Synset('round_of_golf.n.01'), Synset('round.n.06'), Synset('turn.n.09'), Synset('round.n.08'), Synset('round.n.09'), Synset('round.n.10'), Synset('round.n.11'), Synset('round.n.12'), Synset('rung.n.01'), Synset('circle.n.08'), Synset('round.v.01'), Synset('round.v.02'), Synset('round.v.03'), Synset('attack.v.02'), Synset('polish.v.03'), Synset('round_off.v.03'), Synset('round.v.07'), Synset('round.a.01'), Synset('orotund.s.02'), Synset('round.s.03'), Synset('round.r.01')]\n",
      "CONTESTO {'luck', 'pattern', 'spot', 'shape', 'insect', 'harmless', 'orange', 'bug', 'yellow', 'control', 'head', 'culture', 'family', 'coat', 'person', 'color', 'fly', 'insectivore', 'round'}\n",
      "Significato identificato: Synset('round_off.v.03')\n",
      "GENUS  fly\n",
      "GENUS SYNSETS  [Synset('fly.n.01'), Synset('tent-fly.n.01'), Synset('fly.n.03'), Synset('fly.n.04'), Synset('fly.n.05'), Synset('fly.v.01'), Synset('fly.v.02'), Synset('fly.v.03'), Synset('fly.v.04'), Synset('fly.v.05'), Synset('fly.v.06'), Synset('fly.v.07'), Synset('fly.v.08'), Synset('fly.v.09'), Synset('fly.v.10'), Synset('flee.v.01'), Synset('fly.v.12'), Synset('fly.v.13'), Synset('vanish.v.05'), Synset('fly.s.01')]\n",
      "CONTESTO {'luck', 'pattern', 'spot', 'shape', 'insect', 'harmless', 'orange', 'bug', 'yellow', 'control', 'head', 'culture', 'family', 'coat', 'person', 'color', 'fly', 'insectivore', 'round'}\n",
      "Significato identificato: Synset('fly.v.13')\n",
      "GENUS  coat\n",
      "GENUS SYNSETS  [Synset('coat.n.01'), Synset('coating.n.01'), Synset('coat.n.03'), Synset('coat.v.01'), Synset('coat.v.02'), Synset('coat.v.03')]\n",
      "CONTESTO {'luck', 'pattern', 'spot', 'shape', 'insect', 'harmless', 'orange', 'bug', 'yellow', 'control', 'head', 'culture', 'family', 'coat', 'person', 'color', 'fly', 'insectivore', 'round'}\n",
      "Significato identificato: Synset('coat.v.03')\n",
      "GENUS  bug\n",
      "GENUS SYNSETS  [Synset('bug.n.01'), Synset('bug.n.02'), Synset('bug.n.03'), Synset('hemipterous_insect.n.01'), Synset('microbe.n.01'), Synset('tease.v.01'), Synset('wiretap.v.01')]\n",
      "CONTESTO {'luck', 'pattern', 'spot', 'shape', 'insect', 'harmless', 'orange', 'bug', 'yellow', 'control', 'head', 'culture', 'family', 'coat', 'person', 'color', 'fly', 'insectivore', 'round'}\n",
      "Significato identificato: Synset('bug.n.01')\n",
      "GENUS  orange\n",
      "GENUS SYNSETS  [Synset('orange.n.01'), Synset('orange.n.02'), Synset('orange.n.03'), Synset('orange.n.04'), Synset('orange.n.05'), Synset('orange.s.01')]\n",
      "CONTESTO {'luck', 'pattern', 'spot', 'shape', 'insect', 'harmless', 'orange', 'bug', 'yellow', 'control', 'head', 'culture', 'family', 'coat', 'person', 'color', 'fly', 'insectivore', 'round'}\n",
      "Significato identificato: Synset('orange.n.02')\n",
      "GENUS  family\n",
      "GENUS SYNSETS  [Synset('family.n.01'), Synset('family.n.02'), Synset('class.n.01'), Synset('family.n.04'), Synset('kin.n.01'), Synset('family.n.06'), Synset('syndicate.n.01'), Synset('family.n.08')]\n",
      "CONTESTO {'luck', 'pattern', 'spot', 'shape', 'insect', 'harmless', 'orange', 'bug', 'yellow', 'control', 'head', 'culture', 'family', 'coat', 'person', 'color', 'fly', 'insectivore', 'round'}\n",
      "Significato identificato: Synset('kin.n.01')\n",
      "GENUS  insectivore\n",
      "GENUS SYNSETS  [Synset('insectivore.n.01'), Synset('insectivore.n.02')]\n",
      "CONTESTO {'luck', 'pattern', 'spot', 'shape', 'insect', 'harmless', 'orange', 'bug', 'yellow', 'control', 'head', 'culture', 'family', 'coat', 'person', 'color', 'fly', 'insectivore', 'round'}\n",
      "Significato identificato: Synset('insectivore.n.02')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for gen in genus_for_every_label[\"ladybug\"]:\n",
    "    print(\"GENUS \", gen)\n",
    "    print(\"GENUS SYNSETS \",wn.synsets(gen))\n",
    "# Definizione per \"ladybug\"\n",
    "    definition_ladybug = set(definition_clean[\"ladybug\"])\n",
    "    print(\"CONTESTO\", definition_ladybug)\n",
    "\n",
    "# Parola ambigua: \"ladybug\"\n",
    "    ambiguous_word = gen\n",
    "\n",
    "\n",
    "# Applica la funzione lesk\n",
    "    meaning = lesk(definition_ladybug, ambiguous_word)\n",
    "\n",
    "# Stampa il significato identificato\n",
    "    print(\"Significato identificato:\", meaning)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
