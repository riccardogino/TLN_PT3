{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T17:05:39.037465Z",
     "start_time": "2023-11-12T17:05:38.791379Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\r.borra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\r.borra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\r.borra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\r.borra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk import pos_tag\n",
    "sys.path.append('..')\n",
    "nltk.download('stopwords')\n",
    "from utils import utils as u\n",
    "\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from prettytable import PrettyTable\n",
    "from colorama import Fore, Style\n",
    "from nltk.wsd import lesk\n",
    "nltk.download('wordnet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Creo la struttura dati\n",
    "\n",
    "La struttura dati utilizzata avrà la seguente forma: \n",
    "$$data[word] = [definition_i, definition_{i+1}, ..., definition_n]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_path = '../datasets/TLN-definitions-23.tsv'\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "data = {}\n",
    "definitions = []\n",
    "for col in df.columns:\n",
    "    definitions = []\n",
    "    if col != '1':\n",
    "        data[col] = {} ## data[ladybug]\n",
    "        for riga in df[col]:\n",
    "            if col != '1':\n",
    "                definitions.append(riga)\n",
    "        data[col] = definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Tokenizzazione e pulizia delle definizioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T14:43:36.404281Z",
     "start_time": "2023-11-11T14:43:36.354934Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def cleaning_definition_token(data):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Frase di esempio\n",
    "    words= []\n",
    "    words_for_every_label = {}\n",
    "\n",
    "    for item,content in data.items():\n",
    "        words= []\n",
    "        for definition in content:\n",
    "            tokens = word_tokenize(definition)\n",
    "            words_clean = [token.lower() for token in tokens if token.isalpha()\n",
    "                    and token.lower() not in stop_words]\n",
    "            tagged_words = pos_tag(words_clean)\n",
    "            nouns = [word for word, pos in tagged_words if pos == 'NN']\n",
    "            for n in nouns:\n",
    "                words.append(n)\n",
    "        \n",
    "        words_for_every_label[item] = words\n",
    "    return words_for_every_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T17:41:55.898884Z",
     "start_time": "2023-11-11T17:41:55.757422Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def cleaning_definition_token_hypo(data):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    words= []\n",
    "    words_for_every_label = {}\n",
    "    list_of_tuple=[]\n",
    "    for label, content in data.items():\n",
    "        list_of_tuple=[]\n",
    "        for tupla in data[label]:\n",
    "\n",
    "            tokens = word_tokenize(tupla[1])\n",
    "                \n",
    "            words_clean = [token.lower() for token in tokens if token.isalpha()\n",
    "                       and token.lower() not in stop_words]\n",
    "            \n",
    "            tagged_words = pos_tag(words_clean)\n",
    "          \n",
    "            nouns = [word for word, pos in tagged_words if pos == 'NN']\n",
    "        \n",
    "            list_of_tuple.append((tupla[0],nouns))\n",
    "        words_for_every_label[label] = list_of_tuple\n",
    "    return words_for_every_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T17:25:21.627698Z",
     "start_time": "2023-11-12T17:25:21.608032Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_genus(words_for_every_label,k_genus):\n",
    "\n",
    "    top_tokens_for_labels = {}\n",
    "    genus = []\n",
    "    for label, label_tokens in words_for_every_label.items():\n",
    "        genus = []\n",
    " \n",
    "        token_counts = Counter(label_tokens)\n",
    "\n",
    "        top_tokens = token_counts.most_common(k_genus)\n",
    "\n",
    "        for t in top_tokens:\n",
    "            genus.append(t[0])\n",
    "  \n",
    "            top_tokens_for_labels[label] = genus\n",
    " \n",
    "    return top_tokens_for_labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T17:25:19.818394Z",
     "start_time": "2023-11-12T17:25:19.722809Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_genus_with_score(words_for_every_label,k_genus):\n",
    "   \n",
    "    top_tokens_for_labels = {}\n",
    "    genus = []\n",
    "    for label, label_tokens in words_for_every_label.items():\n",
    "        genus = []\n",
    "        token_counts = Counter(label_tokens)\n",
    "        top_tokens = token_counts.most_common(k_genus)\n",
    "\n",
    "        for t in top_tokens:\n",
    "            genus.append((t[0],t[1]))\n",
    "            top_tokens_for_labels[label] = genus\n",
    "    return top_tokens_for_labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Algoritmo di lesk per la WSD dei genus trovati metodo 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T17:43:59.055233Z",
     "start_time": "2023-11-12T17:43:58.973270Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lesk_for_disambiguation(genus_for_every_label, definition_clean):\n",
    "    genus_best_sense = {}\n",
    "    best_senses = []\n",
    "    for label1,genus in genus_for_every_label.items():\n",
    "        best_senses = []\n",
    "        for label2,tokens in definition_clean.items():\n",
    "            if label1 == label2:\n",
    "                for gen in genus_for_every_label[label1]:\n",
    "                    if lesk(tokens,gen)  is not None:\n",
    "                        best_senses.append((gen,lesk(definition_clean[label1],gen)))\n",
    "        genus_best_sense[label1] = best_senses\n",
    "    return genus_best_sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo di lesk per la WSD dei genus trovati metodo 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T17:44:03.480605Z",
     "start_time": "2023-11-12T17:44:03.405906Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lesk_for_disambiguation_metod2(genus_for_every_label, definition_clean):\n",
    "    genus_best_sense = {}\n",
    "    best_senses = []\n",
    "    for label1,genus in genus_for_every_label.items():\n",
    "        #print(label1,genus)\n",
    "        best_senses = []\n",
    "        for label2,tokens in definition_clean.items():\n",
    "            #print(label2,tokens)\n",
    "            if label1 == label2:\n",
    "                #print(\"ok\")\n",
    "\n",
    "                for gen in genus_for_every_label[label1]:\n",
    "                    \n",
    "                    if lesk(tokens,gen[0])  is not None:\n",
    "                        \n",
    "                        best_senses.append((gen[0],lesk(definition_clean[label1],gen[0])))\n",
    "\n",
    "                #print(best_sense)\n",
    "        genus_best_sense[label1] = best_senses\n",
    "    #for l,bs in genus_best_sense.items():\n",
    "    #print(l, genus_best_sense[l])\n",
    "    return genus_best_sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T15:20:39.767774Z",
     "start_time": "2023-11-12T15:20:39.697807Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_synset_genus_metodo2(genus_for_every_label):\n",
    "    genus_sense = {}\n",
    "    best_senses = []\n",
    "    for label1,genus in genus_for_every_label.items():\n",
    "        best_senses = []\n",
    "        for gen in genus_for_every_label[label1]:\n",
    "             best_senses.append((gen[0],wn.synsets(gen[0])))\n",
    "\n",
    "                #print(best_sense)\n",
    "        genus_sense[label1] = best_senses\n",
    "    #for l,bs in genus_best_sense.items():\n",
    "    #print(l, genus_best_sense[l])\n",
    "    return genus_sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_hyponyms(genus_synset):\n",
    "    stak = []\n",
    "    hy = []\n",
    "\n",
    "    for g in genus_synset:\n",
    "        if g.name().split('.')[1] == 'n':\n",
    "            stak.append(g)\n",
    "\n",
    "    while len(stak) != 0:\n",
    "        hypo = stak.pop()\n",
    "\n",
    "        new_hyponims = hypo.hyponyms()\n",
    "\n",
    "        for h in new_hyponims:\n",
    "            if h not in stak and h.name().split('.')[1] == 'n':\n",
    "                stak.append(h)\n",
    "\n",
    "\n",
    "        hy.append(hypo)\n",
    "   \n",
    "\n",
    "    \n",
    "    return hy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T14:43:13.628862Z",
     "start_time": "2023-11-11T14:43:13.573402Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_hypo(synsets_genus):\n",
    "    hyponyms_genus={}\n",
    "\n",
    "    for l,bestsense_genus in synsets_genus.items():\n",
    "        hyponyms = []\n",
    "        #print(\"LABEL --> \",l)\n",
    "        for genus_bs in bestsense_genus:\n",
    "            #print(\"GENUS -->\",genus_bs[0],\" \",genus_bs[1],type(genus_bs[1]))\n",
    "            hyponyms_ = get_all_hyponyms(genus_bs[1].hyponyms())\n",
    "            \n",
    "            #print(hyponyms_)\n",
    "            if hyponyms_ != []:\n",
    "                for h in hyponyms_ :\n",
    "                    hyponyms.append(h)\n",
    "        #print(l,bestsense,hyponyms)\n",
    "        hyponyms_genus[l]= hyponyms\n",
    "    #for l,content in hyponyms_genus.items():\n",
    "        #print(l, \"---->\",hyponyms_genus[l])\n",
    "    return hyponyms_genus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T15:22:26.167844Z",
     "start_time": "2023-11-12T15:22:26.087572Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_hypo_metodo2(synsets_genus):\n",
    "    hyponyms_genus={}\n",
    "\n",
    "    for l,bestsense_genus in synsets_genus.items():\n",
    "        hyponyms = []\n",
    "        #print(\"LABEL --> \",l)\n",
    "        for genus_bs in bestsense_genus:\n",
    "            #print(\"GENUS -->\",genus_bs[0],\" \",genus_bs[1],type(genus_bs[1]))\n",
    "            syns_list_genus = genus_bs[1]\n",
    "            for s in syns_list_genus:\n",
    "                hyponyms_ = get_all_hyponyms(s.hyponyms())\n",
    "            #print(hyponyms_)\n",
    "                if hyponyms_ != []:\n",
    "                    for h in hyponyms_ :\n",
    "                        hyponyms.append(h)\n",
    "        #print(l,bestsense,hyponyms)\n",
    "        hyponyms_genus[l]= hyponyms\n",
    "    #for l,content in hyponyms_genus.items():\n",
    "    #print(l, \"---->\",hyponyms_genus[l])\n",
    "    return hyponyms_genus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T16:07:02.906379Z",
     "start_time": "2023-11-11T16:07:02.858149Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_definitions_hypo(synsets_hypo):\n",
    "    hypo_definitions={}\n",
    " \n",
    "    for label, content in synsets_hypo.items():\n",
    "        definitions=[]\n",
    "        for synset in synsets_hypo[label]:\n",
    "            if synset.definition() is not None:\n",
    "                #for d in synset.definition():\n",
    "                definitions.append((synset,synset.definition()))\n",
    "                \n",
    "        hypo_definitions[label] = definitions\n",
    "\n",
    "  \n",
    "    #for l,content in hypo_definitions.items():\n",
    "        #print(\"----------------------------------------\")\n",
    "        #print(l, \"---->\",hypo_definitions[l])\n",
    "    return hypo_definitions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T16:44:36.256297Z",
     "start_time": "2023-11-11T16:44:36.206653Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def token_intersection_definitions(def_hypo,def_targets):\n",
    "    synset_score={}\n",
    "    tupla_score=[]\n",
    "    for label,content in def_hypo.items():\n",
    "        tupla_score=[]\n",
    "        for token_def in def_hypo[label]:\n",
    "            \n",
    "            interc = set(token_def[1]).intersection(set(def_targets[label]))\n",
    "            score = len(interc) \n",
    "            tupla_score.append((score,token_def[0]))\n",
    "        synset_score[label] = tupla_score\n",
    "    return synset_score\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T17:52:52.243973Z",
     "start_time": "2023-11-11T17:52:52.154252Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def search_best_score(hypo_score):\n",
    "    max_score = 1\n",
    "    best_synsets_list = []\n",
    "    best_synsets_score = {}\n",
    "    for label, c in hypo_score.items():\n",
    "        max_score = 0\n",
    "        best_synsets_list = []\n",
    "        for content in hypo_score[label]:\n",
    "            if  content[0] >= max_score:\n",
    "                if best_synsets_list != [] :\n",
    "                    last_elem= best_synsets_list[-1][1]\n",
    "                    if last_elem < content[0]:\n",
    "                        best_synsets_list.clear()\n",
    "                max_score = content[0]\n",
    "                best_synsets_list.append((content[1],content[0]))\n",
    "        best_synsets_score[label] =  best_synsets_list\n",
    "    return best_synsets_score\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conteggio dei genus all'interno delle definizioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T16:31:02.153504Z",
     "start_time": "2023-11-12T16:31:02.106370Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_of_genus_in_hypo_definition(genus_with_score,hypo_defs):\n",
    "    tupla3_to_append = []\n",
    "    genus_score_in_def = {}\n",
    "    count_score =0\n",
    "    for label,c in genus_with_score.items():\n",
    "        tupla3_to_append = []\n",
    "        for synset_def in hypo_defs[label]:\n",
    "            final_score= 0\n",
    "            for genus in genus_with_score[label]:\n",
    "                count_score =0\n",
    "                count_score = synset_def[1].count(genus[0]) \n",
    "                if count_score != 0:\n",
    "                    final_score = final_score + genus[1] + count_score\n",
    "            tupla3_to_append.append((final_score,synset_def[0]))\n",
    "        genus_score_in_def[label] = tupla3_to_append\n",
    "    return genus_score_in_def\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcolo della distanza tra la parola target e la parola trovata dall'algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T17:59:17.240588Z",
     "start_time": "2023-11-12T17:59:17.192309Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def similarity_synsets(s1,s2):\n",
    "    \n",
    "\n",
    "# Calcola la similarità tra i due Synset utilizzando path_similarity\n",
    "    similarity_score = s1.path_similarity(s2)\n",
    "    similarity_score = \"{:.2f}\".format(similarity_score)\n",
    "   \n",
    "    return similarity_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T17:15:11.181142Z",
     "start_time": "2023-11-12T17:15:11.145860Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def search_synset_target_words(data):\n",
    "    synset_target_word = {}\n",
    "    for label,c in data.items():\n",
    "        synset_target_word[label] = wn.synsets(label)[0]\n",
    "    return synset_target_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulizia definizioni e concatenazione di tutti i nomi contenuti nell'insieme delle definizioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "definition_clean = cleaning_definition_token(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numero di genus in input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_genus = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##METODO 3\n",
    "genus_for_every_label = get_genus(definition_clean,k_genus)\n",
    "synsets_genus= lesk_for_disambiguation(genus_for_every_label, definition_clean)\n",
    "hypo_for_every_genus = get_hypo(synsets_genus)\n",
    "definitions_hypo = get_definitions_hypo(hypo_for_every_genus)\n",
    "definitions_hypo_cleaning= cleaning_definition_token_hypo(definitions_hypo)\n",
    "hypo_score = token_intersection_definitions(definitions_hypo_cleaning,definition_clean)\n",
    "targets_words = search_best_score(hypo_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T18:05:15.830739Z",
     "start_time": "2023-11-12T18:05:12.859393Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "##METODO 2\n",
    "genus_for_every_label_with_Score = get_genus_with_score(definition_clean,k_genus)\n",
    "synsets_genus= get_synset_genus_metodo2(genus_for_every_label_with_Score)\n",
    "synsets_genus_disambiguated = lesk_for_disambiguation_metod2(synsets_genus, definition_clean)\n",
    "hypo_for_every_genus_metodo2 = get_hypo(synsets_genus_disambiguated )\n",
    "definitions_hypo_metodo2 = get_definitions_hypo(hypo_for_every_genus_metodo2)\n",
    "definitions_hypo_cleaning_metodo2= cleaning_definition_token_hypo(definitions_hypo_metodo2)\n",
    "count_of_genus = count_of_genus_in_hypo_definition(genus_for_every_label_with_Score,definitions_hypo_cleaning_metodo2)\n",
    "targets_words_met2 = search_best_score(count_of_genus)\n",
    "\n",
    "#print(\"Metodo1\")\n",
    "#for i,c in targets_words_met2.items():\n",
    "    #print(\"-----\")\n",
    "   # print(i)\n",
    "    #print(targets_words_met2[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "synsets_target_word = search_synset_target_words(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## STAMPA RISULTATI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T18:05:22.861194Z",
     "start_time": "2023-11-12T18:05:22.811139Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------------------------+---------------------------------+\n",
      "|   \u001b[34mParola   |             \u001b[32mMetodo1\u001b[0m             |             \u001b[33mMetodo2\u001b[0m             |\n",
      "+------------+---------------------------------+---------------------------------+\n",
      "|    \u001b[34mdoor    |   \u001b[32mSynset('doorway.n.01'): 0.10\u001b[33m  |   Synset('doorway.n.01'): 0.10\u001b[0m  |\n",
      "|  \u001b[34mladybug   |   \u001b[32mSynset('orange.n.04'): 0.06\u001b[33m   |  Synset('good_luck.n.02'): 0.07\u001b[0m |\n",
      "|    \u001b[34mpain    | \u001b[32mSynset('discomfort.n.02'): 0.08\u001b[33m |   Synset('frisson.n.01'): 0.08\u001b[0m  |\n",
      "| \u001b[34mblurriness |  \u001b[32mSynset('likeness.n.02'): 0.07\u001b[33m  | Synset('emmetropia.n.01'): 0.12\u001b[0m |\n",
      "+------------+---------------------------------+---------------------------------+\n"
     ]
    }
   ],
   "source": [
    "table = PrettyTable()\n",
    "table.field_names = [Fore.BLUE +\"Parola\" , Fore.GREEN + \"Metodo1\" + Style.RESET_ALL, Fore.YELLOW + \"Metodo2\" + Style.RESET_ALL]\n",
    "for i,c in targets_words.items():\n",
    "\n",
    "    table.add_row([Fore.BLUE + i ,Fore.GREEN + str(targets_words[i][0][0]) + \": \"+ str(similarity_synsets(targets_words[i][0][0],synsets_target_word[i]))+ Fore.YELLOW, str(targets_words_met2[i][0][0]) + \": \"+str(similarity_synsets(targets_words_met2[i][0][0],synsets_target_word[i])) +  Style.RESET_ALL])\n",
    "  \n",
    "\n",
    "\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T18:07:47.838849Z",
     "start_time": "2023-11-12T18:07:47.710967Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENUS  insect\n",
      "GENUS SYNSETS  [Synset('insect.n.01'), Synset('worm.n.02')]\n",
      "CONTESTO {'fly', 'color', 'round', 'control', 'person', 'coat', 'family', 'insectivore', 'culture', 'yellow', 'orange', 'insect', 'luck', 'shape', 'harmless', 'spot', 'pattern', 'head', 'bug'}\n",
      "Significato identificato: Synset('worm.n.02')\n",
      "GENUS  luck\n",
      "GENUS SYNSETS  [Synset('fortune.n.04'), Synset('luck.n.02'), Synset('luck.n.03')]\n",
      "CONTESTO {'fly', 'color', 'round', 'control', 'person', 'coat', 'family', 'insectivore', 'culture', 'yellow', 'orange', 'insect', 'luck', 'shape', 'harmless', 'spot', 'pattern', 'head', 'bug'}\n",
      "Significato identificato: Synset('luck.n.03')\n",
      "GENUS  color\n",
      "GENUS SYNSETS  [Synset('color.n.01'), Synset('color.n.02'), Synset('color.n.03'), Synset('color.n.04'), Synset('semblance.n.01'), Synset('coloring_material.n.01'), Synset('color.n.07'), Synset('color.n.08'), Synset('color.v.01'), Synset('tinge.v.01'), Synset('color.v.03'), Synset('color.v.04'), Synset('color.v.05'), Synset('discolor.v.03'), Synset('color.a.01')]\n",
      "CONTESTO {'fly', 'color', 'round', 'control', 'person', 'coat', 'family', 'insectivore', 'culture', 'yellow', 'orange', 'insect', 'luck', 'shape', 'harmless', 'spot', 'pattern', 'head', 'bug'}\n",
      "Significato identificato: Synset('coloring_material.n.01')\n",
      "GENUS  round\n",
      "GENUS SYNSETS  [Synset('round.n.01'), Synset('cycle.n.01'), Synset('beat.n.01'), Synset('round.n.04'), Synset('round_of_golf.n.01'), Synset('round.n.06'), Synset('turn.n.09'), Synset('round.n.08'), Synset('round.n.09'), Synset('round.n.10'), Synset('round.n.11'), Synset('round.n.12'), Synset('rung.n.01'), Synset('circle.n.08'), Synset('round.v.01'), Synset('round.v.02'), Synset('round.v.03'), Synset('attack.v.02'), Synset('polish.v.03'), Synset('round_off.v.03'), Synset('round.v.07'), Synset('round.a.01'), Synset('orotund.s.02'), Synset('round.s.03'), Synset('round.r.01')]\n",
      "CONTESTO {'fly', 'color', 'round', 'control', 'person', 'coat', 'family', 'insectivore', 'culture', 'yellow', 'orange', 'insect', 'luck', 'shape', 'harmless', 'spot', 'pattern', 'head', 'bug'}\n",
      "Significato identificato: Synset('round_off.v.03')\n",
      "GENUS  fly\n",
      "GENUS SYNSETS  [Synset('fly.n.01'), Synset('tent-fly.n.01'), Synset('fly.n.03'), Synset('fly.n.04'), Synset('fly.n.05'), Synset('fly.v.01'), Synset('fly.v.02'), Synset('fly.v.03'), Synset('fly.v.04'), Synset('fly.v.05'), Synset('fly.v.06'), Synset('fly.v.07'), Synset('fly.v.08'), Synset('fly.v.09'), Synset('fly.v.10'), Synset('flee.v.01'), Synset('fly.v.12'), Synset('fly.v.13'), Synset('vanish.v.05'), Synset('fly.s.01')]\n",
      "CONTESTO {'fly', 'color', 'round', 'control', 'person', 'coat', 'family', 'insectivore', 'culture', 'yellow', 'orange', 'insect', 'luck', 'shape', 'harmless', 'spot', 'pattern', 'head', 'bug'}\n",
      "Significato identificato: Synset('fly.v.13')\n",
      "GENUS  coat\n",
      "GENUS SYNSETS  [Synset('coat.n.01'), Synset('coating.n.01'), Synset('coat.n.03'), Synset('coat.v.01'), Synset('coat.v.02'), Synset('coat.v.03')]\n",
      "CONTESTO {'fly', 'color', 'round', 'control', 'person', 'coat', 'family', 'insectivore', 'culture', 'yellow', 'orange', 'insect', 'luck', 'shape', 'harmless', 'spot', 'pattern', 'head', 'bug'}\n",
      "Significato identificato: Synset('coat.v.03')\n",
      "GENUS  bug\n",
      "GENUS SYNSETS  [Synset('bug.n.01'), Synset('bug.n.02'), Synset('bug.n.03'), Synset('hemipterous_insect.n.01'), Synset('microbe.n.01'), Synset('tease.v.01'), Synset('wiretap.v.01')]\n",
      "CONTESTO {'fly', 'color', 'round', 'control', 'person', 'coat', 'family', 'insectivore', 'culture', 'yellow', 'orange', 'insect', 'luck', 'shape', 'harmless', 'spot', 'pattern', 'head', 'bug'}\n",
      "Significato identificato: Synset('bug.n.01')\n",
      "GENUS  orange\n",
      "GENUS SYNSETS  [Synset('orange.n.01'), Synset('orange.n.02'), Synset('orange.n.03'), Synset('orange.n.04'), Synset('orange.n.05'), Synset('orange.s.01')]\n",
      "CONTESTO {'fly', 'color', 'round', 'control', 'person', 'coat', 'family', 'insectivore', 'culture', 'yellow', 'orange', 'insect', 'luck', 'shape', 'harmless', 'spot', 'pattern', 'head', 'bug'}\n",
      "Significato identificato: Synset('orange.n.02')\n",
      "GENUS  family\n",
      "GENUS SYNSETS  [Synset('family.n.01'), Synset('family.n.02'), Synset('class.n.01'), Synset('family.n.04'), Synset('kin.n.01'), Synset('family.n.06'), Synset('syndicate.n.01'), Synset('family.n.08')]\n",
      "CONTESTO {'fly', 'color', 'round', 'control', 'person', 'coat', 'family', 'insectivore', 'culture', 'yellow', 'orange', 'insect', 'luck', 'shape', 'harmless', 'spot', 'pattern', 'head', 'bug'}\n",
      "Significato identificato: Synset('kin.n.01')\n",
      "GENUS  insectivore\n",
      "GENUS SYNSETS  [Synset('insectivore.n.01'), Synset('insectivore.n.02')]\n",
      "CONTESTO {'fly', 'color', 'round', 'control', 'person', 'coat', 'family', 'insectivore', 'culture', 'yellow', 'orange', 'insect', 'luck', 'shape', 'harmless', 'spot', 'pattern', 'head', 'bug'}\n",
      "Significato identificato: Synset('insectivore.n.02')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for gen in genus_for_every_label[\"ladybug\"]:\n",
    "    print(\"GENUS \", gen)\n",
    "    print(\"GENUS SYNSETS \",wn.synsets(gen))\n",
    "# Definizione per \"ladybug\"\n",
    "    definition_ladybug = set(definition_clean[\"ladybug\"])\n",
    "    print(\"CONTESTO\", definition_ladybug)\n",
    "\n",
    "# Parola ambigua: \"ladybug\"\n",
    "    ambiguous_word = gen\n",
    "\n",
    "\n",
    "# Applica la funzione lesk\n",
    "    meaning = lesk(definition_ladybug, ambiguous_word)\n",
    "\n",
    "# Stampa il significato identificato\n",
    "    print(\"Significato identificato:\", meaning)\n",
    "#print(s)\n",
    "#print(s[0].hyponyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Significato identificato: Synset('ladybug.n.01')\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Definizione per \"ladybug\"\n",
    "definition_ladybug = definition_clean[\"ladybug\"]\n",
    "\n",
    "# Parola ambigua: \"ladybug\"\n",
    "ambiguous_word = \"ladybug\"\n",
    "\n",
    "# Contesto come definizione della parola\n",
    "context_sentence = ' '.join(definition_ladybug)\n",
    "\n",
    "# Tokenizza la frase di definizione\n",
    "tokenized_context = word_tokenize(context_sentence)\n",
    "\n",
    "# Applica la funzione lesk\n",
    "meaning = lesk(tokenized_context, ambiguous_word)\n",
    "\n",
    "# Stampa il significato identificato\n",
    "print(\"Significato identificato:\", meaning)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
