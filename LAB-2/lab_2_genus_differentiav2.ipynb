{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/rossellaborra/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from utils import utils as u\n",
    "\n",
    "from nltk import pos_tag\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.wsd import lesk\n",
    "nltk.download('wordnet')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T13:09:31.375700Z",
     "start_time": "2023-11-11T13:09:31.236354Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creo la struttura dati\n",
    "\n",
    "La struttura dati utilizzata avrà la seguente forma: \n",
    "$$data[word] = [definition_i, definition_{i+1}, ..., definition_n]$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_path = '../datasets/TLN-definitions-23.tsv'\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "data = {}\n",
    "definitions = []\n",
    "for col in df.columns:\n",
    "    definitions = []\n",
    "    if col != '1':\n",
    "        data[col] = {} ## data[ladybug]\n",
    "        for riga in df[col]:\n",
    "            if col != '1':\n",
    "                definitions.append(riga)\n",
    "        data[col] = definitions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenizzazione e pulizia delle definizioni"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [],
   "source": [
    "\n",
    "def cleaning_definition_token(data):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Frase di esempio\n",
    "    words= []\n",
    "    words_for_every_label = {}\n",
    "\n",
    "    for item,content in data.items():\n",
    "        words= []\n",
    "        for definition in content:\n",
    "            tokens = word_tokenize(definition)\n",
    "            words_clean = [token.lower() for token in tokens if token.isalpha()\n",
    "                    and token.lower() not in stop_words]\n",
    "            tagged_words = pos_tag(words_clean)\n",
    "            nouns = [word for word, pos in tagged_words if pos == 'NN']\n",
    "            for n in nouns:\n",
    "                words.append(n)\n",
    "        \n",
    "        words_for_every_label[item] = words\n",
    "    return words_for_every_label"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T14:43:36.404281Z",
     "start_time": "2023-11-11T14:43:36.354934Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "outputs": [],
   "source": [
    "\n",
    "def cleaning_definition_token_hypo(data):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Frase di esempio\n",
    "    words= []\n",
    "    words_for_every_label = {}\n",
    "    list_of_tuple=[]\n",
    "    for label, content in data.items():\n",
    "        list_of_tuple=[]\n",
    "        for tupla in data[label]:\n",
    "\n",
    "            tokens = word_tokenize(tupla[1])\n",
    "            #print(tokens)\n",
    "                \n",
    "            words_clean = [token.lower() for token in tokens if token.isalpha()\n",
    "                       and token.lower() not in stop_words]\n",
    "            #print(words_clean)\n",
    "            \n",
    "            tagged_words = pos_tag(words_clean)\n",
    "            #print(tagged_words)\n",
    "            nouns = [word for word, pos in tagged_words if pos == 'NN']\n",
    "        \n",
    "            #print(nouns)\n",
    "            list_of_tuple.append((tupla[0],nouns))\n",
    "        words_for_every_label[label] = list_of_tuple\n",
    "    return words_for_every_label"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T17:41:55.898884Z",
     "start_time": "2023-11-11T17:41:55.757422Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "outputs": [],
   "source": [
    "def get_genus(words_for_every_label):\n",
    "# Dizionario per memorizzare i primi 5 token più frequenti per ogni label\n",
    "    top_tokens_for_labels = {}\n",
    "    genus = []\n",
    "    for label, label_tokens in words_for_every_label.items():\n",
    "        genus = []\n",
    "    # Conta la frequenza di ciascun token per la label corrente\n",
    "        token_counts = Counter(label_tokens)\n",
    "\n",
    "    # Ottieni i primi 5 token più frequenti per la label corrente\n",
    "        top_tokens = token_counts.most_common(100)\n",
    "\n",
    "        for t in top_tokens:\n",
    "            genus.append(t[0])\n",
    "    # Aggiungi i risultati al dizionario top_tokens_for_labels\n",
    "            top_tokens_for_labels[label] = genus\n",
    "    #for l,c in top_tokens_for_labels.items():\n",
    "        #print(l,\"--->\", top_tokens_for_labels[l])\n",
    "    return top_tokens_for_labels\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T18:08:16.823170Z",
     "start_time": "2023-11-11T18:08:16.666399Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "outputs": [],
   "source": [
    "def get_genus_with_score(words_for_every_label):\n",
    "    # Dizionario per memorizzare i primi 5 token più frequenti per ogni label\n",
    "    top_tokens_for_labels = {}\n",
    "    genus = []\n",
    "    for label, label_tokens in words_for_every_label.items():\n",
    "        genus = []\n",
    "        # Conta la frequenza di ciascun token per la label corrente\n",
    "        token_counts = Counter(label_tokens)\n",
    "        # Ottieni i primi 5 token più frequenti per la label corrente\n",
    "        top_tokens = token_counts.most_common(100)\n",
    "\n",
    "        for t in top_tokens:\n",
    "            genus.append((t[0],t[1]))\n",
    "            # Aggiungi i risultati al dizionario top_tokens_for_labels\n",
    "            top_tokens_for_labels[label] = genus\n",
    "    #for l,c in top_tokens_for_labels.items():\n",
    "    #print(l,\"--->\", top_tokens_for_labels[l])\n",
    "    return top_tokens_for_labels\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:57:59.189460Z",
     "start_time": "2023-11-12T16:57:59.113026Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "outputs": [],
   "source": [
    "def lesk_for_disambiguation(genus_for_every_label, definition_clean):\n",
    "    genus_best_sense = {}\n",
    "    best_senses = []\n",
    "    for label1,genus in genus_for_every_label.items():\n",
    "        #print(label1,genus)\n",
    "        best_senses = []\n",
    "        for label2,tokens in definition_clean.items():\n",
    "            #print(label2,tokens)\n",
    "            if label1 == label2:\n",
    "                #print(\"ok\")\n",
    "                \n",
    "                for gen in genus_for_every_label[label1]:\n",
    "                    if lesk(tokens,gen)  is not None:\n",
    "                        best_senses.append((gen,lesk(tokens,gen)))\n",
    "                    \n",
    "                #print(best_sense)\n",
    "        genus_best_sense[label1] = best_senses\n",
    "    #for l,bs in genus_best_sense.items():\n",
    "        #print(l, genus_best_sense[l])\n",
    "    return genus_best_sense"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T14:43:18.646060Z",
     "start_time": "2023-11-11T14:43:18.429462Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "outputs": [],
   "source": [
    "def lesk_for_disambiguation_metod2(genus_for_every_label, definition_clean):\n",
    "    genus_best_sense = {}\n",
    "    best_senses = []\n",
    "    for label1,genus in genus_for_every_label.items():\n",
    "        #print(label1,genus)\n",
    "        best_senses = []\n",
    "        for label2,tokens in definition_clean.items():\n",
    "            #print(label2,tokens)\n",
    "            if label1 == label2:\n",
    "                #print(\"ok\")\n",
    "\n",
    "                for gen in genus_for_every_label[label1]:\n",
    "                    \n",
    "                    if lesk(tokens,gen[0])  is not None:\n",
    "                        \n",
    "                        best_senses.append((gen[0],lesk(tokens,gen[0])))\n",
    "\n",
    "                #print(best_sense)\n",
    "        genus_best_sense[label1] = best_senses\n",
    "    #for l,bs in genus_best_sense.items():\n",
    "    #print(l, genus_best_sense[l])\n",
    "    return genus_best_sense"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T15:11:52.109012Z",
     "start_time": "2023-11-12T15:11:51.409515Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "outputs": [],
   "source": [
    "def get_synset_genus_metodo2(genus_for_every_label):\n",
    "    genus_sense = {}\n",
    "    best_senses = []\n",
    "    for label1,genus in genus_for_every_label.items():\n",
    "        best_senses = []\n",
    "        for gen in genus_for_every_label[label1]:\n",
    "             best_senses.append((gen[0],wn.synsets(gen[0])))\n",
    "\n",
    "                #print(best_sense)\n",
    "        genus_sense[label1] = best_senses\n",
    "    #for l,bs in genus_best_sense.items():\n",
    "    #print(l, genus_best_sense[l])\n",
    "    return genus_sense"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T15:20:39.767774Z",
     "start_time": "2023-11-12T15:20:39.697807Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "outputs": [],
   "source": [
    "def get_hypo(synsets_genus):\n",
    "    hyponyms_genus={}\n",
    "   \n",
    "    for l,bestsense_genus in synsets_genus.items():\n",
    "        hyponyms = []\n",
    "        #print(\"LABEL --> \",l)\n",
    "        for genus_bs in bestsense_genus:\n",
    "            #print(\"GENUS -->\",genus_bs[0],\" \",genus_bs[1],type(genus_bs[1]))\n",
    "            hyponyms_ = genus_bs[1].hyponyms()\n",
    "            #print(hyponyms_)\n",
    "            if hyponyms_ != []:\n",
    "                for h in hyponyms_ :\n",
    "                    hyponyms.append(h)\n",
    "        #print(l,bestsense,hyponyms)\n",
    "        hyponyms_genus[l]= hyponyms\n",
    "    #for l,content in hyponyms_genus.items():\n",
    "        #print(l, \"---->\",hyponyms_genus[l])\n",
    "    return hyponyms_genus"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T14:43:13.628862Z",
     "start_time": "2023-11-11T14:43:13.573402Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "outputs": [],
   "source": [
    "def get_hypo_metodo2(synsets_genus):\n",
    "    hyponyms_genus={}\n",
    "\n",
    "    for l,bestsense_genus in synsets_genus.items():\n",
    "        hyponyms = []\n",
    "        #print(\"LABEL --> \",l)\n",
    "        for genus_bs in bestsense_genus:\n",
    "            #print(\"GENUS -->\",genus_bs[0],\" \",genus_bs[1],type(genus_bs[1]))\n",
    "            syns_list_genus = genus_bs[1]\n",
    "            for s in syns_list_genus:\n",
    "                hyponyms_ = s.hyponyms()\n",
    "            #print(hyponyms_)\n",
    "                if hyponyms_ != []:\n",
    "                    for h in hyponyms_ :\n",
    "                        hyponyms.append(h)\n",
    "        #print(l,bestsense,hyponyms)\n",
    "        hyponyms_genus[l]= hyponyms\n",
    "    #for l,content in hyponyms_genus.items():\n",
    "    #print(l, \"---->\",hyponyms_genus[l])\n",
    "    return hyponyms_genus"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T15:22:26.167844Z",
     "start_time": "2023-11-12T15:22:26.087572Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "outputs": [],
   "source": [
    "def get_definitions_hypo(synsets_hypo):\n",
    "    hypo_definitions={}\n",
    " \n",
    "    for label, content in synsets_hypo.items():\n",
    "        definitions=[]\n",
    "        for synset in synsets_hypo[label]:\n",
    "            if synset.definition() is not None:\n",
    "                #for d in synset.definition():\n",
    "                definitions.append((synset,synset.definition()))\n",
    "                \n",
    "        hypo_definitions[label] = definitions\n",
    "\n",
    "  \n",
    "    #for l,content in hypo_definitions.items():\n",
    "        #print(\"----------------------------------------\")\n",
    "        #print(l, \"---->\",hypo_definitions[l])\n",
    "    return hypo_definitions\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T16:07:02.906379Z",
     "start_time": "2023-11-11T16:07:02.858149Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "outputs": [],
   "source": [
    "def token_intersection_definitions(def_hypo,def_targets):\n",
    "    synset_score={}\n",
    "    tupla_score=[]\n",
    "    for label,content in def_hypo.items():\n",
    "        tupla_score=[]\n",
    "        for token_def in def_hypo[label]:\n",
    "            interc = set(token_def[1]).intersection(set(def_targets[label]))\n",
    "            score = len(interc) \n",
    "            tupla_score.append((score,token_def[0]))\n",
    "        synset_score[label] = tupla_score\n",
    "    return synset_score\n",
    "        \n",
    "        \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T16:44:36.256297Z",
     "start_time": "2023-11-11T16:44:36.206653Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "outputs": [],
   "source": [
    "def search_best_score(hypo_score):\n",
    "    max_score = 1\n",
    "    best_synsets_list = []\n",
    "    best_synsets_score = {}\n",
    "    for label, c in hypo_score.items():\n",
    "        max_score = 0\n",
    "        best_synsets_list = []\n",
    "        for content in hypo_score[label]:\n",
    "            if  content[0] >= max_score:\n",
    "                if best_synsets_list != [] :\n",
    "                    last_elem= best_synsets_list[-1][1]\n",
    "                    if last_elem < content[0]:\n",
    "                        best_synsets_list.clear()\n",
    "                max_score = content[0]\n",
    "                best_synsets_list.append((content[1],content[0]))\n",
    "        best_synsets_score[label] =  best_synsets_list\n",
    "    return best_synsets_score\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T17:52:52.243973Z",
     "start_time": "2023-11-11T17:52:52.154252Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "outputs": [],
   "source": [
    "def count_of_genus_in_hypo_definition(genus_with_score,hypo_defs):\n",
    "    tupla3_to_append = []\n",
    "    genus_score_in_def = {}\n",
    "    count_score =0\n",
    "    for label,c in genus_with_score.items():\n",
    "        tupla3_to_append = []\n",
    "        \n",
    "        for synset_def in hypo_defs[label]:\n",
    "\n",
    "            \n",
    "            final_score= 0\n",
    "            for genus in genus_with_score[label]:\n",
    "                count_score =0\n",
    "                #print(\"confronto\")\n",
    "                #print(synset_def[1])\n",
    "                #print(genus[0])\n",
    "                #print(genus[1])\n",
    "                count_score = synset_def[1].count(genus[0]) \n",
    "                if count_score != 0:\n",
    "                    final_score = final_score + genus[1] + count_score\n",
    "            tupla3_to_append.append((final_score,synset_def[0]))\n",
    "        genus_score_in_def[label] = tupla3_to_append\n",
    "    return genus_score_in_def\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:31:02.153504Z",
     "start_time": "2023-11-12T16:31:02.106370Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "outputs": [],
   "source": [
    "## PULIZIA DEFINIZIONI (TOKENIZZAZIONE, RIMOZIONE STOP WORDS , ESTRAZIONE DI NOMI)\n",
    "definition_clean = cleaning_definition_token(data)\n",
    "\n",
    "\n",
    "##METODO 1\n",
    "\n",
    "genus_for_every_label = get_genus(definition_clean)\n",
    "synsets_genus= lesk_for_disambiguation(genus_for_every_label, definition_clean)\n",
    "hypo_for_every_genus = get_hypo(synsets_genus)\n",
    "definitions_hypo = get_definitions_hypo(hypo_for_every_genus)\n",
    "definitions_hypo_cleaning= cleaning_definition_token_hypo(definitions_hypo)\n",
    "hypo_score = token_intersection_definitions(definitions_hypo_cleaning,definition_clean)\n",
    "targets_words = search_best_score(hypo_score)\n",
    "\n",
    "\n",
    "##METODO 2\n",
    "genus_for_every_label_with_Score = get_genus_with_score(definition_clean)\n",
    "synsets_genus= get_synset_genus_metodo2(genus_for_every_label_with_Score)\n",
    "synsets_genus_disambiguated = lesk_for_disambiguation_metod2(synsets_genus, definition_clean)\n",
    "hypo_for_every_genus_metodo2 = get_hypo(synsets_genus_disambiguated )\n",
    "definitions_hypo_metodo2 = get_definitions_hypo(hypo_for_every_genus_metodo2)\n",
    "definitions_hypo_cleaning_metodo2= cleaning_definition_token_hypo(definitions_hypo_metodo2)\n",
    "count_of_genus = count_of_genus_in_hypo_definition(genus_for_every_label_with_Score,definitions_hypo_cleaning_metodo2)\n",
    "targets_words_met2 = search_best_score(count_of_genus)\n",
    "\n",
    "\n",
    "   \n",
    "        \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:58:11.119770Z",
     "start_time": "2023-11-12T16:58:09.323272Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------------------------------+-------------------------------------+\n",
      "|   \u001B[34mParola   |                 \u001B[32mMetodo 1\u001B[0m                |               \u001B[33mMetodo 2\u001B[0m              |\n",
      "+------------+-----------------------------------------+-------------------------------------+\n",
      "|    \u001B[34mdoor    |     \u001B[32mSynset('doorway.n.01') --> score\u001B[33m    |   Synset('doorway.n.01') --> score\u001B[0m  |\n",
      "|  \u001B[34mladybug   | \u001B[32mSynset('reddish_orange.n.01') --> score\u001B[33m |  Synset('good_luck.n.02') --> score\u001B[0m |\n",
      "|    \u001B[34mpain    |      \u001B[32mSynset('glow.v.05') --> score\u001B[33m      |    Synset('glow.v.05') --> score\u001B[0m    |\n",
      "| \u001B[34mblurriness |    \u001B[32mSynset('likeness.n.02') --> score\u001B[33m    | Synset('reflection.n.05') --> score\u001B[0m |\n",
      "+------------+-----------------------------------------+-------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "from colorama import Fore, Style\n",
    "\n",
    "# Installa colorama se non l'hai già fatto\n",
    "# pip install colorama\n",
    "table = PrettyTable()\n",
    "table.field_names = [Fore.BLUE +\"Parola\" , Fore.GREEN + \"Metodo 1\" + Style.RESET_ALL, Fore.YELLOW + \"Metodo 2\" + Style.RESET_ALL]\n",
    "for i,c in targets_words.items():\n",
    "# Crea una tabella PrettyTable\n",
    "    \n",
    "    \n",
    "    table.add_row([Fore.BLUE + i ,Fore.GREEN + str(targets_words[i][0][0]) + \" --> score\" + Fore.YELLOW, str(targets_words_met2[i][0][0]) + \" --> score\"+ Style.RESET_ALL])\n",
    "   # table.add_row([\"Riga 2, Colonna 1\", Fore.YELLOW + \"Riga 2, Colonna 2\" + Style.RESET_ALL])\n",
    "\n",
    "# Stampa la tabella\n",
    "print(table)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T16:58:13.065273Z",
     "start_time": "2023-11-12T16:58:13.012193Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
