Tecnologie del Linguaggio Naturale
parte III Appunti ufficiali
Docente di riferimento: Prof. Luigi Di Caro
Anno 2022/2023

Disclaimer
L’obiettivo di questo materiale non è quello di sostituire le lezioni in presenza, quanto quello di fornire un ulteriore supporto utile per lo studio. Una sorta di light handbook.
Molto del seguente testo deriva dagli appunti di due ex studenti: Giorgia Persichella e Giuseppe Ruggiero, che ringrazio esplicitamente. Ringrazio anche i collaboratori Federico Torrielli e Giovanni Siragusa per alcuni dei contenuti che verranno esposti durante il corso.
La sezione sui Large Language Models è stata creata attraverso l’u- so degli stessi Large Language Models1. Ovviamente, tutto l’input, i prompt, le generazioni ed i successivi passi iterativi sono stati verifica- ti, validati e revisionati manualmente. Ciò significa che il contenuto semantico di quel che viene espresso in quella sezione è ”conforme” alle idee, concetti, definizioni e conoscenza del docente sui temi de- scritti. La forma (superficiale, del linguaggio utilizzato) è invece una co-produzione.
L’esame verterà su tutto ciò che è contenuto all’interno di questo hand- book. In aggiunta, dovranno essere consegnati e discussi gli esercizi di laboratorio, spiegati durante le lezioni e descritti all’interno delle slides che verranno fornite assieme a questo materiale.
Tutto ciò che è riportato è proprietà del docente e associato al corso Tecnologie del Linguaggio Naturale. È assolutamente vietato copia- re, appropriarsi e ridistribuire il contenuto. È quindi vietata anche la pubblicazione dei contenuti in maniera non autorizzata espressamente dal docente, in qualsiasi canale e sotto qualsiasi forma.
1Claude - https://www.anthropic.com/index/introducing-claude, ChatGPT- 3.5 - https://chat.openai.com/. URLs verificati in data 7/4/2023.
2
 
1 Introduzione
Prima di addentrarsi all’interno dei diversi approcci alla seman- tica è necessario fornire un’introduzione più generale sul con- cetto di semantica ed in particolare sui concetti di semantica statistica, linguistico-distribuzionale, lessicale e formale.
1.1 Semantica computazionale
Una prima e modesta semplificazione della semantica compu- tazionale, per quanto concerne il corso TLN, è la seguente:
• semanticalessicale,unsottocampodellasemanticachecon- siste nello studio di come e che cosa denotano le parole (il lessico) di una lingua;
• semanticaformale,cheriguardaimodellilogico-matematici che definiscono formalmente il linguaggio (o i linguaggi, in generale);
• semantica statistica, una semantica su cui ci concentrere- mo in alcune delle prossime lezioni, prettamente numeri- ca;
• semantica linguistico-distribuzionale, che comprende una serie di teorie e metodi di linguistica computazionale per lo studio della distribuzione semantica delle parole nel linguaggio naturale guidata da un approccio sia statistico che linguistico.
3

1 Introduzione
Nei prossimi capitoli verrà illustrato un approccio puramente statistico all’analisi semantica del linguaggio chiamato Text Mi- ning ed un suo sotto-approccio chiamato semantica documen- tale in cui si analizzano collezioni di documenti per estrapolar- ne risultati semantici. Verrà fatto anche un piccolo cenno alla visualizzazione dei dati testuali, sempre attribuibile a questo campo di ricerca.
Successivamente si passerà all’analisi semantica distribuzio- nale che prende molto dall’approccio statistico aggiungendo- ci alcune influenze e principi della linguistica. Verrà anche menzionato qualche confronto con la semantica formale.
Si affronterà successivamente l’Ontology Learning ovvero il task che permette di codificare (estrarre, modellare) in manie- ra automatica o semi-automatica delle informazioni semantiche sotto forma di ontologie e/o tassonomie attraverso l’estrazione di contenuto estratto dal testo.
1.2 Le origini del trattamento del linguaggio naturale
Per comprendere il linguaggio naturale bisogna risalire al pro- blema originario del trattamento del linguaggio naturale ovvero la comunicazione tra macchina e uomo (uno dei sogni dell’AI).
All’inizio uno dei task principali di questa disciplina era quel- lo del Question Answering: la macchina leggeva un testo (What) e rispondeva alle domande dell’utente (Why) tramite l’utilizzo di risorse e codice (How).
Si pensi a quali tipi di domande una persona può chiedere ad una macchina: ce ne sono diverse, si possono chiedere dei fatti accaduti, si può chiedere di elencare alcuni elementi, si possono chiedere definizioni, si può chiedere il perché di alcu-
4

1 Introduzione
ne cose, si possono fare domande ipotetiche (tipicamente molto complesse), si possono effettuare domande cross-language per ottenere risposte in una lingua diversa, ecc. Come possiamo notare le domande sono tantissime: esiste infatto un’area di ri- cerca (si veda Figura 1.2), anche se piccola, che lavora sulla ca- tegorizzazione del tipo di domanda cercando di modellare tutti i modi con cui l’essere umano pone le domande su dei conte- nuti semantici. Questo ha un impatto anche per il trattamento automatico del linguaggio naturale perché qualsiasi metodo di Question Answering data una domanda deve capirne la sua tipologia.
Figura 1.1: Estratto di tipi di domande (solo esemplificativo, non da leggere/studiare).
Il punto chiave sta nel fatto che per ogni domanda è neces- saria una specifica analisi e quindi ogni domanda deve essere associata a diverse informazioni semantiche.
Le informazioni semantiche che possono servire per risolvere
5
 
1 Introduzione
task di Question Answering sono le seguenti in Figura 2.3.
Figura 1.2: Esempi di tipi di informazioni semantiche.
Sebbene la primissima AI abbia pensato sin da subito al Que- stion Answering questo task venne inizialmente considerato so- lamente da una ristretta area di ricercatori, perché troppo com- plesso per le tecnologie disponibili ai tempi (non è ovviamente più il caso odierno, anno 2023, post-ChatGPT e Large Language Models).
Gran parte della ricerca non riguarda più il QA quanto tutto ciò che ci sta dietro, o che va in parallelo e che si focalizza ad esem- pio sull’estrazione automatica di tali informazioni semantiche riportate in figura.
Ad esempio esistono comunità che lavorano sulla Named En- tity Recognition (NER), ovvero dell’identificazione nel testo di persone, luoghi, ecc.; ci sono altri ricercatori che si concentra- no sull’estrazione di specifiche relazioni (ad es. hypernyms); alcune si concentrano sulla Word Sense Disambiguation (WSD) che è stata affrontata dettagliatamente nelle parti precedenti del corso. Un’altro esempio riguarda l’implementazione di un sug- geritore automatico che si basa su un Language Model, cioè
6
 
1 Introduzione
su un modello statistico che calcola la probabilità di avere una parola X data una parola o contest Y che precede. Il caso ap- plicativo classico è il suggeritore presente negli smartphones, il quale integra anche le frasi più utilizzate dall’utente e assegna un peso maggiore a tale probabilità.
Tutti questi esempi sono task abbastanza banali ma l’evo- luzione dell’analisi semantica va a specificare questi approcci definendo veri e propri filoni di ricerca.
7

2 Significato del significato
2.1 Definizioni di base
Per cominciare è necessario avere chiari alcuni concetti base sul linguaggio, gli obiettivi e le dinamiche che caratterizzano la linguistica computazionale:
Lessico. Corrisponde al dizionario ovvero a tutti gli elementi che si hanno a disposizione per costruire una frase.
Sintassi. Studia come gli elementi di un dizionario possono essere collegati tra loro attraverso una struttura che permette di costruire frasi.
Semantica. Corrisponde all’interpretazione di una struttura lessico-sintattica a cui si attribuisce un significato.
Pragmatica. È una disciplina della linguistica che si occupa dell’uso contestuale della lingua ovvero di come il contesto in- fluisca sull’interpretazione dei significati prescindendo dall’uso di lessico e sintassi. In questo caso, per "contesto" si intende "si- tuazione", cioè l’insieme dei fattori extralinguistici (sociali, am- bientali e psicologici) che influenzano gli atti linguistici. Esem- pio: se dico che “sei pesante” indico che sei lento nel muoverti, mi allontano dalla semantica stretta e parlo in modo astratto.
Ambiguità. È una proprietà del linguaggio che permette di esprimere e comunicare utilizzando un minor numero di paro- le. Questa compressione comporta una maggior difficoltà nel discernere il significato specifico di parole/frasi che possono avere una molteplicità di interpretazioni.
8

2 Significato del significato
Polisemia. Indica un fenomeno per cui una parola può espri- mere più significati. Esempio: quadro: “un bel quadro”, “mo- strami il quadro generale”.
Omonimia. Indica un fenomeno per cui una stessa forma ortografica e fonologica esprime più significati. Esempio: vite (plurale di vita) e vite (pianta) sono sia omofone (si pronun- ciano allo stesso modo) sia omografe (si scrivono allo stesso modo).
Differenza tra polisemia ed omonimia. Semplicemente si può dire che nell’omonimia, le parole sono più di una e han- no significato diverso; nella polisemia, invece, la parola è sem- pre la stessa, ma il suo significato originario si è esteso, per somiglianza e non solo, ad altri referenti.
Altri aspetti cruciali del linguaggio sono i seguenti. Comunicazione. Strumento che permette di condividere i
significati che sono all’interno della nostra mente. Convenzione. Meccanismo con cui veicolare il contenuto
semantico attraverso dei simboli (scritti, sonori, ecc.). Granularità. Dimensione qualitativa che caratterizza i modi in cui vengono concettualizzate le situazioni che si vogliono descrivere, muta il significato della parola in base ai dettagli
con cui la vediamo.
Soggettività. Il linguaggio è solo un’approssimazione delle
immagini mentali e quindi è soggetto ad errori che possono portare a interpretazioni errate.
Similarità. Meccanismo innato che permette di inferire il significato di un termine sconosciuto attraverso l’associazione di esso con un termine simile conosciuto.
Esperienza personale. Insieme di tutti gli eventi della vita che formano la conoscenza di un singolo individuo.
9

2 Significato del significato
Senso comune. Convenzioni che stabiliscono il significato che la collettività dà ad alcuni termini, una sorta di esperienza condivisa.
Cultura. Il significato di alcune parole è legato alla conven- zione della cultura nella quale ci si trova e cambia di cultura in cultura (ad es. ”Il 17 porta sfortuna!”).
2.1.1 Frammenti di ontologia
Attraverso queste definizioni, possiamo dire di aver appena creato un’ontologia. Un’ontologia si ottiene infatti quando si raggiunge la condivisione di un significato legato ai concetti di un certo dominio. Non vi è più interesse al significato speci- fico dei singoli concetti, ma al significato condiviso che gli si attribuisce.
2.2 Il significato delle parole
Esistono tre principali teorie per determinare il significato delle parole (“word meaning”):
• Basate su primitive (es. Kats, Wilks, Lakoff): dove per rap- presentare il significato di una parola questo viene fram- mentato in piccoli contenuti semantici di natura atomica. Esempio: nel momento in cui voglio comprendere il si- gnificato della parola “scrivania” devo innanzi tutto com- prendere il concetto di “tavolo”, quindi di una struttura in cui vi è un “piano” che abbia delle fondamenta. Questo ci permette di scoprire significati nuovi e di creare una relazione con quelli vecchi.
10

2 Significato del significato
• Basate su relazioni (es. Quinlan, Fodor): il significato di una parola non è frutto di combinazioni atomiche di pri- mitive universali, ma nasce dalla relazione con le altre parole. Di per se una parola non ha nessun significato, se non impiegata all’interno di un contesto lessicale.
• Basate su composizioni (es. Pustejovski): non solo una parola prende significato quando inserita in un contesto lessicale, ma anche la composizione di questa con altre parole genera significato, ovvero la congiunzione di due parole che hanno una qualche dipendenza sintattica (sono poste vicine) oltre a dare un significato alle stesse forma un terzo significato composizionale.
2.3 Il triangolo semiotico
Il Triangolo semiotico è un modello del significato. Esso dice che qualsiasi concetto che si ha in mente è rappresentabile at- traverso un triangolo i cui poli indicano rispettivamente il Con- cetto, il Referente e la Rappresentazione. La nomenclatura di questi ultimi può cambiare in quanto diversi linguisti sono arrivati allo stesso modello attraverso stadi differenti dandone quindi nomi diversi: ad esempio noi parliamo di Rappresenta- zione, ma ci sono teorie che parlano di Segni, Termini e Sim- boli; il Referente viene chiamato anche Fenomeno o Istanza; il Concetto, viene chiamato anche Significato o Interpretazione.
Definiamo adesso questi componenti partendo dal Concetto. Esso corrisponde semplicemente a ciò che si ha nella mente sen- za utilizzo di alcuna convenzione (es. “gatto”: tutti sappiamo cos’è, a prescindere dagli altri). Dal concetto si passa al livello della Rappresentazione che include la convenzione, ovvero si utilizza un simbolo convenzionale che ci permette di comuni-
11

2 Significato del significato
 Figura 2.1: Triangolo semiotico.
care il concetto (in forma orale, scritta, ecc.), ad es. la parola “gatto”. Quindi se per esempio il concetto di gatto tra un Ita- liano ed un Inglese è lo stesso1, la rappresentazione cambia: noi lo rappresentiamo con il simbolo “gatto”, mentre l’inglese con “cat”. Infine il Referente è un individuo (nel senso di istanza del concetto), cioè un elemento nel mondo reale: un qualsiasi gatto nel mondo. Su questi livelli (Concetto, Rappresentazione, Referente) si appoggiano diverse teorie più o meno complesse.
Dove si collocano l’Intelligenza Artificiale e il Natural Lan- guage Processing in questo triangolo e in che direzione si muo- vono?
Sicuramente l’unico punto da cui è possibile partire è la Rap- presentazione perché non esistono sistemi informatici che pos-
1Esistono comunque modelli mentali e prototipi diversi tra culture diverse.
12
 
2 Significato del significato
sano prendere un concetto dalla nostra testa, ma abbiamo a disposizione moltissimi testi (ad es. sul Web). Da questi ultimi si cerca di creare una concettualizzazione (analisi semantica) e si cerca anche di spostarsi verso la direzione dei referenti, que- sto soprattutto ultimamente grazie alla Computer Vision (CV), area di ricerca che analizza ed interpreta immagini e cattura elementi coinvolti sviluppando una sorta di percezione di ciò che rappresentano.
Figura 2.2: Triangolo semiotico con AI/CV/NLP.
2.4 Multilinguismo
Oggi come non mai la distruzione delle barriere linguistiche ha un ruolo fondamentale proprio perché lingue diverse creano limiti di comprensione.
13
 
2 Significato del significato
La sfida che ha l’NLP nel trattare più lingue è vista come una sfida ma anche come un’opportunità: un testo in dieci lingue, invece di essere un problema, può essere studiato semantica- mente in maniera più ricca attraverso ad es. strumenti indi- pendenti dal linguaggio, o attraverso l’allineamento delle varie versioni testuali. Comprendendo eventualmente la sfumatura variabile dei concetti nelle varie lingue.
Analizzare semanticamente un testo in più lingue in manie- ra indipendente permette di allineare ed integrare i risultati estrattivi tra di loro per:
• capire quali sono le informazioni semantiche più impor- tanti o più certe o maggiormente condivise;
• migrare informazioni semantiche ritrovate su una lingua su di un’altra lingua.
Quindi l’allineamento, la pesatura ed il confronto dell’analisi semantica su più lingue su testi più o meno simili o paralleli può produrre un valore aggiunto sulla semantica globale.
Altro aspetto rilevante è la rarità di una lingua. Quando si ha un testo in una lingua rara il processamento può diventare mol- to complesso e costituisce da sempre un problema in quanto i lavori scientifici prodotti in ambito NLP riguardano al 90-95% la lingua inglese.
Un ultimo problema da tenere in considerazione raccoglie tutte quelle sfumature di una lingua comprendenti modi di di- re, usanze e convenzioni. Si tratta di concetti diversi che non sono lessicalizzati in tutte le lingue: come per esempio i concetti italiani di “Boh”, “Mamma mia”, “Abbiocco”, “Apericena”, “Tizio, Caio e Sempronio”, “Magari”, “Spaghettata” che non hanno diretti referenti in Inglese.
14

2 Significato del significato
2.5 Granularità
Come è stata già definita precedentemente, la granularità è la visione che si ha sulla struttura semantica e può avere una certa profondità. Può essere a livello di:
• Parola, complessità già elevata (es. triangolo semiotico). La word sense disambiguation avviene a livello di parola;
• Chunk, composizione di parole (per esempio aggettivo + nome). La multiword expressions avviene a livello di chunk;
• Frase, a questo livello si possono già risolvere problemi di question answering;
• Discorso, i chatbot si basano su un tipo di trattamento del linguaggio a livello di discorso;
• Documento, uno dei temi più importanti delle ricerche specifiche riguarda la summarization dei testi che lavora a livello di documento; i sistemi di summarizzazione di testi possono essere estrattivi o astrattivi: i primi estrapo- lano le parti di testo considerate più significative (usan- do WordNet, co-occorrenza di termini), mentre i secondi creano una rappresentazione di tutto il documento e ge- nerano nuove frasi non contenute nel documento origi- nale che però cercano di riassumere tutte le parti salienti della semantica contenuta all’interno.
• Collezione di documenti, a questo livello emerge il topic modeling che cerca di estrapolare gli argomenti principali contenuti all’interno dei documenti. Principalmente lo si fa per conoscerne l’evoluzione temporale.
15

2 Significato del significato
2.6 Word Sense Disambiguation
La Word Sense Disambiguation (WSD) riguarda l’identificazio- ne del senso di una parola polisemica in una frase, e quindi all’interno di un determinato contesto. Questo task ha delle problematiche di:
• Specificità. Prendiamo per esempio WordNet, in cui mol- ti sensi attribuiti alle parole non vengono mai utilizzati in quanto troppo specifici, anzi rappresentano del rumore nell’analisi del linguaggio naturale. Es. se si cerca il senso di “to play” vi sono più di trenta sensi differenti. Questo perché i linguisti hanno cercato di trovare tutte le pos- sibili sfumature di significato. Questo aspetto è criticato da molti ricercatori perché esistono 5 o 6 sensi tra questi (sempre in riferimento a “to play”) che rappresentano il concetto di suonare uno strumento (e sarebbero integra- bili in un senso solo). Ad es. “Suono la chitarra” e “Suono la chitarra ad un concerto” hanno un senso di suonare diverso all’interno di WordNet.
• Copertura. WordNet nonostante sia molto specifico sui sensi, in realtà ha molte falle in quanto possiede molte zone del linguaggio non coperte.
• Soggettività. Il team di WordNet ha dovuto decidere quanti e quali sensi attribuire alle parole. Nonostante sia stata una decisione collettiva, resta comunque soggettiva.
2.7 Word Sense Induction
La Word Sense Induction (WSI) riguarda l’identificazione auto- matica dei sensi di una parola dato il suo uso in tantissimi testi,
16

2 Significato del significato
catturando tutti quelli che sono i contesti che attribuiscono un senso preciso e differente dagli altri alla parola.
Per chiarire il concetto vediamo le differenze tra WSI e WSD:
• La disambiguazione deve “disambiguare” e ha bisogno di quello che viene chiamato Sense Inventory ovvero un di- zionario come WordNet che contiene tutti i possibili sensi per ogni parola, mentre nel WSI non esiste nessun dizio- nario, ma si cerca di comprendere il senso di una parola senza basarsi su alcuna risorsa.
• Nella WSI si cerca di comprendere il senso di una parola senza far riferimento a nessuna risorsa umana a differen- za della WSD, ma basandosi sull’effettivo uso delle parole all’interno di grandi quantità di dati.
• La WSD essendo fatta da linguisti è molto basata sulla grammatica, mentre la WSI è molto basata sull’uso delle parole, anche eventualmente sgrammaticato.
• La valutazione nella disambiguation è molto semplice, ma è criticabile allo stesso tempo perché come detto prima se si ha una frase come “Suono la chitarra ad un concerto” e il sistema attribuisce il senso di “Suonare la chitarra” al- l’interno di WordNet, viene considerato un errore. La va- lutazione nella WSI è più complicata perché non si ha a disposizione un dizionario che dica se il significato asso- ciato a quella parola è corretto o sbagliato. Ad esempio avendo a disposizione Wikipedia si cerca di catturare il significato di “to play”: quello che fa WSI è una specie di clustering ovvero guarda tutte le parole all’interno di una finestra che comprende al centro la parola “to play” e controlla con cosa co-occorre, quindi ad es. “instrument”,
17

2 Significato del significato
“guitar”, “piano”, “videogames”, “football”, ecc. Nel mo- mento in cui si mettono insieme tutti quei contesti simili si creano dei cluster coerenti e diventa possibile attribuire il senso di “to play” indotto dai dati. Vediamo ora però un metodo per la valutazione nella WSI.
2.7.1 Metodo della pseudoword
Questo metodo per la valutazione dei sistemi WSI è tanto ba- nale quanto efficace. Supponiamo di avere sempre Wikipedia. Vengono presi due termini in maniera casuale, ad esempio “ba- nana” e “cane” e ogni volta che occorrono “banana” e “cane” in Wikipedia vengono entrambi sostituiti con “bananacane”. Il sistema di valutazione applica il sistema WSI sulla parola “ba- nanacane” e cerca di capire se creerà due cluster: uno dove il significato di “bananacane” è quello di “banana” e l’altro di “ca- ne”. Sostanzialmente si crea una parola che non esiste unendo due parole e poi si valuta il WSI in base ai cluster che crea (cluster-to-class evaluation).
2.8 Definizione delle definizioni e ricerca onomasiologica
Si è già affrontato il tema della semantica lessicale nella parti del corso precedenti, ma è necessario fare una piccola digressione sulle risorse disponibili in questo senso:
• Dizionari elettronici: WordNet, BabelNet, . . .
• Risorse linguistico-cognitive: Property norms, sono degli studi cognitivi per comprendere diverse dinamiche, come
18

2 Significato del significato
per esempio lo studio della mutazione del linguaggio co- me conseguenza di un trauma. Sostanzialmente Vengono forniti dei questionari in cui si chiede di descrivere alcu- ne parole. Le risposte vengono poi aggregate e raccolte in grandi dataset, queste risorse sono molto promettenti e forniscono statistiche di quali sono le parole più frequenti che sono comparse nelle varie descrizioni.
• Common-sense knowledge: risorse che esprimono infor- mazione di tipo "common-sense". Un esempio è Concept- Net. Spesso queste risorse contengono informazione non disambiguata.
• Visual Attributes: risorse che si basano sulla fisionomia degli oggetti.
• Corpus managers: piattaforma in grado di gestire data- base testuali per la ricerca e l’esplorazione. Un esempion è SketchEngine.
Queste risorse lessico-semantiche si basano quasi tutte sul concetto di definizione, ovvero la descrizione di una parola attra- verso l’uso di altre parole. In tal senso, tutto ciò assomiglia ad un paradosso. Alcune domande che possono venire in mente sono le seguenti:
• Come si descrive un concetto?
• Quali caratteristiche sono più importanti?
• Che relazione c’è con un gruppo semantico più generale?
• Come si scrive una definizione? Come si valuta la qualità di una definizione? Quanto si è d’accordo?
Le risposte a queste domande verranno chiarite attraverso il primo esercizio di laboratorio.
19

2 Significato del significato
[LAB-1]
Misurazione dell’overlap lessicale tra una serie di definizioni per concetti generici/specifici e concreti/astratti. Si vedano le slides ed il materiale su Moodle per maggiori dettagli.
20

2 Significato del significato
[LAB-2]
I comuni dizionari a cui siamo abituati partono dalle parole, ov- vero dalla forma, per arrivare al contenuto. Esistono alcuni tipi di dizionario chiamati dizionari analogici che funzionano ”al contrario”, ovvero non si ricerca per parola ma per definizione. Questo tipo di ricerca viene chiamata ricerca onomasiologica, ovvero si parte dal contenuto per arrivare alla forma. Proprio su questo si basa la seconda esercitazione. Si vedano le slides ed il materiale su Moodle per maggiori dettagli.
21

3 Costruzione del significato
Abbiamo visto nel precedente capitolo le problematiche della lexical semantics basata sull’uso del lessico per comprendere il significato delle parole e su questo erano anche incentrate le prime due esercitazioni di laboratorio. In questo capitolo vedremo invece qualche teoria sulla costruzione del significato usando principi di composizionalità. In particolare tratteremo due teorie: il generative lexcon di James Pustejovsky, una teoria basata sui ruoli qualia, e la teoria delle valenze verbali di Patrick Hanks.
3.1 Pustejovsky
Pustejovsky ha proposto una teoria della semantica linguisti- ca chiamata “generative lexicon”1 che prevede l’utilizzo di una struttura basata su:
• Argument Structure, struttura che esprime il legame tra sintassi e semantica del concetto ovvero come si può map- pare quello che si vuole esprimere su quel concetto attra- verso l’uso di lettere, parole e grammatica.
• Event Structure, struttura che esprime tutti i tipi di eventi che coinvolgono quel concetto come lo stato, il processo o la transizione.
1 https://aclanthology.org/J91- 4003.pdf
22
 
3 Costruzione del significato
• Qualia Structure, esprime la struttura del concetto ov- vero come sono definite le sue caratteristiche. Queste caratteristiche vengono chiamate da Pustejovsky qualia.
• Inheritance Structure, struttura che colloca il concetto al- l’interno di una tassonomia per poterne discernere a gran- di linee già il significato (es. Se ho la parola “mango” devo sapere che è un frutto).
Quello che quindi sostiene Pustejovsky è che per poter ra- gionare semanticamente in maniera non solo precisa, ma anche completa e quindi per poter coprire tutti i tipi di ragionamen- to semantico a cui siamo abituati parlando abbiamo bisogno di formalizzare e definire precisamente tutte queste struttu- re. Per esempio se guardiamo WordNet esso copre solo una piccolissima parte di questa teoria ed è per questo che, secon- do Pustejovsky, non si riescono a gestire tutti i meccanismi di ragionamento.
3.1.1 Qualia Structure
Ci concentriamo adesso maggiormente sulla struttura più inte- ressante che è data dalla Qualia Structure. Secondo Pustejovsky esistono infatti quattro ruoli Qualia:
• Ruolo costitutivo, esprime la parte di composizione del concetto quindi è il ruolo più materiale che riguarda il peso, la dimensione e le parti che compongono il concetto.
• Ruolo formale, esprime tutte quelle caratteristiche che de- finiscono il concetto e lo distinguono dagli altri all’in- terno dello stesso dominio (es. Se parlo del mango il suo ruolo formale è dato da tutte le caratteristiche che lo contraddistinguono dagli altri frutti).
23

3 Costruzione del significato
• Ruolo telico, ciò che rappresenta l’obiettivo o la funzione del concetto, sul ruolo comportamentale del concetto (es. Il cane che abbaia o il mangiare una mela o il ruolo delle forbici di tagliare ecc.. sono informazioni di tipo telico).
• Ruolo ’agentive’, composto da tutta quella serie di entità spesso umane, ma che possono essere anche artificiali o eventi naturali che rappresentano l’origine del concetto.
Si tratta di una teoria formale in cui si assegna a ciascun elemento un ruolo e una struttura in base ai concetti definiti da Pustejovsky. Successivamente, ogni frase o asserzione che contiene uno di questi concetti può essere analizzata in mo- do formale attraverso un modello semantico basato sul lessico, utilizzando ragionamenti condizionali o relazionali rispetto ad altri concetti. In questo modo, si fornisce una rappresentazione formale della semantica delle frasi.
Il problema di questa teoria, elegante e potente, è che la sua complessità è elevata ed è difficile implementarla.
3.2 Hanks
Patrick Hanks è un altro linguista famoso che è diventato im- portante nella linguistica per una risorsa chiamata Corpus Pat- tern Analisys (CPA) che però non verrà trattata in questo corso. Ci concentreremo infatti solamente sulla sua teoria delle va- lenze (su cui si basa poi la risorsa citata) per la costruzione del significato perché è molto più semplice di quella di Pustejovsky e si può provare anche ad implementarla.
La sua teoria si basa sul concetto che il verbo sia la radice del significato, in quanto secondo Hanks non esiste un’espressione di significato senza un verbo. Infatti non si parla per sostantivi,
24

3 Costruzione del significato
ma si incastrano i sostantivi all’interno di strutture verbali e questo permette l’effettiva comunicazione. Partendo quindi dal verbo si va ad osservare la valenza di un concetto.
Figura 3.1: Valenze.
La valenza è la cardinalità degli argomenti che compongono la struttura di cui il verbo è radice. Un verbo può essere transi- tivo, intransitivo e a diversi livelli di transitività con n argomen- ti. “Piove” ad esempio è intransitivo quindi non ha argomenti, invece “Porto la macchina a Milano” ha tre argomenti per la radice del verbo portare: io, la macchina, a Milano. Tutto ciò cambia da lingua a lingua (ad es. ”it rains”).
Hanks afferma che dipendentemente dalla scelta del numero di argomenti utilizzati con un verbo si differenzia già il signi- ficato. Ogni valenza rappresenta quindi un numero di argo- menti che d’ora in poi verranno chiamati slot ed ogni possibile valore che questi slot possono assumere prende il nome di fil- ler. Quindi dato un verbo e una valenza Hanks introduce due concetti:
• Collocazione, rappresenta la combinazione di tutti i pos- sibili filler. Ad esempio, dato il concetto “Io vado lì” di
25
 
3 Costruzione del significato
valenza 2 vengono recuperati per ogni slot tutti i filler: per il primo slot l’insieme di tutti i soggetti per il verbo andare, per il secondo slot l’insieme di tutti i luoghi in cui quei soggetti possono andare. Tutte queste combina- zioni ovviamente vengono fatte tramite l’ausilio di grandi quantità di dati in modo tale da non effettuare combina- zioni illegittime, ma solo combinazioni aventi senso e che quindi sono presenti nei dati.
• Semantic Type, rappresentano delle macro-categorie che servono per raggruppare i vari filler. Ad esempio per le occorrenze “Lo studente va . . . ”, “Michele va . . . ”, ecc. La macro-categoria semantica associata a questi filler sarebbe “Persona”. Questo viene fatto per ogni slot.
A questo punto, una volta che si collezionano tutti i filler e si raggruppano per Semantic Type, Hanks dice che ogni signi- ficato costruito dipende dalla combinazione dei Semantic Type degli argomenti. Quindi se ho il Semantic Type di tipo 1 per il primo argomento e quello di tipo 3 per il secondo argomento, allora questa combinazione con due argomenti per quel verbo rappresenta un significato preciso.
Dopodichè si vanno a catturare le variazioni sintattiche, co- me ad esempio le forme attive e passive, che rappresentano lo stesso significato e le si accorpano in un un’unica combinazione riducendo così lo spazio tenuto da tutti i significati espressi da quel verbo.
3.2.1 Problematiche
Quali sono i Semantic Type? Quale dev’essere il grado di gene- ralizzazione?
26

3 Costruzione del significato
 Figura 3.2: Valenze, slots, fillers, semantic types. Le due righe (in verde) legate rappresentano ad esempio una va- riazione sintattica.
Non sempre si hanno sufficienti dati per tutte le parole, al- cune sono molto rare e non si verificano spesso. Questo pone difficoltà nell’analisi semantica di queste parole.
I termini nei dati possono non sovrapporsi anche se sono simili. I termini si riferiscono a concetti ad un certo livello di generalizzazione dipendentemente dal contesto.
Vediamo un esempio e consideriamo la frase: “The student went to school”. Quale può essere il Semantic Type per “student” con il verbo “went”? Quali proprietà di quel soggetto sono at-
27

3 Costruzione del significato
tivate da quel contesto? È uno Student? È un Person? O è un Living Entity? Molto probabilmente il contesto non è così astratto da considerare una tassonomia cosi alta come l’essere vivente, magari potrebbe bastare il supersense Person o magari in un contesto puramente scolastico basterebbe Student.
3.3 Affordance linguistiche
Un approccio risolutivo alle problematiche della teoria di Hanks è l’approccio basato sulle affordance linguistiche. Prima di par- lare del concetto di affordance linguistica è necessario introdur- re cosa si intende più in generale per affordance.
L’affordance è un termine inventato da uno psicologo ame- ricano amante delle scienze cognitive di nome James Gibson. La teoria dell’affordance dice che dato un oggetto percepibile con i sensi, anche se non lo si è mai visto prima, è possibile capire come utilizzarlo in quanto l’oggetto di per sé fornisce dei suggerimenti d’uso. (Es. Se l’oggetto è piatto, allora posso appoggiarci qualcosa sopra).
Questa teoria si può proiettare anche sul linguaggio e si può dire che ogni parola ha un suo possibile utilizzo e questo può essere intuito. Concentriamoci su questo aspetto partendo da un esempio.
Proviamo a considerare una parola di cui non conosciamo il significato come “grest”. Se proviamo a dire qualcosa su questa parola non ci viene in mente nulla proprio perché non la cono- sciamo. Ma se assumiamo che qualcuno conosca il significato di “grest” e ci dica: “Yesterday I saw a grest” a questo punto sia- mo in grado di associare delle proprietà a quella parola anche se continuiamo a non conoscerla. Sappiamo che è qualcosa che si può vedere quindi è probabile che sia qualcosa di concreto.
28

3 Costruzione del significato
Oppure questa persona ci dice “Yesterday I saw a grest with a telescope”, allora siamo sicuri che è concreto e se poi aggiun- ge “it was very fast”, allora sappiamo anche che può muoversi. Eccetera.
Questo serve per dire che il contesto descrive le proprietà dei concetti che non conosciamo e ci permette, almeno parzialmen- te, di capirli. Il contesto crea un’associazione tra le parole e le proprietà semantiche. Potenzialmente se il contesto è mol- to ampio saremmo in grado di utilizzare la parola sconosciuta anche senza sapere precisamente quello che significa.
Il nostro significato personale di una parola è dato dalle pro- prietà che diamo al concetto che probabilmente rappresenta: alcune proprietà possono essere conosciute, alcune proprietà possono essere assunte (in attesa di più informazioni contestua- li).
Inoltre, una parola in un contesto fa riferimento a una parti- colare granularità concettuale e questo dipende da quale delle sue proprietà viene attivata dal contesto. È così che la sua gra- nularità semantica cambia dinamicamente dall’interazione con gli altri oggetti. Questo è un po’ quello che ammette anche Hanks: il Semantic Type non può essere sempre lo stesso, ma cambia e varia in base al contesto in cui ci si trova. .
3.3.1 La potenza generativa dei pattern
Supponiamo di avere a disposizione un corpus e dei pattern, dove per pattern si intendono delle frasi che all’interno conten- gono dei jolly (*) (Es. Lorem * dolor sit *, consectetur adipiscing *).
Per linguistic instances si intendono tutte le occorrenze di quel pattern in un corpus, in quanto le parole all’interno del corpus faranno match con il pattern creando delle istanze com-
29

3 Costruzione del significato
plete. Ovviamente possono esserci più parole che faranno match con un solo asterisco e quelle saranno dunque i filler per quel jolly.
A questo punto, se si potessero associare tutte le proprietà (anche quelle non attivate dal contesto e appartenenti a ciascun filler possibile) a quei concetti che ricoprono l’asterisco, allora sarebbe possibile raggrupparle attraverso concettualizzazioni: questo significa che se si andrebbero a legare le co-occorrenze degli elementi per ciascun jolly, creando cluster. Questo è un po’ quello che fa Hanks, ma questa volta non viene fatto sulle parole ma sulle loro proprietà.
Questo ci dice che ogni volta che si ha un pattern che ha delle proprietà per un’asterisco, allora il jolly successivo avrà un’altro insieme di proprietà e così via. Questi cluster semantici non sono lessicalizzati, ma esprimono le proprietà che quel jolly deve avere per ottenere una frase di senso compiuto in quel contesto. Quindi il potere espressivo e generativo di un modello del genere diventa più potente di quello presentato da Hanks perché è possibile inserire qualsiasi parola, anche mai vista, nel corpus iniziale a patto che rispetti le proprietà.
Se si hanno degli overlap tra le proprietà in due frasi diverse, quindi con un lessico ed una sintassi diversa, allora è possibile dedurre una similarità semantica fra le due frasi e fra i singo- li costrutti utilizzati all’interno delle due frasi. Si sta dicendo quindi che quei due pattern esprimono un concetto simile.
Ma dove si prendono le proprietà? E i dati? Sicuramen- te sia le proprietà che i dati possono essere ricavati da risor- se linguistiche/semantiche come WordNet, FrameNet, VerbNet ecc. Ad esempio in WordNet, dato un senso esiste una rela- zione di iperonimia che denota le relazioni is-a e anche una relazione di meronimia che denota invece relazioni part-of. In questo modo è possibile ricavare tutte le relazioni paradigmati-
30

3 Costruzione del significato
che che ci interessano. All’interno invece di corpora si possono estrarre relazioni sintagmatiche, ovvero tutte le relazioni che derivano dall’uso congiunto delle parole all’interno di strutture lessico-sintattiche. Altre fonti per le proprietà possono possono essere:
• Questionari, indagini, studi cognitivi (come le property norms in Sezione 1.3)
• Proprietà latenti derivanti da similarità (derivate da misu- re statistiche)
• Ulteriori risorse linguistiche per i dati possono essere:
• Corpora (Wikipedia, etc.)
• Annotazioni manuali
• Open Information Extraction
• Hackathons
• Machine Learning
Oltre alla potenza generativa, un altro vantaggio dei pattern non lessicalizzati è costituito dal fatto che, a differenza dell’ap- proccio lessicalizzato di Hanks, non c’è bisogno di grandi quan- tità di dati in input perché l’andamento delle proprietà sui ter- mini segue una forma logaritmica. Per esempio supponiamo di aver definito dieci frutti e di aver definito le loro proprietà (hanno la buccia, sono succosi, ecc..), una volta che arriviamo ad aggiungere il millesimo frutto probabilmente utilizzeremo delle proprietà che abbiamo già visto, quando arriviamo all’ul- timo frutto probabilmente non si avranno più proprietà da ag- giungere. Quindi all’inizio ci sarà l’aggiunta di molte proprie- tà, ma arrivati ad un certo punto, anche se si continuano ad aggiungere termini non ci saranno più nuove proprietà.
31

3 Costruzione del significato
 Figura 3.3: Concept properties and terms.
Il modello sulle linguistic affordances è troppo complesso da implementare per lo scopo di questo corso, ma è possibile provare ad implementare un piccolo modello di Patrick Hanks (LAB 2).
Di seguito la lista dei supersenses di WordNet utilizzabili come Semantic Types.
32

[LAB-3]
3 Costruzione del significato
 Figura 3.4: Wordnet supersenses.
Si richiede un’implementazione della teoria sulle valenze di Pa- trick Hanks. In particolare, partendo da un corpus a scelta e uno specifico verbo (tendenzialmente non troppo frequente e/o generico ma nemmeno raro), l’idea è di costruire dei possibili cluster semantici, con relativa frequenza. Ad es., dato il verbo "to see" con valenza v = 2, e usando un parser sintattico (ad es. Spacy), si possono collezionare eventuali fillers per i ruoli di subj e obj del verbo, per poi convertirli in semantic types. Un cluster frequente su "to see" potrebbe unire subj = noun.person con obj = noun.arti f act.
Si richiede di partire da un corpus di almeno alcune centinaia di istanze del verbo.
33

4 Text Mining
Text Mining è il nome di una comunità di ricerca che dagli anni ’80/’90 applica gli avanzamenti tecnologici del Data Mining al testo. In questo capitolo analizzeremo meglio gli aspetti che la riguardano e ne studieremo le principali applicazioni.
4.1 Il testo secondo la statistica
In linea generale, si può affermare che l’approccio della lin- guistica computazionale classica, che si occupa dello studio di formalismi descrittivi per codificare il funzionamento del linguaggio naturale in programmi eseguibili dai computer, è di tipo top-down. Al contrario, l’approccio statistico è di ti- po bottom-up e si concentra sull’analisi qualitativa e quanti- tativa di fenomeni specifici per effettuare inferenze in modo automatico.
4.2 Rappresentazione vettoriale (Vector Space Model)
Per il Text Mining le parole sono token ovvero una sequenza di caratteri senza nessuna particolare valenza a livello lessicale.
Un testo quindi non è altro che un insieme di token e questi token possono avere una certa frequenza all’interno del testo.
34

4 Text Mining
 Figura 4.1: Approcci top-down e bottom-up al linguaggio natu- rale.
I testi così intesi possono essere espressi tramite la rappresen- tazione vettoriale inventata nel ’74 da Salton (padre dei motori di ricerca). Per creare questo vettore si parte per prima cosa dalla creazione di un dizionario che identifica tutti i token di- versi contenuti nel testo e ad ognuno di essi assegna un indice. Successivamente, nel caso più classico, si vanno a quantificare le occorrenze di ogni token nel testo ottenendo così un vetto- re numerico che codifica l’importanza di ogni parola nel testo (Figura 4.2).
Nel momento in cui si hanno a disposizione più testi (Cor- pus) quello che si ottiene è una matrice numerica. Ovviamente in questo caso il dizionario viene creato prendendo in conside- razione tutta la collezione documentale (i vettori devono ave- re medesima lunghezza) di conseguenza quest’ultima sarà una matrice sparsa.
Il vantaggio di questa rappresentazione è enorme in quanto ha creato un metodo velocissimo per poter reperire dati in po- chissimo tempo anche su una base documentale di miliardi di
35

4 Text Mining
 Figura 4.2: Rappresentazione testuale attraverso il Vector Space Model (VSM).
documenti. Questo perché si basa sul prodotto di vettori ed è possibile calcolare una sorta di similarità semantica utilizzando la Cosine Similarity (Misura coseno):
La Cosine Similarity non è altro che il prodotto degli elementi di una coppia di vettori diviso le norme dei due vettori. Questo semplice prodotto normalizzato è spesso utile per confrontare la similarità semantica fra due vettori, in quanto ogni documen- to viene visto come un punto all’interno di uno spazio multi- dimensionale e di cui ne viene disegnato il vettore (che parte dall’origine) e il coseno non fa altro che misurare l’angolo che si forma fra i due documenti all’interno dello spazio. Ad esem- pio se un documento parla molto di cani e un po’ di gatti e un altro documento parla più di gatti e un po’ di cani, allora quei due documenti per la formula coseno avranno un alto valore
36

4 Text Mining
di somiglianza perché anche se parlano di cose diverse a livello quantitativo sono uguali a livello proporzionale (possiedono lo stesso angolo all’interno del piano multidimensionale).
 Figura 4.3: Cosine Similarity.
I primi motori di ricerca avevano implementato un semplice Vector Space Model ottenendo un grande successo: veniva resti- tuita una lista di documenti "simili" ad una query data in input. Sebbene per noi oggi questo sia assolutamente naturale, è im- portante sapere la complessità del problema prima dell’arrivo di VSM + Cosine Similarity.
4.3 Metodi statistici
I metodi statistici applicati ai documenti di testo si concentrano essenzialmente su due tipi di informazioni statistiche:
• Frequenza di una parola in un testo: indica l’importanza, la rilevanza, la “dominance”, la significatività di un ter- mine nel testo. Le frequenze si sono rilevate non proprio adatte per creare sistemi di ricerca e di confronto seman- tico perché necessitano di essere normalizzate per poter essere confrontate (es. se si ha una frase e la si vuole con- frontare con un testo ovviamente i numeri saranno molto
37

4 Text Mining
diversi quindi sicuramente bisognerà normalizzare quei valori o per la lunghezza della frase o per la lunghezza del testo). Il TF-IDF è il valore che spesso viene incapsulato nei vettori, dove TF indica la Term Frequency opportuna- mente normalizzata sulla lunghezza del documento e IDF indica l’Inverse Document Frequency che esprime quan- to quella parola è presente nei vari testi a disposizione. Anch’esso è un rapporto ed è posto in forma logaritmica, esso avviene tra il numero di documenti nella collezione e il numero di documenti che contengono la parola. I fattori TF ed IDF vengono infine moltiplicati.
Figura 4.4: Strategia di pesatura TF-IDF.
• Co-occorrenza di due parole in un testo: indica la simi- larità tra due parole (sempre da intendersi come simila- rità statistico-semantica), assumendo che due parole con simile significato siano presenti negli stessi contesti. Le co-occorrenze vengono rappresentate all’interno di una matrice diagonale di dimensione
|D|x|D|
dove D è il dizionario. Rispettivamente nelle celle (i, j) verrà memorizzata la co-occorrenza dei termini i e j all’in- terno del contesto. Il contesto può essere a livello di frase, di paragrafo, di documento o di collezione di documen- ti. La co-occorenza così intesa come similarità permette di calcolare una Cosine Similarity molto più performante
38
 
4 Text Mining
in termini di risultati perché se per esempio si effettua la Cosine Similarity fra due documenti uno dei quali par- la di gatti e l’altro parla di mici allora questa ritornerà 0 anche se gatto e micio sono simili. Invece se si va a misura- re la co-occorrenza di gatto e micio questa avrà un valore probabilmente maggiore di 0.
Fino ad ora si è visto in generale che la semantica è un qual- cosa che varia rispetto alla granularità: se si affronta la granu- larità a livello di parola si parla di WSD; se a livello di discorso si parla di chatbot; se a livello di collezione di testi si parla di semantica documentale e quindi di Topic Modeling, tecniche che permettono di leggere tutta collezioni di documenti crean- do cluster semantici che individuano i topic (i temi) discussi al loro interno, ecc.
Viene presentata ora una piccola carrellata di applicazioni del Text Mining (alcune significative anche solo dal punto di vista storico dell’evoluzione tecnologica in questo ambito) .
4.4 Applicazioni
4.4.1 Tag Clouds
La prima banale applicazione sono le tag clouds dove in sostan- za l’associazione del peso di dominanza ad una parola viene espressa attraverso la grandezza della parola stessa all’interno di una nuvola di parole.
Questo è un modo per far capire velocemente di che cosa par- la una certa collezione di dati testuali. Dalla figura per esempio si può supporre che il testo parlasse delle elezioni al tempo di quando fu eletto Obama.
39

4 Text Mining
 Figura 4.5: Esempio di Tag Cloud.
Le tag clouds usano come tipo di informazione statica la fre- quenza delle parole. Ma se si volesse integrare al loro interno anche la co-occorrenza? In tal caso, la tag cloud riuscirebbe ad organizzare anche le parole in maniera che parole vicine abbia- no semantica simile. Per esempio, parole come Obama, president e McCain risulterebbero vicine nella tag cloud di Figura 4.4.1. In sostanza quindi la strategia di visualizzazione dovrebbe map- pare questi vettori di co-occorrenze all’interno di una strategia di collocamento nello spazio bidimensionale.
4.4.2 Tag Flakes
La seconda e meno banale applicazione sono le tag-flakes in cui vengono incluse le co-occorrenze. L’obiettivo è l’estrazio- ne automatica di una gerarchia di termini: si calcola un rank (ordinamento) delle parole basato sulla loro frequenza e poi si cerca di estrarre una denominazione del topic a cui si riferisco-
40

4 Text Mining
no, creando una sorta di raggio di diversità semantica basata sulla similarità reciproca dei termini.
Figura 4.6: Idea di segmentazione tematica di una tag cloud.
Potrebbe quindi essere possibile creare delle tag cloud dispo- ste in maniera gerarchica per organizzare le informazioni su dei topic: una volta estratti automaticamente i topic, si organizza la tag cloud in base ad essi, creando di fatto una struttura a sub- topic. In questo modo è quindi possibile visualizzare soltanto la parte del tema a cui si è interessati maggiormente.
La figura in basso ad esempio mostra dati presi da pagine di giornale relative all’uragano Katrina (con qualche misclassifica- tion, ad es. person-hurricane, spesso presente quando si tratta il testo come tokens analizzati solamente statisticamente).
4.4.3 Document clustering
Il clustering dei documenti non è da confondersi con la classifi- cazione perché per clustering si intende qualsiasi approccio non supervisionato di separazione di documenti in sotto-gruppi più o meno omogenei.
41
 
4 Text Mining
 Figura 4.7: Esempio di Tag Flake.
Figura 4.8: Concettualizzazione del clustering.
42
 
4 Text Mining
Non c’è bisogno di nessun training set ma vengono utilizzate soltanto frequenze, pesi associati alle parole, co-occorrenze, ecc. Esistono diverse tecniche ma tutte si basano sul concetto di di- stanza (inverso della similarità) tra due elementi. Ovviamente, i dati possono essere raggruppati in modi differenti a seconda della misura di distanza adottata.
Esistono due concetti fondamentali alla base del clustering:
• non esiste il clustering perfetto;
• non esiste sempre una misura oggettiva per valutare la bontà di un clustering: questo perché non si utilizzano delle etichette pre-impostate, a differenza per esempio della classificazione in cui è noto che un determinato testo appartiene ad una certa classe.
Figura 4.9: Pipeline con clustering + tag clouds.
Volendo quindi tornare all’esempio delle tag clouds, avendo a disposizione una base documentale è possibile effettuare un clustering separando i vettori sulla base della distanza recipro- ca tra di essi e poi fornire delle tag clouds mirate ad esprimere i vari cluster: in figura è stata utilizzata una pipeline esempli- ficativa che trasforma i testi in vettori, su cui viene applicato
43
 
4 Text Mining
il clustering prima della creazione delle tag clouds sui singoli clusters.
4.4.4 Document classification / categorization
Nel caso della classificazione (o categorizzazione) si vuole ri- condurre un testo ad una determinata etichetta (all’interno di un set di etichette, eventualmente organizzate in una tassono- mia o ontologia). La figura propone un esempio: una delle tantissime tassonomie esistenti che raccolgono tutto il web e lo incasellano in una delle categorie proposte.
 Figura 4.10: Classificazione di testi su labels (in questo caso, te- matiche, ma potrebbe essere ad esempio il "senti- ment" o altri tipi di classi).
Ovviamente, in caso di corpora di grandi volumi, la classi- ficazione non può essere eseguita manualmente: è necessario
44

4 Text Mining
qualche meccanismo automatico che prenda un testo, lo analiz- zi semanticamente e cerchi di capire in quale categoria collocar- lo all’interno di una tassonomia o ontologia pre-esistente.
Nel caso della classificazione è più semplice effettuare una valutazione perché, avendo una tassonomia già "popolata" con alcune istanze incasellate all’interno delle categorie, è possibile fare training con dei testi e poi valutare il modello appreso at- traverso altri testi (di test). Questo permette di dare un livello di confidenza più o meno alto ad un modello che effettua delle predizioni su documenti che non ha mai visto.
4.4.5 Document segmentation
Il task consiste nel separare le diverse parti all’interno di un documento (che può essere lungo anche decine di pagine) cer- cando di mantenere insieme aree semanticamente coerenti tra di loro: è una sorta di topic modeling ma sequenziale e intra- documentale. Esistono metodi automatici per la segmentazione basati su word count, co-occorrenze, ecc. Il più famoso algorit- mo è il text tiling: l’immagine che segue mostra un esempio di utilizzo.
Sull’asse delle x si ha l’indice del numero della frase (05 = quinta frase, 35 = trentacinquesima frase, ecc...) mentre sull’as- se delle y si hanno delle parole con la loro frequenza all’interno delle diverse frasi. Come è possibile notare dall’immagine di esempio, tra la 30sima e la 50sima frase sono usate esclusiva- mente le parole in basso; dalla sessantesima frase diventano più frequenti le parole poste in alto. Risulta quindi evidente un cambio di tematica all’interno del documento.
Il text tiling è di per se una tecnica molto semplice: l’obiettivo è la segmentazione di un documento e l’input dell’algoritmo è
45

4 Text Mining
 Figura 4.11: Esempio di segmentazione (preso dall’articolo del Text Tiling.
il numero di finestre da creare e la dimensione massima che devono avere. L’algoritmo si compone dei seguenti step:
• Separazione del testo in finestre di lunghezza fissa;
• Calcolo della coesione intra-gruppo: il valore di coesione è semplicemente quanto si usano le stesse parole tra bloc- chi successivi di frasi (o tokens). Facendo il plot di que- sto valore all’interno delle finestre si ottengono dei picchi, che possono essere usati per modificare successivamente le finestre;
• Ricerca di parti di testo a bassa coesione circondate da parti di testo ad alta coesione. I picchi più importanti sono quelli verso il basso: sono proprio le parti di testo a bas-
46

4 Text Mining
sa coesione circondate da parti di testo ad alta coesione. Vengono anche chiamati break points;
• Riadattamento delle finestre rispetto al break point più vicino: alla fine si ottiene qualcosa di simile a quanto mostrato in figura.
Figura 4.12: Break points nel text tiling.
L’algoritmo è iterativo: quella descritta è solo un’iterazione. Le successive iterazioni serviranno per raffinare sempre meglio le finestre, diventanto tendenzialmente sempre più accurate. Le iterazioni si bloccano quando non si raggiunge una certa con- vergenza o semplicemente le finestre non cambiano più dopo un massimo di iterazioni impostato.
Al termine delle iterazioni, le finestre non hanno più una dimensione prestabilita perché sono state riadattate in base ai break points.
47
 
4 Text Mining
4.4.6 Document summarization
L’obiettivo di questo task è la riduzione del testo, mantenen- do però la semantica contenuta al suo interno. Esistono due metodi per la document summarization:
• Estrattivi: dato un testo o una collezione di testi, i metodi estrattivi cercano di dare un valore di salience (impor- tanza) alle frasi all’interno del documento con l’obiettivo di creare il nuovo documento estraendo appunto le frasi con maggiore salience. Uno dei degli algoritmi storici è il TextRank che genera un rank delle frasi all’interno del documento sulla base della salience calcolata per ognuna di esse;
• Astrattivi: i metodi astrattivi sono generalmente molto più complessi di quelli estrattivi perché richiedono la ge- nerazione di nuovo contenuto testuale da inserire all’in- terno del riassunto.
Fino a qualche anno fa esistevano quasi soltanto metodi estrat- tivi ma ultimamente sono stati creati metodi astrattivi perfor- manti basati sull’utilizzo di reti neurali: si è scoperto che dan- do in input alle reti molti documenti associati al loro riassunto (spesso generati a mano), esse riescono ad imparare abbastanza bene a riassumere un nuovo documento.
La valutazione di un riassunto viene spesso effettuata me- diante ROUGE (Recall-Oriented Understudy for Gisting Eva- luation), un sistema che mappa i bigrammi e trigrammi dei riassunti fatti dagli umani con bigrammi e trigrami dei riassunti ottenuti automaticamente.
48

4 Text Mining
4.4.7 Information Retrieval
L’information retrieval è uno dei task principali del Text Mi- ning. Si basa sul recupero di un documento di interesse usan- do una query (set di keywords). Queste ultime possono essere anche relazionate a concetti ontologici o altri tipi di metadati.
Inizialmente, il task era basato su un modello booleano: si cercava un match più o meno diretto tra le keyword ed il con- tenuto dei documenti. Al giorno d’oggi il metodo si è evolu- to: non si basa più su semplice match booleano ma si effet- tuano delle analisi sofisticate che catturano la semantica conte- stualizzata delle parole nella query e la semantica generale dei documenti, a prescindere dalla sovrapposizione lessicale.
I possibili sviluppi sono:
• Navigazione aumentata: non si utilizzano solo testi o pa- role ma anche links, snippets, ecc, sommari generati au- tomaticamente, ecc.;
• Integrazione di immagini, video, mappe;
• Utilizzodimodelliavanzatidiinterazionecomeperesem- pio chatbot o set di domande e risposte pre-generate.
49

4 Text Mining [LAB-4a] (in alternativa al LAB-4b)
Si richiede un’implementazione di un sistema di text segmen- tation, prendendo ispirazione da TextTiling. In particolare, par- tendo da un corpus composto da almeno 3 sezioni su tematiche molto diverse (ad es. potete usare paragrafi da tre pagine di Wi- kipedia diverse), dovrete testare il vostro sistema in modo che riesca ad individuare le giuste linee di taglio (o quasi).
50

5 Semantica distribuzionale
Questo tema nasce e si evolve nella linguistica computazionale che è quasi completamente diversa dal Text Mining. Di fatto, ciò che è centrale nella semantica distribuzionale è l’uso del linguaggio, la linguistica, che è completamente assente nel Text Mining: si potrebbe dire che la semantica distribuzionale è una rivisitazione in chiave linguistica delle tecniche di Text Mining.
5.1 Citazioni da ricordare
Nel tempo la storia della semantica distribuzionale ha visto alcuni protagonisti:
• Harris, "Le parole che si verificano negli stessi contesti tendono ad avere significati simili": nel 1954 Harris per primo afferma che parole che occorrono in stessi contesti tendono ad avere significati simili;
• Firth, "Una parola è caratterizzata dalla compagnia che mantiene": similmente ad Harris, nel 1957, Firth afferma che una parola è caratterizzata da quelle che la accompa- gnano;
• Furnas, "L’uso congiunto delle parole serve principalmen- te a specificare più strettamente l’oggetto del discorso": nel 1983 Furnas sostiene che la congiunzione di varie pa- role permette di specificare più precisamente l’oggetto del discorso;
51

5 Semantica distribuzionale
• Deerwester, "C’è una struttura semantica latente sottostan- te nei dati che è parzialmente oscurata dalla casualità del- la scelta delle parole rispetto al loro recupero": nel 1990 Deerwester introduce i concetti latenti con cui si afferma che esiste una struttura di base che viene parzialmente oscurata dalla scelta "casuale" delle parole che viene fatta per comporre un discorso;
• Blei, "L’argomento del documento influenzerà in modo probabilistico la scelta delle parole dell’autore durante la scrittura del documento": nel 2003 Bley introduce una vi- sione probabilistica secondo cui il tema del documento influenza in modo probabilistico le parole utilizzate nel documento;
• Turney, "Coppie di parole che si verificano in modelli simi- li tendono ad avere relazioni semantiche simili": sempre nel 2003 Turney impiega per la prima volta il concetto di coppie di parole. L’intuizione è che se una certa coppia di parole
(x, y)
presenta più o meno gli stessi pattern di legame che pre-
senta la coppia
(w,z)
allora è possibile costruire una sorta di "proporzione" nu- merica nel significato delle due coppie (e.g. x : y = w : z). Ciò che viene catturato non è dunque la semantica concettuale ma la semantica relazionale.
52

5 Semantica distribuzionale
5.2 Aree diverse, nomi diversi
Bisogna anche dire che la semantica distribuzionale assume nomi diversi a seconda del contesto in cui viene utilizzata:
• Nell’ambito della linguistica si parla di Distributional Hy- pothesis: Harris e Firth ad esempio sono due linguisti;
• Nell’ambito del Natural language Processing si parla di Distributional Semantics;
• Nell’ambito dell’Information Retrieval si parla di Vector Space Models oppure di Latent Semantic Analysis;
• Nell’ambito delle Scienze Cognitive si parla di Concep- tual Spaces oppure di Graded Categorization: nel primo caso, i concetti interpretati dal punto di vista cognitivo sono definiti attraverso una serie di qualities (dimensioni, features) più percettive rispetto al concetto stesso mentre nel secondo caso si parla di tutti quegli studi che si basa- no su come il cervello interpreta gli oggetti (anche quelli non noti). Ad un oggetto viene attribuita una sorta di categorizzazione su alcune features;
• Nell’ambitodellaPsicometriasiparladiHyperspaceAna- logue to Language: si basa soprattutto su studi psicologici del linguaggio;
• Nell’ambitodellaGraphTheorysiparladimatricidiadia- cenza.
5.3 Perchè usare matrici?
Anche nel caso della semantica distribuzionale, vengono utiliz- zate le matrici per soddisfare diversi compiti. Ma come mai
53

5 Semantica distribuzionale
finora sono state utilizzate solo le matrici? E perché questo approccio funziona bene? La risposta ad entrambe le doman- de è legata a quanto segue: anche il linguaggio è un concetto approssimato, come lo è la rappresentazione vettoriale.
Le matrici si collocano a metà tra due rappresentazioni:
• Rappresentazione simbolica: sono proprie dei metodi for- mali di rappresentazione. Attraverso simboli è possibile effettuare ad es. delle inferenze logiche. La rappresen- tazione è di per sé potente ma povera a livello di infor- mazione: mentre è vero che anche le parole sono simboli, attraverso la rappresentazione vettoriale è possibile asso- ciare un significato numerico a tali simboli. Nella logica formale i simboli sono solo simboli che hanno un potere inferenziale all’interno di una base di conoscenza.
• Rappresentazione associazionistica/connessionistica: teorie più legate alle scienze cognitive in cui si pensa che tutto sia connesso con tutto, un po’ come se tutto fosse una seman- tic network. La fase di learning si basa sull’apprendere i pesi delle connessioni tra tutto con tutto.
Collocandosi a metà tra queste due rappresentazioni, le ma- trici facilitano la condivisione della conoscenza ed il significato diventa una regione geometrica. Le matrici infatti hanno lega- mi sia con i conceptual spaces (entità espresse attraverso quali- ties) sia con la prototype theory che deriva dalle scienze cogni- tive e che dice semplicemente che gli esseri umani hanno in mente delle categorie ma anche dei prototipi, ossia una spe- cie di centroidi per ogni categoria. Per esempio, tutti hanno in mente il prototipo per gatto che probabilmente non è un gatto Tonchinese.
54

5 Semantica distribuzionale
5.4 Parametri, tecniche, misure, operazioni...
Scegliendo la rappresentazione vettoriale, è anche importante capire quali tecniche poter utilizzare su questo tipo di dato:
• Similarità come per esempio la Cosine similarity, Jaccard similarity, ognuna delle quali è specifica per l’obiettivo che ci si propone. Strategie di pesatura come frequenze, tf-idf, ecc...
• Trasformazioni matriciali come SVD, NNMF, ecc...
• Clustering come K-means, EM, ecc... tutte quelle tecniche che permetto di raggruppare dati
Parte della letteratura si concentra sullo studio o anche com- binazione di questi parametri. In ogni caso comunque per ottenere i migliori risultati esistono delle operazioni di pre- processing che dovrebbero sempre essere applicate che si ba- sano sostanzialmente su normalizzazione e denormalizzazione di un testo:
• Normalizzazione: procedimento lessicale, sintattico o mor- fologico che si traduce nell’esecuzione di operazioni come tokenizzazione, stemming, lemmatizzazione. La norma- lizzazione è importante perché restringe la variabilità del linguaggio;
• Denormalizzazione: procedimento di arricchimento seman- tico attuato mediante named entities, semantic roles e qual- siasi tecnica che presa una parte di testo gli associa un’e- tichetta semantica. Con le named entities si tenta di as- sociare automaticamente un’etichetta a parti di frasi sulla
55

5 Semantica distribuzionale
base di luoghi, persone, organizzazioni, ecc; con le se- mantic roles si tenta di determinare i ruoli semantici delle parti di una frase. Questi ruoli sono solitamente "agent", "patient", "mean" (mezzo), ecc. Anche la WSD può es- sere considerata un processo di denormalizzazione se- mantica perché associa ad una certa parola un’etichetta che rappresenta il suo significato all’interno della frase. Al contrario della normalizzazione, la denormalizzazio- ne allarga la variabilità del linguaggio dal punto di vista semantico.
5.5 Configurazioni matriciali
Nell’ambito della semantica distribuzionale è importante il la- voro del già citato Peter Turney. In un suo articolo pubblicato nel 2010, cerca di mettere ordine sulle ricerche fatte fino a quel momento e fornisce una categorizzazione su alcune configura- zioni matriciali. In particolare, secondo Turney ne esistono tre tipi di configurazioni primarie:
• Term-Document Matrix: è il caso più semplice, considera i documenti espressi come termini. Su ogni riga dunque si ha un documento e su ogni colonna un termine. Questa configurazione viene utilizzata per effettuare diverse cose come per esempio il calcolo della similarità tra documen- ti, il clustering dei documenti, la classificazione dei do- cumenti, la segmentazione dei documenti e parzialmente anche la question answering intesa come i) analisi della domanda, ii) recupero del documento che contiene la ri- sposta, iii) recupero della frase che contiene la risposta ed infine iv) estrazione della risposta;
56

5 Semantica distribuzionale
• Term-Context Matrix: è la generalizzazione del tipo prece- dente di matrice e su ogni riga si ha un contesto mentre su ogni colonna un termine. L’idea è che un contesto non deve essere necessariamente un documento ma può an- che essere una frase, un paragrafo, una dipendenza sin- tattica, ecc. Questa configurazione viene spesso utilizzata per calcolare la similarità tra parole, per fare clustering e classificazione di parole, per la WSD, per l’Informatoon Extracion, ecc.
• Pair-Pattern Matrix: è la proposta da Turney. In queste matrici su ogni riga si hanno coppie di parole mentre su ogni colonna si ha un pattern. Questi ultimi metto- no insieme tutte le possibili coppie di parole attraverso delle relazioni come per esempio "X è risolto da Y", "X causa Y", "X è causato da Y" che vengono poi enumerate in base al peso associato alla specifica relazione. Queste enumerazioni vanno poi ad incastrarsi nello spazio vet- toriale che definisce la matrice. Ma perché usare coppie di parole piuttosto che per esempio triple di parole? Le motivazioni si basano sostanzialmente sul costo compu- tazionale che sarebbe veramente eccessivo e sul fatto che si otterrebbero matrici decisamente troppo sparse. Que- sta configurazione viene spesso utilizzata per calcolare la relational similarity (la similarità di una coppia x e y con un’altra coppia w e z), la pattern similarity (fondamen- talmente clustering su pattern che legano coppie simili), la relational clustering/classification e la relational search (ad es. per rispondere a domanda tipo "individua la lista di tutte le X tali che X causa il cancro". Non si cerca più un semplice documento che parla di X ma si prendono tutti i concetti che sono in relazione con X.
57

5 Semantica distribuzionale
Nota bene: la relational similarity è utile perché se per esem- pio si effettua un calcolo attraverso la Cosine similarity tra due coppie di parole e questa restituisce un risultato alto di similari- tà, vuol dire che quelle due coppie di parole sono legate lessico- sintatticamente più o meno allo stesso modo nei documenti e quindi tendono ad avere una semantica relazionale simile.
5.6 Il ruolo della similarità
Il concetto di similarità è fondamentale tanto che spesso la se- mantica distribuzionale viene anche chiamata informalmente semantica della similarità. Lato "scienze cognitive" si ragiona sulla similarità intesa come processo mentale in quanto ogni giorno tutti gli esseri umani usano principi di similarità: il mon- do che un uomo percepisce, le parole ed i concetti che impara vengono sempre associati a qualcosa che è già presente nella mente. Esiste quindi costantemente un rapporto di similarità tra ciò che si vive e ciò che è già stato vissuto.
Negli anni ’60, Quine1 asserisce che la similarità è fondamen- tale per l’apprendimento e per il pensiero e inoltre consente di categorizzare le cose in modo che possano funzionare co- me significati, come stimoli. L’uomo infatti apprende da eventi passati e riesce a prendere decisioni su eventuali eventi futu- ri simili; è in grado cioè di prevedere le funzionalità di oggetti sconosciuti se questi appaiono simili a qualcosa che già conosce.
All’interno della linguistica computazione sono stati definiti diversi tipi di similarità2:
1Nel libro Word and Object.
2E’ importante sapere tutte queste definizioni e le varie differenze, facendo
eventualmente degli esempi.
58
 
5 Semantica distribuzionale
• Semantic similarity: prende spazio nella maggior parte de- gli articoli ed è quella più usata (e mal interpretata). Que- sto tipo di similarità intende concetti che hanno (quasi) lo stesso significato (sinonimia);
• Semantic Relatedness: spesso viene confusa con la simila- rità semantica. Questo tipo di similarità intende concetti che condividono delle proprietà, che hanno quindi una qualche affinità semantica. Possono essere meronimi, an- tonimi ma anche sinonimi (esattamente come nella simi- larità semantica). Il fatto che i concetti possano essere an- che sinonimi permette di capire il motivo per cui le due semantiche vengono spesso confuse. Inoltre è possibile affermare che ciò che è semanticamente simile è anche semanticamente relazionato ma in generale non è vero il contrario (e.g. ruota e camion non sono simili semantica- mente ma sono relazionati semanticamente). Questo tipo di semantica è quasi inutilizzabile perché restituisce co- me output il fatto che due concetti siano relazionati ma non specifica il motivo per cui lo sono (è spesso troppo generica);
• Attributional similarity: è praticamente la stessa definizio- ne precedente, anche se forse questo nome sarebbe più appropriato. Questo tipo di similarità intende concetti che condividono degli attributi (appunto delle proprietà). In letteratura comunque si preferisce utilizzare il nome semantic relatedness;
• Taxonomical Similarity: più facilmente misurabile perché riguarda quei concetti che condividono degli iperonimi;
• Relational Similarity: lavora su coppie (o tuple) di concetti;
59

5 Semantica distribuzionale
• Semantic Association: termine utilizzato maggiormente nel- le scienze cognitive piuttosto che nell’NLP. Tratta concetti che co-occorrono frequentemente. E’ molto simile alla re- latedness ma la differenzia sostanziale sta nel fatto che la semantic association è relativa alle co-occorrenze di con- cetti (ad es. culla e neonato). Ai dice essere orientata alla corpus analysis, mentre la relatedness è un qualcosa di più generico che prescinde dalla co-occorrenza. E’ possi- bile quindi avere dei concetti che non co-occorrono molto spesso insieme (e quindi hanno una semantic association bassa) ma che sono comunque relazionati semanticamen- te.
Sulla base delle tante definizioni associate alla similarity è possibile effettuare due considerazioni: la prima è che la di- stributional semantics ha come cardine proprio la similarità; la seconda è che tale concetto è molto fragile perché persone di- verse possono attribuire diversi valori di similarità (a causa di tutte le definizioni appena viste).
5.7 Problemi: word order e rappresentazione non composizionale
Il problema principale delle rappresentazioni matriciali è che non tengono in considerazione l’ordine delle parole nel testo. Studi abbastanza recenti sulla lingua inglese hanno dimostrato che rappresentare il testo attraverso matrici permette di rag- giungere al massimo un livello di 80% di accuratezza, cioè è possibile esprimere circa l’80% del contenuto. Dal punto di vi- sta dell’Information Retrieval, questo è comunque un buon ri-
60

5 Semantica distribuzionale
sultato: il concetto che si vuole esprimere può essere compreso (recuperato) anche se si iniziasse a parlare senza dare un ordine ed una grammatica alle parole. Ovviamente non si capirebbero tutte le parole o le frasi ma basterebbe capire la maggior parte di esse per riuscire ad estrapolarne il concetto. Per esempio, la lingua Warlpiri è una lingua australiana che è completamente order-free.
Per porre rimedio a questa situazione sono state pensate al- cune soluzioni come per esempio l’uso di matrici pair-pattern che sono un pò più sensibili all’ordine oppure l’introduzione di vettori ausiliari detti vettori di ordinamento volti a fornire informazioni aggiuntive sull’ordine.
Il secondo problema è che la rappresentazione matriciale non è composizionale: le rappresentazioni matriciali sono piuttosto orientate al significato di singole parole. Il linguaggio o la se- mantica invece sono composizionali: la composizione di parole crea nuovo significato che può essere anche diverso dal signi- ficato singolo delle parole. La possibile soluzione che viene spesso adottata è la combinazione di vettori per creare costrut- ti più complessi e quindi per generare nuovo significato. Ad esempio, si usa il vettore di "vino" e lo si combina con quello di "rosso" per creare un nuovo vettore che approssimi il significato di "vino rosso".
61

5 Semantica distribuzionale [LAB-4b] (in alternativa al LAB-4a)
Si richiede un’implementazione di un metodo per la generazio- ne di una nuova lingua (che chiameremo NL). In particolare, partendo da una lingua di partenza L1 (ad es. la lingua Ingle- se), si prendano i termini di L1 usando un dizionario elettronico (ad es. WordNet o BabelNet). Per ogni termine t ed i suoi sensi St, dovrete cercare un nuovo termine tt (in una seconda lingua L2 a vostra scelta) da accoppiare a t, per la costruzione del ter- mine t − tt, da inserire in NL. Il termine tt in L2 va selezionato tra quelli meno ambigui per il concetto St di riferimento. Si ri- chiede di calcolare un valore di riduzione dell’ambiguità della nuova lingua rispetto a quella di partenza (ad es. calcolando il numero di sensi associabili ai termini t − tt in NL rispetto a quelli associabili ai termini t in L1. Una volta implementato il sistema, potrete cambiare la lingua L2 per valutare il potere disambiguante di diverse lingue rispetto a quella di partenza L1.
62

6 Semantica documentale
La semantica documentale riguarda tutto quel tipo di analisi e di ricerca che si effettua al livello di collezione di documenti. In questo capitolo verrà discusso il Topic Modeling, il Dynamic Topic Modeling e verrà fatto qualche cenno anche sulla Text Visualization.
6.1 Topic Modeling
Che cos’è un topic model? Un topic model è un modello stati- stico o probabilistico che analizza l’uso del linguaggio ed indi- vidua automaticamente gli argomenti di una collezione di testi (o base documentale).
Il topic model è un modello non supervisionato, non neces- sita quindi di alcun tipo di annotazione manuale1.
In particolare, un topic è rappresentato da una lista pesata di parole. Quindi non va pensato in termini di argomento strut- turato: tale passaggio, solitamente è lasciato all’interpretazione e annotazione manuale. Uno dei problemi di queste tecniche è proprio il fatto che non sempre l’interpretabilità di questi topic è ovvia. I topic che vengono estratti, dipendentemente dalla tecnica di topic modeling utilizzata, misurano quanto i termi- ni vengono usati negli stessi contesti. Quindi è possibile che i topic estratti non siano utili, e che rappresentino una sempli- ce coincidenza statisticamente significativa (ad es. collezioni di
1Esistono anche versioni semi-supervisionate.
63
 
6 Semantica documentale
function words, eventi anomali, numeri, combinazioni lessicali accidentali e non di contenuto).
6.2 Latent Semantic Analysis
La prima tecnica di topic modeling adottata in letteratura pren- de il nome di Latent Semantic Analysis (LSA). È l’applicazione di una fattorizzazione matriciale chiamata Singular Value De- composition (SVD) che prende in input una matrice, nel nostro caso l’insieme dei vettori del dizionario contenti le frequenze normalizzate, e crea in output tre matrici approssimando la matrice di partenza.
La prima matrice è una nuova rappresentazione multidimen- sionale dei testi (stesse righe), ma con nuove features che ven- gono chiamate concetti latenti. La terza matrice è una nuova rappresentazione multidimensionale delle features latenti, ma trasposta. La seconda matrice è più particolare, contiene 0 in tutte le celle eccetto per la diagonale. La diagonale contiene i cosiddetti singular values.
Cerchiamo di capire meglio cosa produce l’esecuzione di SVD: data la matrice di partenza, quindi sapendo quali sono le fre- quenze delle parole, essa riesce a identificare automaticamente delle ridondanze. Nel caso di matrici term-document come in- put, viene catturata l’informazione di co-occorrenza di parole X con parole Y, creando nuove dimensioni/features che accorpa- no tali co-occorrenze, dando vita a concetti latenti in un nuovo spazio vettoriale. In maniera più formale, l’SVD cattura combi- nazioni lineari delle features iniziali combinandole in features uniche nel nuovo spazio latente.
Questi concetti latenti hanno la qualità di essere ordinati, dal concetto latente più importante (che esprime maggiore varian-
64

6 Semantica documentale
za nel dataset originario) a quello meno importante. Questo permette di poter troncare le matrici considerando solamente le prime dimensioni. L’SVD permette quindi di approssima- re la matrice di partenza in altre matrici, molto più piccole, che rappresentano il contenuto numerico espresso nella matri- ce di partenza con minor uso di dimensioni (riduzione della dimensionalità).
Spendendo qualche parola in più a favore della fattorizzazio- ne matriciale possiamo dire che essa effettua un’analisi delle varianze e una riorganizzazione del contenuto matriciale sotto forma delle varianze maggiori verso quelle minori.
L’SVD applicata ad una matrice che rappresenta testi ha due vantaggi principali: permette di avere molte meno dimensioni e riduce la sparsità dei dati (si passa da una matrice sparsa ad una matrice densa, dando luogo anche valori negativi e questo ne rappresenta un difetto). I nuovi vettori hanno la particola- rità di non essere interpretabili ma allo stesso tempo di essere molto più potenti del dizionario iniziale perché permettono di valutare una similarità lessicale indiretta.
Immaginiamo di avere due documenti d1 e d2 all’interno di una grande base documentale. Se si effettua la Cosine Similari- ty tra d1 e d2 (le celle bianche in figura indicano valori uguali a 0) il risultato sarà zero, ovvero i due documenti saranno identi- ficati come semanticamente dissimili. Ma se i termini in grigio iniziali di d1 e i termini grigi finali di d2 analizzati dal calcolo dell’SVD vengono accorpati all’interno di singoli concetti laten- ti (poiché co-occorrono all’interno di un contesto indiretto, cioè negli altri documenti che non sono nè d1 nè d2), diventando una nuova dimensione della matrice finale (in arancione, in figura). Su questo nuovo spazio vettoriale, la Cosine Similarity darà un risultato di similarità maggiore di 0, indice di una similarità positiva per quei due documenti.
65

6 Semantica documentale
 Figura 6.1: Contesti indiretti e concetti latenti con SVD.
La Latent Semantic Analysis però ha alcuni problemi: i) è un modello che non generalizza su documenti non visti, ov- vero se si aggiungono documenti alla base documentale allora bisogna ricalcolare l’intero SVD; ii) valori negativi dopo la fatto- rizzazione sono di non facile interpretazione semantica; iii) esi- stono comunque varianti di questo approccio come la NNMF (non-negative matrix factorization).
Un’evoluzione della Latent Semantic Analysis è stata la Pro- babilistic LSA (pLSA), che a sua volta venne generalizzata dalla Latent Dirichlet Allocation (LDA). La LDA sfrutta la statistica Bayesiana e si basa sull’assunto che un documento sia un mix di topics e ogni parola ha una certa probabilità che compaia in ogni singolo topic. Questa è un’assunzione importante perché aggiunge la caratteristica generativa al modello: da un insieme di parole è possibile dedurre sia il topic di appartenenza sia
66

6 Semantica documentale
 Figura 6.2: Esempio di topics estratti da una collezione di testi [Blei].
altre parole che molto probabilmente verranno scritte in quei documenti topic-related.
Un sistema di topic modeling cattura quindi cluster di pa- role affini che molto probabilmente esprimono un argomento all’interno di una collezione di testi. Con l’evoluzione del topic modeling è nata anche una sotto-area di ricerca chiamata Dy-
67

6 Semantica documentale
namic Topic Modeling (DTM) dove non solo vengono estratti automaticamente i topic ma vengono proiettati nel tempo per poter catturare la loro evoluzione.
6.3 Text Visualization
Un problema comune a tutto il text mining e alla semantica do- cumentale è quello di rappresentare eventualmente uno spazio a n dimensioni (quello dei termini) in uno spazio bidimensiona- le. In un’altra area di ricerca, la Text Visualization, si studia come visualizzare il testo in modo tale da riuscire a trasferire un certo contenuto semantico espresso in una base documentale ad un utente attraverso grafici.
La Text Visualization utilizza diverse strategie di mapping multidimensionale (fattorizzazioni matriciali, Multi Dimensio- nal Scaling (MDS), e approcci grafici come Parallel Coordinates, RadViz, HeatMap, Correlation Circle, ecc.).
Vediamo alcuni di questi approcci grafici che nel corso del tempo si sono evoluti e che ci permettono di capire come uno spazio multidimensionale si possa mappare su un plot a due dimensioni.
6.3.1 Parallel Coordinates
Ogni dimensione viene associata ad una coordinata parallela alle altre (in figura si può vedere uno spazio a 4 dimensioni). Un punto nello spazio multidimensionale diventa una retta che congiunge i valori per quelle dimensioni. Se si colorano diffe- rentemente le linee in base alla loro posizione/trend si possono identificare dei cluster semantici che rappresentano gli argo- menti di un testo. Esistono anche delle varianti che collassano
68

6 Semantica documentale
varie righe e le comprimono per migliorare la visibilità. Il pro- blema di questo approccio è che se ci sono tantissimi dati è necessario comprimerli.
Figura 6.3: Esempio di visualizzazione con Parallel Coordinates a 4 dimensioni.
 69

6 Semantica documentale
 Figura 6.4: Esempio di visualizzazione con Parallel Coordinates su documenti di testo, usando tecniche di clustering visivo.
6.3.2 RadViz
La Radial Visualization o RadViz è un altro schema di mapping dove le dimensioni vengono inserite all’interno di una circonfe- renza. I punti all’interno sono le istanze (ad es. dei documenti) mentre la loro posizione deriva dall’attrazione (valore di im-
70

6 Semantica documentale
portanza) delle dimensioni/features poste sulla circonferenza. Nell’esempio in figura, i punti rossi avranno ad esempio un al- to valore per quella dimensione e un basso valore per le altre in alto.
Si utilizza quindi un semplice concetto di gravità. In questo caso ogni feature rappresenta un punto di attrazione in un cer- chio e i vari documenti si dispongono a una distanza da tali punti in modo proporzionale a tali attrazioni.
Il problema di RadViz è il conflitto gravitazionale tra feature differenti. Ad es., se si ha un valore alto per il topic 6 in figura, e un valore alto per il topic 1, l’attrazione si annulla.
Figura 6.5: Esempio di visualizzazione con RadViz.
71
 
6 Semantica documentale
6.3.3 HeatMap
Nelle HeatMap i dati vengono espressi attraverso una matrice e il colore o l’intensità del colore esprime il valore numerico.
Figura 6.6: Esempio di visualizzazione HeatMap.
6.3.4 Correlation Circle
I Correlation Circle sono semplicemente varianti di cerchi in cui sulla circonferenza vengono posti degli elementi (ad esem- pio termini, o topic) e i collegamenti fra questi rappresentano la loro correlazione. Questo è quindi un modo per visualiz- zare correlazioni, co-occorrenze, ecc. I termini possono anche essere organizzati gerarchicamente, come nel caso della figura sottostante.
72
 
6 Semantica documentale
 Figura 6.7: Esempio di visualizzazione con Correlation Circle.
[LAB-5]
Si richiede un’implementazione di un esercizio di Topic Mo- deling, utilizzando librerie open (come ad es. GenSim2). Si richiede l’utilizzo di un corpus di almeno 1k documenti. Testa- re un algoritmo (ad esempio LDA) con più valori di k (num. di topics) e valutare la coerenza dei risultati.
2 https://radimrehurek.com/gensim/
73
 
7 Text2Everything: Non Solo Testo
Il Natural Language Processing è una disciplina il cui focus è principalmente sul testo e le relazioni che si possono inferire da/verso esso. Ciò non toglie che durante gli anni, anche gra- zie all’ascesa di modelli neurali pre-trained, la disciplina sia cambiata drasticamente. Questi cambiamenti impongono an- che uno shift di prospettiva non banale: si è passati dall’inve- stigare il testo come modalità di comunicazione umano-umano ad utilizzarlo come mezzo per triangolare la relazione uomo- macchina-ambiente. Inoltre, è importante considerare un altro importante cambiamento, ovvero quello della multi-modalità. L’NLP non è più solamente testo, ma anche immagini, concetti, audio, video e tutte le relazioni che possono esistere tra queste modalità.
In questo capitolo, perciò, esploreremo la rivoluzione coper- nicana che rappresentano i pre-trained models: come impara- no, vengono allenati e raffinati e cosa vuol dire fare inferenza. Parleremo inoltre del problema dell’allineamento e della gran- dezza, due problematiche presenti e future. Infine, concludere- mo parlando di modelli multi-modali.
74

7 Text2Everything: Non Solo Testo
7.1 Meccanismi di apprendimento per modelli
Come per gli umani, anche le macchine hanno bisogno di im- parare prima di produrre. La domanda fondamentale è: come? Esistono diverse metodologie, alcune con il supporto umano, altre meno, di cui andiamo ora a parlare.
Nel mondo dell’intelligenza artificiale, l’apprendimento au- tomatico rappresenta un campo fondamentale e affascinante. Esistono vari tipi di apprendimento automatico, ognuno con le proprie caratteristiche, vantaggi e applicazioni pratiche. Oggi esploreremo alcuni di questi tipi, includendo l’apprendimen- to supervisionato, semi-supervisionato, debole supervisionato, auto-supervisionato e per rinforzo.
Iniziamo con l’apprendimento supervisionato. Questo ti- po di apprendimento richiede dati etichettati, ossia coppie di input-output predefinite. Il modello, quindi, impara a prevede- re gli output basandosi sugli input dati.
In contrasto, l’apprendimento semi-supervisionato utilizza una combinazione di dati etichettati e non etichettati. Il modello impara dai dati etichettati e sfrutta i dati non etichettati per migliorare la generalizzazione.
L’apprendimento debole supervisionato rappresenta un’ul- teriore variante. In questo caso, i dati di addestramento hanno etichette rumorose o imprecise, e il modello deve imparare a prevedere gli output nonostante queste incertezze.
Passando a un altro tipo di apprendimento, l’apprendimento auto-supervisionato è caratterizzato dal fatto che il modello genera la propria supervisione prevedendo parti dell’input. In questo caso, non è necessario disporre di dati etichettati. Un esempio è l’addestramento di un modello transformer per la
75

7 Text2Everything: Non Solo Testo
comprensione del linguaggio prevedendo parole mascherate in una frase, come avviene con BERT.
L’apprendimento per rinforzo, invece, riguarda il modello che impara interagendo con un ambiente e ricevendo feedback. Il modello ottimizza le proprie azioni per massimizzare una ri- compensa. Un esempio può essere addestrare un modello tran- sformer per giocare a un gioco basato su testo, dove il modello riceve ricompense per aver preso decisioni corrette nel gioco.
Infine, è importante menzionare il concetto di pre-training e fine-tuning. Il primo è un framework di inizializzazione che viene utilizzato in congiunzione con task downstream di fine-tuning. In questo schema, i parametri del modello ven- gono allenati su task pre-impostati nel pre-training per cattu- rare attributi specifici, strutture e informazioni, per poi essere ulteriormente perfezionati su task specifici durante la fase di fine-tuning.
7.2 Pre-training e fine-tuning
Il pre-training nelle reti neurali è un processo in cui una rete viene inizialmente allenata su un compito utilizzando un set di dati specifico, e i pesi ottenuti da questo allenamento vengono salvati. Questi pesi salvati vengono quindi utilizzati come pun- to di partenza per l’allenamento di una rete diversa su un nuo- vo compito. Questo metodo ha vantaggi rispetto all’inizializ- zazione casuale dei pesi, poiché fornisce alla rete un vantaggio iniziale, come se avesse già visto i dati.
Il fine-tuning, d’altra parte, è una fase già finale. Dopo che il modello ha acquisito una comprensione generale dei dati attra- verso il pre-training, viene addestrato su un compito specifico utilizzando un set di dati più piccolo e spesso etichettato. Que-
76

7 Text2Everything: Non Solo Testo
sto è simile a come un bambino, dopo aver appreso la struttu- ra del linguaggio, inizia a imparare compiti più specifici come scrivere saggi o risolvere problemi matematici.
In altre parole, il pre-training insegna al modello le regole generali dei dati, mentre il fine-tuning lo addestra su un com- pito specifico, consentendogli di applicare efficacemente queste regole generali in un contesto specifico.
Nell’ambito del pre-training esistono diverse tecniche e me- todi che si utilizzano per allenare i modelli. Tra questi ci sono il Masked Language Modeling (MLM), il Denoising AutoEnco- der (DAE), il Replaced Token Detection (RTD), il Next Sentence Prediction (NSP) e il Sentence Order Prediction (SOP).
Il Masked Language Modeling (MLM) è un metodo di pre- training in cui alcune parole vengono cancellate randomica- mente dalla sequenza in input. Il modello viene quindi ad- destrato a prevedere le parole cancellate sulla base del contesto fornito dalle parole rimanenti. Questo metodo permette al mo- dello di apprendere la struttura e la sintassi del linguaggio in modo efficace.
Il Denoising AutoEncoder (DAE), d’altra parte, è un approc- cio che mira a migliorare la robustezza del modello introdu- cendo deliberatamente del rumore nei dati di addestramento. Durante il pre-training, al corpus originale viene aggiunto del rumore, e il modello deve cercare di ricostruire l’input origi- nale a partire da questo corpus rumoroso. Questo permette al modello di apprendere le caratteristiche salienti dei dati pur essendo esposto a variazioni e distorsioni.
Il Replaced Token Detection (RTD) è un task discriminativo in cui il modello deve determinare se un token nella sequen- za è stato sostituito da un altro token generato da un model- lo di linguaggio. Questo tipo di compito aiuta il modello a comprendere meglio il contesto e la coerenza del linguaggio.
77

7 Text2Everything: Non Solo Testo
Il Next Sentence Prediction (NSP) è un altro metodo utiliz- zato per addestrare i modelli a capire la correlazione tra due frasi. In questo task, vengono introdotte due frasi da docu- menti diversi e il modello deve determinare se l’ordine delle frasi è corretto. Questo metodo aiuta il modello a catturare la rappresentazione a livello di frase e a comprendere le relazioni semantiche tra le frasi.
Infine, il Sentence Order Prediction (SOP) è una variante del NSP. Invece di utilizzare frasi da documenti diversi, il SOP uti- lizza due frammenti contigui di un documento come campioni positivi e due frammenti con l’ordine invertito come campioni negativi. Questo aiuta il modello a comprendere la coerenza e la sequenzialità all’interno di un singolo documento.
7.3 Task multi-modali in inferenza
Andiamo qua ad elencare solo alcuni tra i task più citati quando si parla di inferenza su modelli pre-trained.
7.4 PTM: Pre-Trained (Language) Models
I modelli di linguaggio pre-addestrati sono basati sulla capacità di predire una parola in un testo dato il contesto circostante. Ad esempio, data la frase "Il cielo è ___", un modello di linguaggio dovrebbe essere in grado di completare la frase con una parola appropriata come "blu".
Il processo di pre-addestramento consiste nel far "leggere" al modello miliardi di frasi, permettendogli di imparare una va- rietà di informazioni linguistiche, come la sintassi, la semantica,
78

7 Text2Everything: Non Solo Testo
Tabella 7.1: Alcuni Task in inferenza
                                                 79
Compito
Descrizione
Modalità
Classificazione del testo
assegnare un’etichetta a una data sequenza di testo
generare testo dato un prompt
generare un riassunto di una sequenza di testo o documento
NLP
Generazione di testo Riassunto
NLP NLP
Classificazione delle imma- gini
Image segmentation
assegnare un’etichetta a un’immagine
Visione artificiale Visione artificiale
Rilevamento degli oggetti
assegnare un’etichetta a ogni singolo pixel di un’immagine (supporta segmentazione semantica, panoramica e istantanea) prevedere le caselle delimitatrici e le classi degli oggetti in un’immagine
Visione artificiale
Classificazione audio Automatic speech recogni- tion
Risposta alle domande visi- ve
Document question answe- ring
Didascalia delle immagini
assegnare un’etichetta a alcuni dati audio trascrivere il discorso in testo
Audio Audio
rispondere a una domanda sull’immagine, data un’immagine e una domanda rispondere a una domanda su un docu- mento, data un’immagine e una domanda generare una didascalia per una data im- magine
Multimodale Multimodale Multimodale

7 Text2Everything: Non Solo Testo
il contesto delle parole, e anche alcune nozioni di conoscenza generale. Questo permette al modello di generalizzare a nuovi compiti con meno dati.
Dopo il pre-addestramento, il modello può essere affinato o fine-tuned su un compito specifico, come la classificazione del testo, il riconoscimento delle entità nominate, la risposta alle domande, e così via. Questo processo di affinamento adat- ta le rappresentazioni apprese durante il pre-addestramento al compito specifico, permettendo al modello di raggiungere pre- stazioni elevate con meno dati di addestramento rispetto a un modello addestrato da zero.
7.4.1 Modelli aperti e chiusi
Sicuramente, ci sono vantaggi sia nell’avere un modello di ap- prendimento automatico "aperto", cioè accessibile per il down- load e l’utilizzo diretto, sia nel disporre di un modello "chiuso" accessibile solo attraverso un’API (Application Programming Interface). Tuttavia, ci sono alcuni motivi chiave per cui un modello aperto può avere dei vantaggi significativi:
1. Personalizzazione: Con un modello aperto, gli sviluppa- tori hanno la possibilità di personalizzare il modello per adattarlo alle loro esigenze specifiche. Possono modifica- re i parametri del modello, utilizzare diversi metodi di ad- destramento o persino integrare il modello in sistemi più complessi. Questa flessibilità può essere particolarmente utile per applicazioni di nicchia o specifiche.
2. Privacy: L’utilizzo di un modello aperto può consentire di mantenere i dati sensibili all’interno del tuo ambiente sicuro, senza doverli condividere con un fornitore di API.
80

7 Text2Everything: Non Solo Testo
Questo può essere particolarmente importante per orga- nizzazioni che gestiscono dati sensibili o confidenziali.
3. Costi: L’uso di un modello aperto può ridurre i costi, so- prattutto se si prevede un alto volume di richieste. Men- tre un fornitore di API potrebbe addebitare una tariffa per ogni richiesta, l’utilizzo di un modello aperto comporte- rebbe solo i costi di calcolo e di archiviazione.
4. Indipendenza dai fornitori: Con un modello aperto, non si è legati a un particolare fornitore. Se un fornitore di API dovesse cambiare le sue politiche, aumentare i prezzi o interrompere il servizio, questo non influenzerebbe la capacità di utilizzare il modello.
5. Ricerca e sviluppo: I modelli aperti sono fondamenta- li per la ricerca scientifica. Permettono ai ricercatori di studiare come funzionano i modelli, di confrontare diver- si modelli e di costruire su di essi. Questo non sarebbe possibile con un modello chiuso dietro ad un’API.
Tuttavia, è importante notare che l’uso di modelli aperti ri- chiede anche risorse e competenze tecniche. Ad esempio, po- trebbe essere necessario disporre di infrastrutture di calcolo, di competenze in materia di apprendimento automatico e di capacità di gestire questioni di sicurezza e privacy.
7.4.2 RLHF: Reinforcement Learning from Human Feedback
L’apprendimento per rinforzo da feedback umano (RLHF - Rein- forcement Learning from Human Feedback) è una tecnica che addestra un "modello di ricompensa" direttamente dal feedback
81

7 Text2Everything: Non Solo Testo
umano e usa il modello come una funzione di ricompensa per ottimizzare la politica di un agente attraverso l’apprendimen- to per rinforzo (RL) mediante un algoritmo di ottimizzazione come la PPO, o Proximal Policy Optimization. Il modello di ricompensa viene addestrato in anticipo rispetto alla politica da ottimizzare per prevedere se un dato output è buono (alta ricompensa) o cattivo (bassa ricompensa). RLHF può migliora- re la robustezza e l’esplorazione degli agenti RL, specialmente quando la funzione di ricompensa è scarsa o rumorosa.
Il feedback umano viene raccolto chiedendo agli umani di classificare gli esempi di comportamento dell’agente. Queste classificazioni possono quindi essere utilizzate per valutare gli output, ad esempio con il sistema di valutazione Elo.
L’apprendimento per rinforzo ordinario, in cui gli agenti im- parano dalle proprie azioni basandosi su una "funzione di ri- compensa", è difficile da applicare ai compiti NLP perché le ricompense non sono spesso facili da definire o misurare, spe- cialmente quando si tratta di compiti complessi che coinvolgo- no valori o preferenze umane. RLHF può permettere ai modelli di linguaggio di fornire risposte che si allineano con questi va- lori complessi, di generare risposte più verbose, e di rifiutare domande che sono o inappropriate o al di fuori dello spazio di conoscenza del modello. Un esempio di modello addestrato con RLHF è ChatGPT.
Vediamo il funzionamento:
1. Si parte da un LLM pre-trained, senza fine-tuning
2. Si genera un secondo modello, detto "reward model", che data una sequenza di testo genera uno scalare di ricom- pensa che rappresenta in qualche modo una preferenza umana. Per far ciò, si utilizzano umani per comparare dif- ferenti output dallo stesso prompt. Questi output vengo-
82

7 Text2Everything: Non Solo Testo
no poi classificati con un sistema, tipo Elo, e normalizzati in punteggi numerici.
3. Poi, utilizziamo il RL per fare fine-tuning del LLM sul reward model.
Figura 7.1: La creazione di un reward model
Per fare fine-tuning con RL bisogna però impostare il proble- ma come un classico problema di Reinforcement Learning:
• Policy: un LM che prende un prompt e ritorna una se- quenza di testo
• Spazio d’azione: tutti i token del vocabolario del LM
• Spazio d’osservazione: la distribuzione delle possibili se- quenze in input
83
 
7 Text2Everything: Non Solo Testo
• Funzione di ricompensa: la combinazione del preference model e un constraint sul cambiamento della policy
Figura 7.2: Reinforcement Learning from Human Feedback
Andiamo a preferire la distribuzione dei token che piace di più all’umano rispetto ad altre, calcolando la Kullback-Leibler (KL) divergence tra quello proposto e quello voluto.
Le problematiche di RLHF
Purtroppo il RLHF non è la soluzione ai vari problemi che atta- nagliano i modelli stato dell’arte. Ci sono differenti difetti che andiamo ad elencare:
84
 
7 Text2Everything: Non Solo Testo
1. RLHF ha bisogno di tanti crowdworkers per poter essere efficace
2. RLHF mette l’etica del modello in mano a chi lo deve giudicare: le compagnie addestrano questi crowdworkers sulla base delle risposte volute ma il risultato dell’opera- zione è sempre limitato all’abilità del singolo di giudicare eticamente o meno una risposta.
3. RLHF ha un costo operativo oltre a quello computaziona- le
Come risolvere il problema?
7.4.3 Constitutional AI
Una soluzione, proposta da Anthropic è un metodo di self- supervision chiamato "Constitutional AI". Brevemente, il pro- cesso è il seguente:
• Si prende un modello già con un fine-tune RLHF
• Il modello risponde a molte domande, alcune delle quali potenzialmente dannose, e genera risposte in bozza
• Il sistema mostra al modello la sua risposta in bozza, in- sieme ad un prompt che dice qualcosa del tipo "riscrivi questo in modo più etico"
• L’AI lo riscrive per renderlo più etico
• Il sistema ripete questo processo fino a raccogliere un am- pio dataset di risposte "in bozza" e risposte rivisitate (più eticamente corrette)
85

7 Text2Everything: Non Solo Testo
• Il sistema addestra il modello a riscrivere risposte meno simili alle bozze e più simili alle rivisitate
Questo grafico 7.3 mette a confronto l’"Elo dell’efficacia" e l’"Elo dell’innocuità" con l’approccio RLHF standard e quello RL Costituzionale.
La pratica convenzionale suddivide l’IA etica in due catego- rie: "efficacia" e "innocuità". Un’IA è definita efficace se rispon- de alle domande in maniera appropriata. È definita innocua se non compie azioni dannose o offensive.
A volte, questi obiettivi possono entrare in conflitto. Un’IA potrebbe raggiungere un livello di innocuità massimo evitando di rispondere a qualsiasi domanda, comportamento riscontrato in alcuni dei primi modelli. Allo stesso modo, potrebbe rag- giungere un livello di efficacia massimo rispondendo a tutte le domande, anche a quelle problematiche come "come posso costruire una bomba?" o "classifica tutte le razze umane dalla migliore alla peggiore". Le società che producono questi model- li ambiscono a creare sistemi che bilancino questi due obiettivi, collocandosi lungo una sorta di frontiera di Pareto: non posso- no incrementare l’efficacia senza compromettere l’innocuità, e viceversa.
In questo contesto, Anthropic valuta l’efficacia e l’innocuità attraverso il sistema di punteggio Elo, un sistema originaria- mente utilizzato negli scacchi per determinare quale tra due giocatori vince più frequentemente. Se l’IA #1 ha un Elo di ef- ficacia di 200, e l’IA #2 ha un Elo di efficacia di 100, e si pone a entrambi una domanda, l’IA #1 dovrebbe risultare più efficace il 64% delle volte.
Il grafico sovrastante 7.3 mostra che i modelli formati con l’approccio costituzionale tendono a essere "meno nocivi a pa- rità di efficacia". Questa tecnica non solo risulta più economica e gestibile, ma si dimostra anche più efficiente.
86

7 Text2Everything: Non Solo Testo
 Figura 7.3: Il grafico di confronto tra RLHF standard e l’utilizzo di Constitutional AI
87

7 Text2Everything: Non Solo Testo
7.5 Dal Testo a Tutto
In questa breve sezione andiamo a parlare di alcuni modelli pre-trained transformer-based multi-modali che sono conside- rati stato dell’arte. Tutti i seguenti modelli sono transformativi e generativi, ovvero:
1. Transformativi: Partono da un input e lo trasformano in un output differente
2. Generativi: Generano contenuto, spesso in maniera sto- castica
Il materiale e la teoria di questa sezione è interamente visio- nabile ai seguenti Google Colab:
• Dal Testo al Testo
• Dal Testo all’Audio
• Dal Testo alle immagini • Dalle Immagini al Testo
88

8 Basicness
La maggior parte delle ricerche nel campo dell’elaborazione del linguaggio naturale non include lo studio del nostro vocabola- rio: si concentra spesso più sulla grammatica. Eppure, i conte- nuti sono incapsulati all’interno delle parole, che generalmente possono avere un livello di complessità differente. Inoltre, il vo- cabolario è tipicamente in costante evoluzione all’interno di un contesto linguistico dinamico.
8.1 L’origine: il lavoro di Roger Brown
Esistono parole di base che sono usate da bambini e adulti in molte lingue e situazioni. Queste parole sono semplici, facili da imparare e da ricordare e si riferiscono a concetti comuni a tutti gli esseri umani.
Nel lavoro di Roger Brown Nel suo lavoro "How Shall a Thing be Called?" del 19581, Brown si chiede come scegliere il termini appropriati per descrivere oggetti e concetti. Questa domanda è particolarmente rilevante per i concetti complessi, per i quali è difficile trovare un termine "definitivo". Nonostante il tem- po trascorso, la questione rimane irrisolta e il lavoro di Brown sembra essere ancora molto significativo.
1Brown, Roger. How shall a thing be called?. Psychological review 65.1 (1958): 14.
89
 
8 Basicness
8.2 Livello base e livello avanzato
Il concetto di "vocabolario di base", o basic level, non è nuovo e ha già suscitato l’interesse di studiosi come Ogden nel lontano 19302 .
Figura 8.1: Una parte della "Ogden’s words list".
Ogden propose un insieme di parole che dovrebbero costitui- re la base della comunicazione quotidiana, pensato per essere la base lessicale per una comunicazione efficace, chiara e precisa.
Brown, parallelamente, sottolineava l’importanza di stabilire criteri chiari e precisi per determinare quali parole dovrebbero essere considerate "di base" in una lingua. Tali parole, secondo Brown, devono soddisfare i seguenti vincoli: essere brevi, rela- tive a concetti concreti, facili da pronunciare, e frequentemente usate.
Alcuni ricercatori hanno successivamente introdotto un altro concetto, simile se non identico a quello del basic level, chia- mato middle-level. Considerando una gerarchia di termini (dai
2 http://ogden.basic- english.org/words.html
90
  
8 Basicness
più generici a quelli più specifici), il basic level si colloca tipi- camente "a metà". Ad esempio, si apprendono prima termini come prima “forchetta”, per poi passare al più astratto “posata”. Nell’altro verso, si acquisiscono prima parole come “cane” per poi passare a sottospecificazioni, come ad es. “bassotto”.
Figura 8.2: Alcune definizioni di basic level / basic vocabulary presenti in letteratura.
Nuove forme di definizione del basic level menzionano il concetto di "sopravvivenza sociale": in altri termini, l’acquisi- zione del vocabolario di base diventa essenziale per i second language learners nel soddisfare bisogni primari, comunicando efficacemente con la comunità circostante. In questo contesto, vengono definiti "strumenti per la sopravvivenza sociale" le pa- role che sono necessarie per la comunicazione e l’interazione quotidiana.
91
 
8 Basicness
Legame termine-concetto
Nonostante queste considerazioni, non tutti i concetti sono fa- cilmente descritti da un termine corrispondente, in una speci- fica lingua. Viene inoltre affermato in letteratura che in molti casi i termini basic possono essere utilizzati per rappresentare concetti avanzati. Si pensi ad esempio alla parola "cane" per far riferimento al seguente concetto: Nelle armi da fuoco, barretta d’acciaio cilindrica che percuote il detonatore provocando l’accensione della carica di lancio e l’espulsione del proiettile3.
Soggettività
Il livello base, per un dizionario in una specifica lingua, è qual- cosa di soggettivo. Categorie, concetti e relativi termini vengono interpretati in maniera differente dalle persone, come dimostra- to da esperimenti recenti in letteratura4.
E i termini "advanced"?
Per quanto riguarda i termini di livello "avanzato", non è pre- sente alcuna definizione in letteratura se non "l’inverso del ba- sic". In altre parole, tutto ciò che non è basic, è advanced.
Applicazioni
Qual è lo scopo dei termini di livello base? L’acquisizione del vocabolario è un aspetto cruciale dell’apprendimento ad es. di
3Preso da Italian Open Multilingual WordNet.
4How shall a machine call a thing?, Federico Torrielli, Amon Rapp, and Luigi
Di Caro - International Conference on Natural Language and Information Systems, 2023.
92
 
8 Basicness
una seconda lingua come dello sviluppo delle competenze co- municative nei bambini. Il loro "sfruttamento" ed uso, ben con- testualizzato, può facilitare l’apprendimento di concetti com- plessi e avanzati, creare sistemi in ambito "health / language disorders" o in generale permettere un accesso informativo più semplice ed efficace.
93

8 Basicness
[LAB-6]
Si richiede di sviluppare uno tra i seguenti tre moduli:
• LAB-6a: ranking su basicness dei synset di WordNet. Si richiede di creare un mapping tra i synset di WordNet ed un basicness score (ad es. [0, 1]), utilizzando dati, risorse, features e approcci che credete opportuni alla risoluzione del task.
• LAB-6b: basicness in downstream tasks. Si richiedere l’u- so dei concetti e del dataset fornito su basicness (e/o al- tri dati e risorse) all’interno di in un task esistente (ad es. WSD, MT, text simplification, text segmentation, sum- marization, ecc.) motivandone e dimostrandone l’utilità (anche solamente in casi/domini specifici).
• LAB-6c: classificazione basic/advanced. Si richiedere l’u- so (o meno) del dataset su basicness per fare classificazio- ne automatica (binaria, basic/advanced) su nuovi termini e/o synset presi in esame.
94

9 Ontology Learning e Open Information Extraction
L’obiettivo di questo capitolo è di concentrarsi ulteriormen- te sulla semantica lessicale attraverso un approccio più auto- matico: costruire un’ontologia automaticamente da dati non stutturati.
Questo task, tutt’altro che banale, viene definito in lettera- tura come Ontology Learning e dal punto di vista tecnolo- gico riprendere temi legati al Text Mining, alla Distributional Semantics e alla Formal Semantics.
Successivamente verrà introdotta la Open Information Ex- traction (OIE), una metodologia che permette di estrarre una grande quantità di informazioni in formato semi-strutturato, come approccio più "moderato" all’interno dello stesso contesto dell’Ontology Learning.
9.1 Ontology Learning
Una prima definizione per l’Ontology Lerning viene data da Philipp Cimiano il quale afferma che essa può essere vista come un procedimento di reverse engineering: data una conoscenza di un certo dominio, con la sua rappresentazione e la sua codifica, si cerca di ritornare alla concettualizzazione di partenza.
Le problematiche legate a questo processo sono sostanzial- mente due:
95

9 Ontology Learning e Open Information Extraction
• La conoscenza del mondo non è codificata e dunque non è uguale per tutti. Si potrebbero avere visioni del mondo molto diverse fra loro;
• Non tutto quello che si sa del dominio viene effettivamen- te utilizzato: si potrebbe conoscere benissimo un sistema ma quando lo si va a concettualizzare si pensa soprattutto al suo utilizzo.
L’Ontology Learning comprende diverse sottospecificazioni, come ad esempio:
• Ontology Population: si ha a disposizione un’ontologia già costruita e quello che si vuole fare è analizzare la base testuale per trovare delle istanze da collocare all’interno dei concetti già esistenti dell’ontologia. Si cerca quindi di popolare l’ontologia con delle nuove istanze;
• Ontology Annotation: data una ontologia già costruita e partendo da una base documentale, l’obiettivo è quello di taggare il testo con delle informazioni concettuali. Si utilizza dunque la concettualizzazione come annotazione semantica all’interno di un testo;
• Ontology Enrichment: data un’ontologia già costruita e par- tendo da una base documentale, l’obiettivo è non solo quello di trovare istanze ma dire qualcosa in più riguar- do l’ontologia stessa, eventualmente ristrutturandola sia a livello di concetti (aggiungendo/rimuovendo nodi), sia a livello di relazioni (aggiungendo/rimuovendo archi).
Nella figura, il grado di formalizzazione della serie di rappre- sentazioni dei dati cresce da sinistra verso destra. Infatti la mi- nor formalizzazione possibile è quella del testo non strutturato
96

9 Ontology Learning e Open Information Extraction
(document repository), a cui segue i) quella della terminologia in cui abbiamo una serie di termini espressi per dominio di in- teresse, e ii) la rappresentazione a glossario in cui è possibile trovare termini associati alle proprie definizioni.
Una rappresentazione più sofisticata è quella basata su un thesaurus (come WordNet) che fornisce una rete di relazioni tra parole, come sinonimi, iperonimi e così via. Una tassonomia, ad esempio, organizza i concetti in una gerarchia, mentre un’onto- logia più avanzata offre una struttura che comprende una va- rietà di relazioni (ad esempio "parte-di", "utile-per", ecc.), regole e assiomi verso una maggiore profondità semantica. Infine, è possibile imporre vincoli logici per abilitare l’elaborazione di inferenze. In generale, man mano che si passa dai metodi me- no sofisticati a quelli più avanzati, la complessità degli approcci automatici aumenta.
9.1.1 Tasks
Nell’Ontology Learning esistono diversi task:
• Term Extraction: trovare nomi sia per i concetti sia per le relazione tra i concetti;
• Synonym Extracion: estrazione di parole che hanno lo stes- so significato in determinati contesti;
• Concept Extraction: può essere intensionale o estensionale. Nel primo caso si parla anche di gloss learning: si cerca di astrarre e di rappresentare attraverso un formato stringato tutto ciò che può descrivere quel determinato concetto. Nel secondo caso si tratta di enumerare tutti gli elementi che descrivono il determinato concetto;
97

9 Ontology Learning e Open Information Extraction
• Concept Hierarchies Induction: si cerca di strutturare dei
concetti già noti attraverso una tassonomia;
• Relation Extraction: si cerca di strutturare dei concetti già noti attraverso delle relazioni. Si è già parlato di relation extraction quando si è parlato di Pair-Pattern Matrix che però è piuttosto un approccio distribuzionale all’estrazio- ne di relazioni;
• Population: spesso fatta attraverso o Named-Entity Reco- gnition (NER) o attraverso Information Extraction (IE). Nel primo caso si cerca di popolare l’ontologia attraver- so istanze ricavate da testi sfruttando relazioni chiamate instance-of; nel secondo caso si ha qualcosa di più gene- rico rispetto al NER (che tratta solo named entity). Ad esempio si potrebbero voler riconoscere i ruoli seman- tici di alcune persone all’interno di un testo e inserirli all’interno dell’ontologia;
• Notazione di Sussunzione: meccanismi formali legati al cam- po della logica-matematica che permettono di costruire automaticamente delle gerarchie, come ad esempio la For- mal Concept Analysis (FCA).
9.1.2 Metodi
Per cercare di portare a compimento i task appena descritti si hanno sostanzialmente tre metodologie, basate su tecniche di Natural Language Processing, matematiche (ad es. la Formal Concept Analysis), ed il Machine learning.
Col Natural Language Processing (NLP), approcci diversi pos- sono essere costruiti sull’estrazione ed uso di informazioni qua- li le part-of-speech e named entities, fasi di pre-processing, re-
98

9 Ontology Learning e Open Information Extraction
gole su alberi di parsing, informazioni statistiche, risorse lessi- cali, ecc. Vale in sostanza quel che avete appreso (e non solo) all’interno di questo corso.
Con la Formal Concept Analysis (o FCA), si sfrutta un ap- proccio che deriva dal mondo della matematica formale e che si può applicare anche alla costruzione automatica di gerarchie. Gli elementi fondamentali sono sostanzialmente tre: oggetti, at- tributi e incidenza. Gli oggetti sono l’equivalente dei concetti (o istanze, in generale) che hanno associate delle features (o at- tributi). L’incidenza rappresenta il fatto che un certo oggetto possiede o meno un certo attributo. Questa relazione tra ogget- ti e attributi viene espressa mediante una matrice di incidenza chiamata formal context.
Figura 9.1: Formal Concept Analysis (FCA): formal context (a), tabella dei concetti (b) e lattice risultante (c).
Sulla base di questa matrice, si possono creare degli operatori che analizzano in maniera deterministica la matrice per dedurre informazioni strutturate in un lattice. In particolare, il formal
99
 
9 Ontology Learning e Open Information Extraction
context presenta in ogni riga un attributo, e in ogni colonna un oggetto. Sugli elementi di questa matrice possiamo applicare due operatori:
• Operatore up: viene applicato sulle colonne della matri- ce (agli oggetti) e fornisce gli attributi che quell’oggetto possiede;
• Operatore down: viene applicato sulle righe (sugli attribu- ti) e specifica quali oggetti possiedono quell’attributo.
Ad esempio, considerando il formal context mostrato in Fi-
gura 9.1.2, si possono immaginare due applicazioni dei due operatori come segue:
    up(C4) = {f1, f2}
    down(f2, f3) = {C5, C7}
Sulla base di questi due operatori esiste un metodo deter- ministico che, data un formal context, restituisce un lattice di concetti. L’algoritmo tenta sostanzialmente di creare un ordine tra gli oggetti in base alla loro appartenenza a determinati attri- buti: sfruttando il fatto che esistono oggetti più generali di altri, viene costruito un lattice (reticolo) in cui innanzitutto vengono considerati gli insiemi di elementi che non possiedono attributi, poi gli insiemi degli elementi che ne possiedono uno, poi due , e così via. Attraverso quindi la navigazione di questa struttu- ra, è possibile costruire eventualmente anche una gerarchia di oggetti (e dedurre un’ontologia).
Col Machine (o Deep) Learning, si intendono tutte le tecniche di addestramento di reti (nelle sue varie forme) per la costru- zione di strutture concettuali e di ontologie (usando modello per l’estrazione di concetti e di relazioni tra di essi).
100

9 Ontology Learning e Open Information Extraction
9.2 Open Information Extraction
L’Open Information Extraction (o OIE) nasce dalla necessità di estrarre efficientemente grandi quantità di informazioni da cor- pora di grandi dimensioni. Si potrebbe definire l’Information Extraction come un lavoro di chirurgia sul testo per l’estrazione di specifiche parti informative. Per far questo si tenta di ana- lizzare il contesto utilizzando sensi, glosse, sintassi, ecc. per intuire la presenza di un’informazione ricercata all’interno di uno specifico punto del testo.
Queste operazioni di estrazione sono spesso molto costose dal punto di vista computazionale. Soprattuto se parliamo di miliardi di documenti, come nel Web. "Open", davanti ad In- formation Extraction indica in particolare una tipologia di estra- zione agile ed efficiente per la costruzione di conoscenza semi- strutturata. Nella maggior parte dei casi, tale conoscenza as- sume la forma di triple e rappresenta informazione relaziona- le. Tipicamente, si estraggono triple composte da argomento, espressione verbale e un ulteriore argomento (arg1 - vphrase - arg2).
Questo tipo di estrazione non è però semanticamente fondata in quanto si corre il rischio di estrarre anche tantissimo rumore su una grande quantità di dati a disposizione. Di fatto però è un approccio molto interessante in quanto riesce a ridurre lo spazio testuale iniziale e, allo stesso tempo, ottiene una picco- la forma di strutturazione che permette l’esecuzione di query (e.g. "restituisci gli argomenti che hanno un verbo all’interno delle verbal phrase"). Si ottiene quindi un approccio data-oriented che permette di effettuare delle estrazioni pseudo-semantiche su un insieme ridotto di dati testuali.
Questo approccio viene spesso utilizzato per fare Question Answering su larga scala perché spesso le risposte a doman- de puntuali come "chi ha fatto cosa" sono solitamente presenti
101

9 Ontology Learning e Open Information Extraction
all’interno di triple.
Un vantaggio dell’approccio OIE è che diventa facile inserire
dei vincoli sugli argomenti o sulle verbal phrase per richiedere dati più specifici diminuendo cosi il rumore: ad esempio, si potrebbe richiedere che la data di nascita di un personaggio che si sta cercando sia successiva ad un certo anno. L’OIE nasce e si sviluppa usando pattern semplici e tecniche NLP.
I problemi dell’OIE sono però molteplici. In primis, non esi- ste un approccio rigoroso o unico nell’estrazione di triple: que- sto fa si che in generale le triple siano disallineate tra i diversi sistemi, e non è facile capire quale sia l’approccio migliore. In secondo luogo, dal punto di vista scientifico è difficilissimo va- lutare e comparare questi sistemi proprio perché non esiste un gold standard o un metodo unico per definire le triple. Infine, non è sempre semplice applicare l’uso delle triple estratte in contesti reali.
I sistemi esistenti più famosi sono: ReVerb, uno dei primi, basato su vincoli sintattici; KrankeN, che usa uno step di WSD; ClausIE, che ha la particolarità di estrarre non solo triple; DefIE, che combina Parsing e WSD creando di fatto un grafo su cui si applicano pesatura degli archi e filtri di rilevanza.
102

10 Large Language Models e Prompting
I linguaggi di modellizzazione del linguaggio (LLMs) sono mo- delli di intelligenza artificiale utilizzati per generare testi. Gli LLMs basici sono allenati sulla predizione del prossimo token in un testo, ma la loro utilità si estende ben oltre la generazione di testi casuali. In questo libro, ci concentreremo sugli LLMs ”instructed”, un tipo di LLMs addestrati sulla base di domande e risposte.
In particolare, mentre i primi sono in grado di generare testi, non sono in grado di rispondere a domande specifiche o guida- re la generazione di testi in una direzione specifica. In contra- sto, i secondi sono addestrati sulla base di domande e risposte. Invece di generare testi casuali, sono in grado di generare te- sti in risposta a domande specifiche o di seguire un prompt specifico per generare testi in una particolare direzione.
Il prompting è un processo fondamentale nell’utilizzo degli LLMs. Si tratta di fornire al modello di intelligenza artificiale un input specifico che lo guida nella generazione di un output.
Tuttavia, il processo di prompting non è sempre facile e ri- chiede una certa attenzione per ottenere risultati di alta qualità. In questa sezione, esploreremo i principi chiave del prompting con LLMs.
103

10 Large Language Models e Prompting
10.1 Linee guida per il prompting
Per ottenere risultati di alta qualità con gli LLMs, è impor- tante seguire alcune linee guida per il prompting. Di seguito, elenchiamo alcuni dei principi chiave del prompting con LLMs:
• Istruzioni chiare e specifiche: è importante fornire al mo- dello di intelligenza artificiale istruzioni chiare e specifi- che che gli consentano di comprendere esattamente ciò che si sta chiedendo.
• Uso dei segni di punteggiatura: è possibile utilizzare i segni di punteggiatura, come trattini, virgolette, tag <> ecc., per specificare meglio le richieste di prompting.
• Richiesta di formato in output: è possibile specificare il formato in cui si desidera ricevere l’output, ad esempio in formato JSON con specifici metadati.
• Iterative prompt development: è possibile raffinare il pro- cesso di prompting attraverso un approccio iterativo che prevede la generazione di idee, l’implementazione, l’ana- lisi dei risultati e l’analisi degli errori.
Oltre a questi principi generici, ci sono alcune strategie di prompting con LLMs che possono aiutare a ottenere risultati di maggiore qualità o livello di dettaglio. Di seguito, elenchiamo alcuni di esse:
• Prompting basato su sequenze di istruzioni: è possibi- le utilizzare il prompting per fornire al modello una se- rie di istruzioni da seguire, ad esempio per eseguire una particolare attività.
104

10 Large Language Models e Prompting
• Prompting iterativo: è possibile chiedere al modello di svolgere una serie di mini-step prima di arrivare alla con- clusione finale. Questo è particolarmente utile quando si tratta di task complessi, come la risoluzione di problemi matematici.
• Prompting basato su dialoghi: se l’input è un dialogo tra più persone, è possibile utilizzare il prompting per conti- nuare il discorso o fare domande e ricevere risposte da un particolare personaggio nella conversazione.
• Forzare il modello a ragionare "piano piano": in alcuni casi, è necessario forzare il modello a non fornire sempli- cemente una risposta finale, ma una lista di piccoli step di ragionamenti. Questo aiuta il modello a ragionare in modo più accurato e dettagliato.
• Richiestadifeedback(chiamataChainofThought,oCoT): è possibile chiedere al modello di fornire un feedback sul proprio lavoro, ad esempio segnalando eventuali errori o imprecisioni.
Il prompting è un processo fondamentale nell’utilizzo degli LLMs. Seguendo le linee guida e i principi chiave del promp- ting, è possibile ottenere risultati di alta qualità e personalizza- ti. Inoltre, il prompting iterativo consente di raffinare costan- temente il processo di prompting, migliorando la qualità del lavoro svolto dal modello di intelligenza artificiale.
10.2 Summarization con LLMs
Gli LLMs possono essere utilizzati anche per sintetizzare testi lunghi in testi più brevi, conservandone il significato essenzia-
105

10 Large Language Models e Prompting
le. Questo processo di sintesi è utile per risparmiare tempo e ottenere una visione d’insieme di un testo complesso.
La sintesi con LLMs funziona attraverso la generazione di un testo di output che contiene le informazioni più rilevanti del testo di input. È possibile specificare il numero di token, frasi o caratteri che si desidera ottenere in output, in modo da personalizzare la lunghezza della sintesi. Inoltre, è possibile specificare un aspetto da considerare tra i tanti possibili. Ad esempio, se si sta sintetizzando una recensione di un ristorante, è possibile specificare di volere un output che si concentri sulla qualità del cibo, o sulla cortesia del personale, o su qualsiasi altro aspetto rilevante.
Un esercizio interessante per sperimentare la sintesi con LLMs consiste nel prendere un insieme di recensioni di un prodotto, servizio, ristorante, film o qualsiasi altro elemento, e provare di- versi tipi di sintesi. Ad esempio, si può provare a sintetizzare le recensioni in base alla loro valutazione complessiva, oppure in base alla presenza di parole chiave specifiche, come "efficienza", "affidabilità", "comodità", ecc.
La sintesi con LLMs presenta alcuni vantaggi rispetto ai me- todi tradizionali di sintesi. In primo luogo, gli LLMs sono in grado di conservare il significato essenziale del testo di input, fornendo un output di alta qualità. In secondo luogo, la sintesi con LLMs è altamente personalizzabile, consentendo agli uten- ti di definire il numero di token/frasi/caratteri in output e di specificare gli aspetti rilevanti del testo di input.
Infine, la sintesi con LLMs è molto efficiente, consentendo di risparmiare tempo e ottenere un’idea generale del testo di input in modo rapido ed efficace.
106

10 Large Language Models e Prompting
10.3 Inferenza con LLMs
Gli LLMs sono modelli di intelligenza artificiale che possono essere utilizzati per inferire informazioni da testi.
Un esempio di inferenza è l’analisi del sentiment, ovvero la capacità di determinare se un testo esprime un’opinione positi- va o negativa su un determinato argomento.
Il modo tradizionale di svolgere l’analisi del sentiment richie- de l’annotazione manuale dei testi e la creazione di modelli di classificazione. Tuttavia, con gli LLMs è possibile definire tutto a livello di prompting. Ad esempio, è possibile chiedere di- rettamente ”Qual è il sentiment di questa recensione di prodotto?” e specificare di volere una singola parola come risposta, come ”positivo” o ”negativo”. In alternativa, è possibile chiedere una lista di emozioni espresse da uno specifico testo, fornendo quin- di più materiale per comprendere ciò che viene espresso in una recensione.
In un approccio standard (nel senso: senza LLMs), queste domande richiederebbero l’utilizzo di modelli e addestramenti diversi, mentre con gli LLMs è possibile gestire tutto tramite un unico modello.
Un altro esempio di inferenza è l’estrazione di informazio- ni da un testo, come il nome di un’azienda o di un prodotto. Con gli LLMs, è possibile definire la richiesta di informazioni in modo flessibile e personalizzabile. Ad esempio, è possibi- le chiedere di estrarre il nome dell’azienda o del prodotto da una recensione, restituendo in caso contrario ad es. ”unknown”. Eventualmente, in formato JSON.
Inoltre, gli LLMs possono essere utilizzati per identificare i principali argomenti trattati in un testo. Anche in questo ca- so, è possibile personalizzare la richiesta di informazioni. Ad esempio, è possibile chiedere di fornire una lista di 5 argomenti,
107

10 Large Language Models e Prompting
ognuno composto da massimo 2 parole. In alternativa, è possi- bile fornire una lista di argomenti e chiedere se un testo è legato ad ognuno di quei topic, rispondendo 0 o 1. Questo tipo di in- ferenza può essere effettuato senza la necessità di addestrare un modello specifico di topic modeling.
In sintesi, gli LLMs offrono una maggiore flessibilità e perso- nalizzazione nell’inferenza di informazioni da un testo. Grazie alla loro natura altamente adattiva, gli LLMs consentono agli utenti di definire e personalizzare le richieste di informazioni in modo flessibile e intuitivo.
10.4 Trasformazione con LLMs
Gli LLMs possono essere utilizzati non solo per generare te- sti, ma anche per trasformarli in altri formati, lingue o stili di scrittura. In questa sezione, esploreremo alcuni esempi di trasformazione con LLMs.
Gli LLMs possono essere utilizzati per trasformare testi in di- versi modi. Ad esempio, possono sostituire le semplici espres- sioni regolari, che spesso richiedono molto tempo e fatica per essere costruite. Alcuni altri esempi di trasformazione con LLMs includono la traduzione, il rilevamento della lingua, la conver- sione di dati in formati diversi e la correzione di errori.
La traduzione è uno dei task principali di trasformazione. Ad esempio, si può tradurre un documento dall’Italiano all’Inglese. È anche possibile utilizzare gli LLMs per rilevare la lingua di un testo e tradurlo automaticamente in una lingua specifica.
Gli LLMs possono anche essere utilizzati per trasformare lo stile di scrittura di un testo. Ad esempio, si può trasformare un testo informale in uno formale o viceversa.
108

10 Large Language Models e Prompting
Un altro scenario è quello di utenti di un servizio che rilevano problemi e li comunicano all’interno di un sistema, magari in più lingue. Il task sarebbe quindi quello di tradurre tutto in una singola lingua in output. Altri esempi possono riguardare la trasformazione di un file JSON in una tabella HTML piuttostoc che la correzione di errori di ortografia e grammatica in un testo. Riuscite ad immaginarne voi altri?
10.5 Espansione con LLMs
I modelli di linguaggio (LLM) si sono dimostrati strumenti in- credibilmente versatili, in grado di svolgere una vasta gamma di compiti di elaborazione del linguaggio naturale. Una del- le loro applicazioni più potenti è nell’espansione di testi di input, consentendo agli utenti di generare contenuti più lun- ghi e dettagliati basati su una quantità inferiore di materiale di partenza.
L’utilizzo di un modello di linguaggio per espandere il testo è un processo semplice. Per iniziare, l’utente fornisce al modello una piccola quantità di testo, come alcune frasi o punti elenco. Il modello utilizza quindi la sua comprensione del linguaggio e del contesto per generare ulteriore testo che si basa sull’input originale.
Ad esempio, un utente potrebbe fornire al modello un in- sieme di istruzioni precise e schematiche e chiedere di gene- rare una e-mail basata su quelle informazioni. Il modello uti- lizzerebbe la sua comprensione del linguaggio e del contesto per creare una e-mail coerente che si espande sulle istruzioni originali e aggiunge ulteriori dettagli se necessario.
Una considerazione importante quando si utilizzano i mo- delli di linguaggio per l’espansione del testo è il concetto di
109

10 Large Language Models e Prompting
"temperatura". La temperatura è un parametro che può essere regolato per controllare la variabilità dell’output del modello. Con una temperatura bassa, il modello genererà risposte più prevedibili e attese, mentre una temperatura alta comporterà un output più diverso e potenzialmente inaspettato.
Il parametro di temperatura è una considerazione importan- te quando si utilizzano i modelli di linguaggio per l’espansione del testo. In alcuni casi, come nella generazione di documenta- zione tecnica o di pareri legali, una temperatura bassa può esse- re preferibile per garantire che l’output sia accurato e coerente. In altri casi, come nella generazione di scrittura creativa o di copywriting, una temperatura più alta può essere desiderabile per consentire una maggiore variazione e creatività nell’output.
In generale, i language model sono strumenti potenti per l’e- spansione dei testi di input, consentendo agli utenti di generare contenuti più lunghi e dettagliati basati su una quantità inferio- re di materiale di partenza. Regolando il parametro di tempe- ratura, gli utenti possono modificare l’output per soddisfare le loro esigenze specifiche e raggiungere il livello desiderato di variabilità e creatività.
10.6 Search / IR con LLMs
Tra le molte possibilità di applicazione, i LLMs possono essere utilizzati come sistemi per la ricerca di dati e informazioni, al pari di motori di ricerca noti come Google, Bing, ecc. In que- sta sezione, parleremo di come utilizzare i LLMs per effettuare ricerche o, in generale, per compiti di recupero di informazioni.
Quando si tratta di utilizzare i LLMs per la ricerca di in- formazioni, esistono alcune tecniche chiave che possono essere
110

10 Large Language Models e Prompting
utilizzate per garantire che il modello generi risultati accurati e pertinenti.
Un approccio comune consiste nell’utilizzare prompt basati sul linguaggio naturale che imitano il modo in cui un essere umano potrebbe formulare una domanda o effettuare una ri- chiesta. Ad esempio, anziché inserire semplicemente una paro- la chiave o una frase in una barra di ricerca, si potrebbe formu- lare la query sotto forma di domanda, ad esempio ”Come posso riparare un rubinetto che perde?”.
Un’altra tecnica consiste nell’utilizzare prompt più specifici e dettagliati che forniscono ulteriore contesto e vincoli per la ricerca. Ad esempio, se si cercano informazioni su un partico- lare argomento, può essere utile includere parole chiave o frasi specifiche relative a quell’argomento, insieme a eventuali filtri o attributi rilevanti come il periodo temporale, il luogo o il tipo di fonte.
È anche importante considerare le limitazioni dei LLMs quan- do si formulano prompt per compiti di recupero di informazio- ni. Sebbene questi modelli siano estremamente potenti e in grado di generare risultati altamente accurati, non sono infalli- bili e possono ancora produrre errori o risultati non pertinenti in determinati casi. Pertanto, può essere utile utilizzare più prompt o approcci quando si cercano informazioni, e valutare attentamente i risultati per garantire che siano accurati e per- tinenti alla query. Inoltre, è da tenere a mente che i modelli hanno ”codificato” informazioni (durante il training) che pos- sono risultare non corrette e/o non aggiornate rispettivamente al tempo corrente di utilizzo (inference time).
Ad esempio, per cercare informazioni su un particolare argo- mento, si potrebbe utilizzare un prompt basato sul linguaggio naturale come "Quali sono le ultime ricerche sulla terapia geni- ca per la malattia di Parkinson?", o si potrebbe utilizzare una
111

10 Large Language Models e Prompting
query più specifica che include parole chiave e filtri come "Te- rapia genica Parkinson ultimi 5 anni" per ottenere risultati più rilevanti e limitati nel tempo. In entrambi i casi, la chiave è quella di utilizzare prompt accurati e pertinenti per aiutare i LLM a generare i risultati desiderati.
10.7 Aspetti pratici
10.7.1 Elementi del prompting
Il prompting riguarda diverse componenti:
• Istruzione - si riferisce ad una specifica attività o direzio- ne che il modello deve eseguire.
• Contesto - può includere informazioni esterne o contesti aggiuntivi che possono guidare il modello verso risposte migliori.
• Input dati - si riferisce all’input o alla domanda per la quale vogliamo trovare una risposta.
• Indicatore di Output - indica il tipo o il formato dell’out- put.
10.7.2 Istruzioni
È possibile ideare prompt efficaci per svariati compiti semplici attraverso l’utilizzo di comandi precisi che indicano al modello l’obiettivo da raggiungere, come ad esempio "Scrivi", "Classifi- ca", "Riassumi", "Traduci", "Ordina" e così via.
È tuttavia importante tenere in considerazione che sarà neces- sario sperimentare diverse istruzioni, con parole chiave, conte- sti e dati differenti, al fine di individuare la soluzione più adatta
112

10 Large Language Models e Prompting
al proprio caso d’uso e alla propria attività. In genere, maggio- re è la specificità e la pertinenza del contesto rispetto all’attività che si vuole svolgere, maggiori saranno le possibilità di suc- cesso. Nelle prossime guide, approfondiremo l’importanza del campionamento e dell’aggiunta di ulteriore contesto.
E’ preferibile solitamente collocare le istruzioni all’inizio del prompt e utilizzare un separatore chiaro per differenziare l’i- struzione dal contesto.
10.7.3 Evitare le negazioni
Un altro suggerimento diffuso nella progettazione dei prompt consiste nell’evitare di esprimere ciò che non si vuole ottene- re, ma piuttosto di focalizzarsi su ciò che si desidera ottenere. Questo approccio incoraggia una maggiore precisione e atten- zione ai dettagli, fattori cruciali per ottenere risposte accurate dal modello.
10.7.4 Zero-shot, few-shot, chain-of-thought
Gli LLms sono addestrati su vasti dataset e configurati per se- guire precise istruzioni, in grado di eseguire compiti senza al- cuna supervisione (zero-shot). Ad esempio si può chiedere il sentimento (positivo o negativo) espresso da un testo.
A volte, può essere invece necessario definire qualche esem- pio per "spiegare" agli LLMs cosa si intende per una determi- nata etichetta semantica o compito richiesto in output. Questo è il caso few-shot.
Infine, quando si richiede un ragionamento e nemmeno le strategie few-shot sembrano funzionare, è possibile aggiungere al prompt un richiesta esplicita di passaggi (chain-of-thought)
113

10 Large Language Models e Prompting
per permettere ai modelli di performare meglio (come anticipa- to in una sezione precedente).
114

11 Esame e criteri di valutazione
L’esame per la terza parte del corso è composto principalmente da due parti: 1) domande sulla parte descrittiva e teorica; 2) domande sulla parte laboratoriale.
Nella prima parte, verranno presi in esame i seguenti criteri per la valutazione: 1.1) linguaggio tecnico utilizzato; 1.2) idee precise e chiare sulle definizioni e sul contesto tematico relativo alle domande poste; 1.3) capacità di legare i diversi contenuti all’interno di un discorso comparativo tra approcci, risorse, e in generale i temi spiegati a lezione.
Nella seconda parte, verranno scelti alcuni esercizi tra quelli presentati. La valutazione osserverà i seguenti criteri: 2.1) co- noscenza personale del codice presentato; 2.2) correttezza del codice, semplicità e leggibilità; 2.3) livello di approfondimento sui risultati ottenuti (ad es. contezza sui limiti degli approcci implementati, eventuali tentativi alternativi, ecc.).
115
